# Linux内核设计与实现

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035789-36.png)

操作系统是指在整个系统中负责完成最基本功能和系统管理的那些部分。这些部分应该包括内核、设备驱动程序、启动引导程序、命令行 Shell 或者其他种类的用户界面、基本的文件管理工具和系统工具。

处理器的活动：

- 运行于用户空间，执行用户进程。
- 运行于内核空间，处于进程上下文，代表某个特定的进程执行。
- 运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断。

| 目录          | 描述                                 |
| ------------- | ------------------------------------ |
| arch          | 特定体系结构的源码                   |
| block         | 块设备 I/O 层                        |
| crypto        | 加密 API                             |
| Documentation | 内核源码文档                         |
| drivers       | 设备驱动程序                         |
| firmware      | 使用某些驱动程序而需要的设备固件     |
| fs            | VFS 和各种文件系统                   |
| include       | 内核头文件                           |
| init          | 内核引导和初始化                     |
| ipc           | 进程间通信代码                       |
| kernel        | 像调度程序这样的核心子系统           |
| lib           | 通用内核函数                         |
| mm            | 内存管理子系统和 VM                  |
| net           | 网络子系统                           |
| samples       | 示例，示范代码                       |
| scripts       | 编译内核所用的脚本                   |
| security      | Linux 安全模块                       |
| sound         | 语音子系统                           |
| usr           | 早期用户空间代码（所谓的 initramfs） |
| tools         | 在 Linux 开发中有用的工具            |
| virt          | 虚拟化基础结构                       |

# 进程

## 进程管理

内核使用进程描述符来描述一个具体的进程任务，也就是 [task_struct](https://elixir.bootlin.com/linux/v6.14.5/C/ident/task_struct) ，然后在内核进程栈中维护了一个 [thread_info](https://elixir.bootlin.com/linux/v6.14.5/C/ident/thread_info) 结构指向这个task结构

```C
struct thread_info {
        struct pcb_struct        pcb;                /* palcode state */

        struct task_struct        *task;                /* main task structure */
        unsigned int                flags;                /* low level flags */
        unsigned int                ieee_state;        /* see fpu.h */

        unsigned                cpu;                /* current CPU */
        int                        preempt_count; /* 0 => preemptable, <0 => BUG */
        unsigned int                status;                /* thread-synchronous flags */

        int bpt_nsaved;
        unsigned long bpt_addr[2];                /* breakpoint handling  */
        unsigned int bpt_insn[2];
        unsigned long fp[32];
};
```

然后每个CPU存在[current_thread_info](https://elixir.bootlin.com/linux/v6.14.5/C/ident/current_thread_info) 这个结构指向这个thread_info结构用来获取当前正在执行的任务信息

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035733-1.png)

进程描述符的存放是根据系统的硬件结构来决定的，有的硬件结构直接存在一个寄存器用来放这个task_struct的结构的指针，有的是需要根据这个内核栈的偏移来获取的数据结果。

| 比较项     | `fork`                                                       | `vfork`                                                      |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 功能目的   | 创建新进程，子进程是父进程的副本                             | 创建新进程，与父进程共享地址空间                             |
| 内存使用   | 子进程复制父进程的地址空间，包括数据段、堆栈段和代码段       | 子进程和父进程共享地址空间，直到子进程调用 `exec` 系列函数或 `_exit` 函数 |
| 执行顺序   | 父进程和子进程并发执行，执行顺序由操作系统调度器决定         | 子进程先执行，父进程被阻塞，直到子进程调用 `exec` 系列函数或 `_exit` 函数 |
| 资源消耗   | 由于复制地址空间，资源消耗相对较大                           | 共享地址空间，资源消耗小                                     |
| 返回值     | 父进程中返回子进程的进程 ID（正整数），子进程中返回 0，出错返回 -1 | 父进程中返回子进程的进程 ID（正整数），子进程中返回 0，出错返回 -1 |
| 数据独立性 | 子进程有自己独立的数据副本，对数据的修改不会影响父进程       | 子进程对数据的修改会影响父进程，因为共享地址空间             |
| 用途       | 适用于需要子进程独立运行且有自己的内存空间的场景             | 适用于子进程立即调用 `exec` 系列函数加载新程序的场景，以节省资源 |

当子进程调用 `exec` 系列函数时，会按以下步骤进行内存操作：

- **替换当前进程映像**：`exec` 函数的主要目的是用新的程序替换当前进程（即子进程）的映像。这里的 “替换” 不是简单地在原有共享内存上覆盖数据，而是一系列复杂的操作。
- **重新分配内存**：操作系统会为新程序重新分配内存空间。这是因为新程序有其自身的代码结构、数据布局和运行要求，原有的共享内存空间可能无法满足这些需求。新分配的内存区域会根据新程序的大小和内存布局进行规划，以确保新程序能够正常运行。
- **加载新程序**：在新分配的内存区域中，操作系统会将新程序的代码、数据等加载进去。这个过程会把新程序的二进制代码加载到代码段，初始化全局变量和静态变量到数据段，设置好堆栈等运行环境。
- **断开与父进程的共享关系**：一旦新程序加载完成，子进程就拥有了一个全新的、独立于父进程的内存映像。此时，子进程和父进程不再共享同一片地址空间，子进程对新内存区域的任何操作都不会影响到父进程，反之亦然。

### 在linux中创建线程

### 用户空间

在Linux中模糊了线程的概念，在其他的系统中可以是一个进程管理多个线程，但是在Linux中，将就进程中的线程看做进程，其实也就是多个任务共享资源

在Linux中创建线程使用的是`clone()`系统调用，根据参数（系统调用的传递的参数）可以创建不同的线程

| 参数标志             | 含义                                                |
| -------------------- | --------------------------------------------------- |
| CLONE_FILES          | 父子进程共享打开的文件                              |
| CLONE_FS             | 父子进程共享文件系统信息                            |
| CLONE_IDLETASK       | 将 PID 设置为 0（只供 idle 进程使用）               |
| CLONE_NEWNS          | 为子进程创建新的命名空间                            |
| CLONE_PARENT         | 指定子进程与父进程拥有同一个父进程                  |
| CLONE_PTRACE         | 继续调试子进程                                      |
| CLONE_SETTID         | 将 TID 回写至用户空间                               |
| CLONE_SETTLS         | 为子进程创建新的 TLS                                |
| CLONE_SIGHAND        | 父子进程共享信号处理函数及被阻断的信号              |
| CLONE_SYSVSEM        | 父子进程共享 System V SEM_UNDO 语义                 |
| CLONE_THREAD         | 父子进程放入相同的线程组                            |
| CLONE_VFORK          | 调用 vfork ()，所以父进程准备睡眠等待子进程将其唤醒 |
| CLONE_UNTRACED       | 防止跟踪进程在子进程上强制执行 CLONE_PTRACE         |
| CLONE_STOP           | 以 TASK_STOPPED 状态开始进程                        |
| CLONE_SETTLS         | 为子进程创建新的 TLS (thread-local storage)         |
| CLONE_CHILD_CLEARTID | 清除子进程的 TID                                    |
| CLONE_CHILD_SETTID   | 设置子进程的 TID                                    |
| CLONE_PARENT_SETTID  | 设置父进程的 TID                                    |
| CLONE_VM             | 父子进程共享地址空间                                |

### 内核空间

在内核中创建线程是通过 [kthread_create](https://elixir.bootlin.com/linux/v6.14.5/C/ident/kthread_create) 这个宏来实现的，内核线程只能由其他的内核线程创建

```C
/**
 * kthread_create - create a kthread on the current node
 * @threadfn: the function to run in the thread
 * @data: data pointer for @threadfn()
 * @namefmt: printf-style format string for the thread name
 * @arg: arguments for @namefmt.
 *
 * This macro will create a kthread on the current node, leaving it in
 * the stopped state.  This is just a helper for kthread_create_on_node();
 * see the documentation there for more details.
 */
#define kthread_create(threadfn, data, namefmt, arg...) \
        kthread_create_on_node(threadfn, data, NUMA_NO_NODE, namefmt, ##arg)
```

`kthread_run`宏是创建一个内核线程（进程 任务）并通过`wake_up_process`实现这个任务并执行，单纯创建后是不可以运行的需要使用这个宏进行唤醒

```C
/**
 * kthread_run - create and wake a thread.
 * @threadfn: the function to run until signal_pending(current).
 * @data: data ptr for @threadfn.
 * @namefmt: printf-style name for the thread.
 *
 * Description: Convenient wrapper for kthread_create() followed by
 * wake_up_process().  Returns the kthread or ERR_PTR(-ENOMEM).
 */
#define kthread_run(threadfn, data, namefmt, ...)                           \
({                                                                           \
        struct task_struct *__k                                                   \
                = kthread_create(threadfn, data, namefmt, ## __VA_ARGS__); \
        if (!IS_ERR(__k))                                                   \
                wake_up_process(__k);                                           \
        __k;                                                                   \
})
```

内核线程结束是改线程执行`do_exit`退出，或者其他的内核线程调用这个`kthread_stop`

```C
int kthread_stop(struct task_struct* k);
```

## 进程调度

### 时间片相关概念

- **时间片定义**：时间片是指进程在被抢占前能持续运行的时间。它的值很关键，太长会使系统对交互响应变差，感觉系统无法并发执行应用程序；太短则会因进程频繁切换增加处理器开销，毕竟进程切换需要系统时间，而实际运行时间却很短。并且不同类型进程对时间片需求不同，I/O 消耗型进程不需要长的时间片，处理器消耗型进程希望时间片越长越好，比如长的时间片能提高其高速缓存命中率。
- **默认时间片设置**：很多操作系统为避免交互表现欠佳，默认时间片较短，如 10ms 。但 Linux 的 CFS（完全公平调度器）调度器不直接分配时间片给进程，而是给进程分配处理器使用比例，这个比例受进程 `nice` 值影响 。`nice` 值作为权重调整进程对处理器的使用比例，`nice` 值越高（优先级越低 ），权重越低，获得处理器使用比例小；`nice` 值越低（优先级越高 ），权重越高，获得处理器使用比例大。
- **Linux 的抢占机制**：Linux 是抢占式系统，进程进入可运行态就可投入运行。在多数操作系统中，是否抢占当前进程取决于进程优先级和时间片是否用完。而在 Linux 的 CFS 调度器中，抢占时机取决于新的可运行程序消耗的处理器使用比例。若新进程使用比例比当前进程小，就立刻抢占当前进程投入运行，否则推迟运行。

### 文字编辑程序和视频编码程序例子解读

- **进程类型特点**：文字编辑程序是 I/O 消耗型进程，大部分时间在等待用户键盘输入，用户期望按下键系统能马上响应；视频编码程序是处理器消耗型进程，除了开始读取原始数据流和最后输出处理好的视频外，大部分时间都在进行视频编码，对开始运行时间要求不严格，用户不太关心它是立刻运行还是半秒钟后运行，更关注它完成任务的早晚。
- **传统调度目标**：在多数操作系统中，为满足文字编辑程序的交互性，目标一是给它更多处理器时间，不是因为它实际需要，而是希望它需要时能随时得到处理器；目标二是让它被唤醒（用户打字时 ）能抢占视频编码程序，确保交互性能。通常靠给文字编辑程序分配比视频编码程序更高的优先级和时间片来实现。
- **Linux CFS 调度做法**：Linux 不通过分配固定优先级和时间片来实现上述目标。假设文字编辑程序和视频编码程序只有这两个运行进程且 `nice` 值相同，按 CFS 调度，它们理论上处理器使用比例各 50% 。但文字编辑程序因常等待用户输入，实际使用处理器时间少；视频编码程序会更多占用处理器时间。当文字编辑程序被唤醒（用户输入 ），CFS 发现它之前运行时间短，为保证所有进程公平分享处理器，会立刻抢占视频编码程序，让文字编辑程序运行。文字编辑程序处理完用户输入又进入睡眠等待下一次输入，因没消耗完 50% 的处理器使用比例，下次需要时 CFS 仍会优先让它运行，视频编码程序只能在剩余时间运行。 这样既保证了文字编辑程序的交互性，又在整体上兼顾了公平性 。

### 调度器的模块化设计目的

Linux 调度器采用模块化设计，以模块方式提供不同的调度算法。这么做是因为不同类型的进程对 CPU 资源分配的需求不同，例如实时进程要求低延迟、高优先级的资源分配，普通交互进程则更注重响应速度和公平性。通过模块化，能让不同类型进程针对性地选择合适的调度算法，满足多样化的调度需求。

### 调度器类（scheduler classes）

- **概念**：调度器类是一种将不同调度算法进行分类管理的模块化结构。它允许多种不同的调度算法同时存在于系统中，并且这些算法可以动态添加。每个调度器类负责调度属于自己范畴的进程。
- **优先级**：每个调度器类都有一个优先级。优先级决定了调度器类在竞争调度权时的先后顺序。基础的调度器代码定义在 `kernel/sched.c` 文件中，系统会按照优先级顺序遍历调度器类。
- **调度决策**：在调度时，系统会检查每个调度器类中是否有可执行的进程。拥有可执行进程且优先级最高的调度器类会胜出，由它来选择下一个要执行的进程。例如，实时调度器类的优先级通常高于普通进程调度器类，如果实时调度器类中有可执行的实时进程，那么它就会优先获得调度权，去选择实时进程执行，而普通进程调度器类则需等待。 这种机制确保了高优先级的进程能够优先获得 CPU 资源，同时也兼顾了不同类型进程的调度需求。

### CFS

CFS（完全公平调度器）的设计基于一个理想的多任务处理器模型理念，即系统能让每个进程都公平地获得处理器资源。在理想情况下，若有 `n` 个可运行进程，每个进程应获得 `1/n` 的处理器时间 。例如有两个进程，理想状态下它们应在相同时间内各使用一半的处理器能力，而不是像标准 Unix 调度模型那样，一个运行时独占处理器。但实际中，在一个处理器上无法真正同时运行多个进程，且进程频繁切换会带来开销，影响缓存效率 ，所以 CFS 要在追求公平的同时兼顾系统性能。

CFS 总是尝试选择 vruntime 最小的进程运行，即选择之前占用 CPU 时间短、受 “不公平” 对待的进程，以此保证公平性，同时兼顾高优先级进程（通过权重机制获得更多运行时间 ） 。 当进程从一个 CPU 的运行队列转移到另一个时，其 vruntime 会根据队列的 `min_vruntime` 值进行调整（出队列减去，入队列加上 ），维持相对公平 。 此外，CFS 还设定了进程占用 CPU 的最小时间值 `sched_min_granularity_ns` ，正在运行的进程若不足这个时间一般不调离 CPU ，避免过于短暂的进程切换消耗 。

#### 核心概念

- **虚拟运行时间（vruntime）** ：CFS 通过 vruntime 实现公平性。计算公式为 `vruntime = 实际运行时间 × 1024 / 进程权重` 。实际运行时间即进程实际占用 CPU 的时长；进程权重由进程的 nice 值决定，nice 值越小（优先级越高），权重越大，比如 nice = 0 时对应权重 1024 。通过该公式，将实际运行时间按权重标准化，便于比较不同进程对 CPU 资源的消耗情况 。例如，进程 A 权重为 1，运行 10ms，其 vruntime = 10×1024 / 1 = 10240；进程 B 权重为 2，运行 5ms，其 vruntime = 5×1024 / 2 = 2560 ，说明进程 B 消耗资源相对少。
- **调度周期与最小调度粒度** ：调度周期指将所有处于可运行状态（TASK_RUNNING）的进程都调度一遍的时间，默认约 20ms，保证所有进程至少运行一次。最小调度粒度默认 4ms ，防止进程频繁切换。当进程数量过多，导致按调度周期平分的时间片小于最小调度粒度时，调度周期会调整为 `最小粒度 × 进程数` ，确保每个进程至少运行最小粒度时间 。

#### 特殊处理机制

- **新进程处理** ：新进程初始 vruntime 设为当前运行队列的 `min_vruntime` ，防止新进程饥饿，使其与老进程在 vruntime 上保持合理差距，能较快获得运行机会 。
- **唤醒进程处理** ：休眠进程唤醒时会得到 vruntime 补偿，增加其 vruntime 值，使其更易抢占 CPU 。这样可提升交互式任务响应速度，因为交互式进程常因等待用户输入而休眠，唤醒后能尽快运行 。

#### 工作原理

1. **摒弃传统时间片分配**：CFS 不再像传统调度那样给每个进程分配固定时间片，而是基于可运行进程总数来计算每个进程应运行的时长。它会循环轮转，选择运行时间最少的进程作为下一个运行进程 。
2. **基于权重分配时间**：进程的 `nice` 值在 CFS 中作为权重，决定进程获得的处理器运行比例。`nice` 值越高（优先级越低 ），进程获得的处理器使用权重越低；`nice` 值越低（优先级越高 ），权重越高 。例如，`nice` 值为 5 的进程权重是默认 `nice` 值（0）进程的 `1/3` 。
3. **目标延迟与时间片计算**：CFS 设置了 “目标延迟”，它是对理想中无限小调度周期的近似值。目标延迟越小，交互性越好，但切换代价越高、系统总吞吐量越差 。假设目标延迟为 20ms，若有两个优先级相同的可运行任务，每个任务在被抢占前运行 10ms；若有 4 个，每个运行 5ms；若有 20 个，每个运行 1ms 。
4. **最小粒度限制**：当可运行任务数量趋于无穷时，为避免过高的切换消耗，CFS 引入最小粒度（默认 1ms ）。即使进程数量极多，每个进程最少也能获得 1ms 的运行时间 。
5. **公平性与相对值影响**：CFS 确保每个进程能公平获得处理器使用比例。进程获得的处理器时间由它与其他可运行进程 `nice` 值的相对差值决定，`nice` 值对时间片的作用是几何加权而非算数加权 。例如，`nice` 值为 10 和 15 的两个进程，分配的时间片可能和 `nice` 值为 0 和 5 的进程分配情况类似（假设目标延迟相同 ），因为起决定作用的是相对值 。

### 上下文切换

当要从一个正在执行的进程，换成另一个进程去执行的时候，就涉及到上下文切换。这一操作由定义在 `kernel/sched.c` 文件中的 `context_switch()` 函数来负责。每次 `schedule()` 函数选好一个新的进程准备让它运行时，就会调用这个 `context_switch()` 函数。它主要做两件事：

- 第一件事是调用 `switch_mm()` 函数（在 `<asm/mmu_context.h>` 中声明 ），这个函数的工作是把虚拟内存的映射，从上一个进程切换到新进程上。简单理解，就是让新进程能正确访问自己的内存空间。
- 第二件事是调用 `switch_to` 函数（在 `<asm/system.h>` 中声明 ），它负责把处理器的状态，从上一个进程切换到新进程。比如保存上一个进程的运行状态信息，像栈信息、寄存器信息等，再把新进程的这些状态信息恢复好，这样新进程就能接着之前的状态继续运行啦。

#### need_resched 标志

- **标志作用**：内核得知道啥时候该进行进程调度，如果光靠用户程序自己主动去调用调度函数，那可不行，可能会导致有些进程一直霸占着 CPU 不撒手。所以内核弄了个 `need_resched` 标志，用来告诉内核是不是得重新进行一次进程调度啦。要是某个进程该被别的进程挤下去（被抢占 ），或者有个优先级更高的进程准备好可以运行了，这个标志就会被设置。
- **相关操作函数**：有专门的函数来设置、清除和检查这个标志。比如 `set_task_need_resched()` 是设置标志，`clear_task_need_resched()` 是清除标志，`need_resched()` 是检查标志有没有被设置。
- **存储位置变化**：以前这个标志是全局变量，后来在不同内核版本里，它被挪到了进程描述符相关的结构体里，比如从 `task_struct` 又挪到了 `thread_info` 结构体里。为啥这么挪呢？因为从进程描述符里访问这个标志更快，这样能让内核更高效地判断是不是要调度。
- **检查时机**：当内核要从自己的地盘回到用户空间，或者从中断处理完要返回的时候，都会瞅一眼这个 `need_resched` 标志。要是发现标志被设置了，那就赶紧调用调度程序，重新选个合适的进程来运行。

##### 设置时机

- **进程时间片用完**：在时钟中断处理程序中，会检查当前进程的时间片是否用完。如果时间片用完，就会置位`need_resched`标志，表明需要进行进程调度，让其他进程有机会运行。
- **更高优先级进程进入就绪状态**：在中断处理程序或者系统调用中，当由于唤醒等动作导致更高优先级的进程进入就绪状态时，会设置`need_resched`标志。这样，当内核回到可以进行调度的时机时，就会优先调度更高优先级的进程。
- **进程主动调用相关函数**：进程调用`sleep`或`exit`等函数进行状态切换时，这些函数通过系统调用进入内核，会主动调用`schedule`函数，在调用`schedule`函数之前通常会置位`need_resched`标志。另外，内核代码中如果显式调用`cond_resched()`函数，也会检查是否满足条件并置位`need_resched`标志，触发抢占。
- **内核中主动置位**：在中断处理程序或者系统调用中，如果有必要，也可以直接主动置位`need_resched`标志，告知内核需要进行进程调度。

#### 用户抢占

当内核快要回到用户空间的时候，会看一眼 `need_resched` 标志。要是标志被设置了，那就会调用 `schedule()` 函数，这时候就发生了用户抢占。简单说，就是本来在用户空间运行的进程，可能因为这个标志被设置，就被内核换成别的进程去运行了。这种用户抢占一般发生在从系统调用返回用户空间，或者从中断处理程序返回用户空间的时候。

#### 内核抢占

- **和其他系统的不同**：Linux 这个系统很厉害，它完整支持内核抢占，这和其他好多操作系统不太一样。在那些不支持内核抢占的系统里，内核代码一旦开始执行，就得等它全部执行完，别的进程才能有机会。但 Linux 不是，只要条件允许，随时能把正在执行的内核任务打断，去运行别的更合适的任务。
- **能抢占的条件**：啥时候能进行内核抢占呢？首先得没有锁，用 `preempt_count` 这个计数器来记录锁的情况，它初始是 0，每次加锁就加 1，解锁就减 1 。当 `preempt_count` 变回 0 而且 `need_resched` 标志被设置的时候，就说明可以安全地进行内核抢占啦。每次从中断返回内核空间时，内核都会检查这俩条件。
- **内核抢占发生场景**：一般在几种情况下会发生。比如内核代码变得又可以被抢占了；或者内核里的任务自己主动调用 `schedule()` 函数；还有就是内核里的任务被堵住（阻塞 ）了，这时候也会调用 `schedule()` 函数，然后可能就发生内核抢占。

### 调度策略类型

- **实时调度策略**：Linux 提供 `SCHED_FIFO` 和 `SCHED_RR` 两种实时调度策略 。
- **非实时调度策略**：`SCHED_NORMAL` 为普通的非实时调度策略。实时策略不由常规公平调度器管理，而是由特定实时调度器管理，其实现定义在 `kernel/sched_rt.c` 文件中。

#### SCHED_FIFO 调度策略

- **算法原理**：实现简单的先入先出调度算法，不使用时间片。可运行状态下，`SCHED_FIFO` 级进程优先于 `SCHED_NORMAL` 级进程被调度。一旦 `SCHED_FIFO` 进程开始执行，除非它主动阻塞（如等待资源 ）或释放处理器，否则会一直执行下去。
- **抢占规则**：只有更高优先级的 `SCHED_FIFO` 或 `SCHED_RR` 任务能抢占它。同优先级的 `SCHED_FIFO` 进程轮流执行，且仅在它们主动让出处理器时才会退出执行，其他较低级别进程需等待其执行完毕才有机会运行。

#### SCHED_RR 调度策略

- **算法原理**：类似于 `SCHED_FIFO`，但带有时间片，属于实时轮流调度算法。进程耗尽分配的时间片后，同一优先级的其他实时进程会被轮流调度。时间片仅用于重新调度同一优先级的进程。
- **抢占规则**：高优先级 `SCHED_RR` 能立即抢占低优先级进程，但低优先级进程不能抢占 `SCHED_RR` 任务，即便其时间片耗尽 。

#### 两种策略的共性

- 都是静态优先级算法，内核不为实时进程计算动态优先级，保证给定优先级的实时进程总能抢占优先级更低的进程。
- 提供软实时工作方式，内核尽力让进程在限定时间前运行，但不保证一定满足，区别于硬实时系统（能保证满足时间要求 ）。不过 Linux 对实时任务的调度性能较好，可满足较严格时间要求。

#### 优先级范围

- 实时优先级范围从 0 到 `MAX_RT_PRIO` - 1，默认 `MAX_RT_PRIO` 为 100，即默认实时优先级范围是 0 到 99 。
- `SCHED_NORMAL` 级进程的 `nice` 值与实时优先级共享取值空间，`nice` 值 -20 到 +19 对应 100 到 139 的实时优先级范围 。

### CFS和其他调度策略

#### CFS 与实时调度策略共存

- **实时调度策略**：Linux 提供 `SCHED_FIFO`（先进先出 ）和 `SCHED_RR`（时间片轮转 ）等实时调度策略，用于对时间敏感、要求低延迟的实时任务。比如工业控制系统中的数据采集与控制任务，多媒体播放中的音频、视频解码任务等 。
- **CFS**：主要负责普通进程（非实时进程 ）的调度，旨在为普通进程公平分配 CPU 时间。像日常使用的办公软件、浏览器等应用程序的进程，由 CFS 进行调度。
- **共存方式**：内核根据进程的调度策略类型进行区分调度。实时进程优先级高于普通进程，当系统中同时存在实时进程和普通进程时，实时进程会优先获得 CPU 资源。例如，在播放视频时（实时任务 ）同时打开文档编辑（普通任务 ），视频播放相关进程会优先被调度，保证视频流畅播放，而文档编辑进程在实时进程空闲时才得到执行机会 。

#### CFS 与其他普通调度策略共存

- **其他普通调度策略**：除 CFS 外，还有 `SCHED_BATCH` 用于批处理任务，这类任务对响应时间不敏感，如夜间进行的系统备份、数据批量处理任务等；`SCHED_IDLE` 用于低优先级任务，在系统空闲时运行，像一些系统清理、统计相关的后台任务 。
- **共存方式**：不同普通调度策略适用于不同应用场景，CFS 作为默认调度策略处理大多数普通用户进程。例如，系统会根据进程特性为批处理任务分配 `SCHED_BATCH` 策略，使其在系统资源相对空闲时运行，不与交互式任务（多由 CFS 调度 ）争夺资源；而 `SCHED_IDLE` 策略的任务仅在 CPU 无其他工作时才执行 。 此外，Linux 内核通过调度类（sched_class ）的框架来管理不同调度策略，每种调度类有各自的调度策略，调度类之间有优先级顺序，由高到低为：停机调度类 > 限期调度类 > 实时调度类 > 公平调度类（CFS ）> 空闲调度类 。这种机制保证了多种调度策略在系统中有序共存、协同工作 。

#### 停机调度类

- **例子**：在 Linux 系统中，迁移线程属于停机调度类 。比如当系统进行 CPU 热插拔操作时，为了保证操作期间系统状态的一致性和稳定性，迁移线程会被调度执行。它会将相关进程迁移到合适的 CPU 上，在此过程中，迁移线程可抢占其他进程，以确保自身能优先执行完成任务，而其他进程不能抢占它 。

#### 限期调度类（SCHED_DEADLINE ）

- **例子**：在一些对时间要求极为苛刻的工业自动化控制场景中，比如汽车生产线上的机器人动作控制。机器人需要在精确的时间点完成零件抓取、安装等操作。对应这些任务的进程会采用限期调度类 。系统会按照进程的绝对截止期限进行调度，保证机器人能在规定时间内准确执行动作，避免生产事故和产品质量问题 。

#### 实时调度类

- **SCHED_FIFO 例子**：在航空航天领域的飞行控制系统中，传感器数据采集进程常采用 `SCHED_FIFO` 调度策略。这些进程需要不间断地采集各类传感器数据（如高度、速度、姿态等 ），一旦开始执行，除非主动等待资源（如等待新的数据传输 ），否则要一直执行，以保证飞行控制系统能实时获取准确数据 。
- **SCHED_RR 例子**：在多媒体播放系统中，音频和视频解码进程可能采用 `SCHED_RR` 调度策略。这些进程需要按时间片轮流执行，以保证音频和视频的同步播放。每个解码进程在自己的时间片内执行，时间片耗尽后，同一优先级的其他解码进程会被轮流调度 。

#### 公平调度类（CFS，SCHED_NORMAL ）

- **例子**：日常使用的办公软件，如文字处理软件（如 WPS 文字 ）、电子表格软件（如 Excel ）等。当在这些软件中进行文字编辑、数据计算等操作时，相关进程由 CFS 调度。CFS 会根据进程的虚拟运行时间等因素，公平地为这些进程分配 CPU 时间，使得用户在操作过程中感觉流畅，不同办公软件进程之间也能合理共享 CPU 资源 。

#### 空闲调度类（SCHED_IDLE ）

- **例子**：系统中的一些统计信息收集任务，比如统计系统运行过程中各个进程的 CPU 使用率、内存占用率等信息的进程。这些任务对实时性要求极低，只有在系统处于空闲状态，没有其他可运行进程时，才会被调度执行 。这样既不会影响正常业务进程的运行，又能在空闲时完成系统信息收集工作 。

# 中断

## 中断和时间

Linux的中断处理是一个很长的过程，汇编获得硬件中断号后会转到内核对应的中断号，然后根据中断号转化出`irq_desc`这个结构体，这个结构体在内核的存在形式是数组或基数树，这个根据获得的这个结构体找到对应的`action`这个结构是一个指向`irqaction`结构体的指针，会调用里面的函数，然后这个是一个链表结构，会不断的执行后续的中断，直到链表遍历完毕。

也就是说，多个中断会共享一个中断号，在这个中断号发生中断时，内部的中断函数都会执行，所以需要驱动判断中断是否是自己的管理的设备发生的

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035733-2.png)

### 驱动的中断

也就是说，驱动实现的中断也就是构造`irqaction`这个结构体并注册到内核中，发生中断会调到handle对应的中断服务函数

```C
int request_irq(unsigned int irq, irqreturn_t (*handler)(int, void *, struct pt_regs *), 
                unsigned long irqflags, const char *devname, void *dev_id); 
void free_irq(unsigned int irq, void *dev_id);
```

- irq：设备使用的中断号，不是硬件手册上的，是内核对应的IRQ的号，这个号决定了`irqaction`结构会插入到那个链表中
- handler：注册的中断服务函数
- irqflags：中断标志，是初始化`irqaction`的flags变量，描述中断如何触发等
- name： 在/proc结构中显示的名字，也是`irqaction`的名字
- dev_id：设备号，在链表中用于区分这个各个设备的中断，这个变量会传回中断服务函数

### 中断下半部

#### 软中断

这个是内核支持的一个中断，存在一个全局变量和一个软中断的一个函数数组

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035733-3.png)

这个是硬件最后调用的`handle_IRQ`函数，`irq_enter`函数会给中断进程中的抢占计数器`preempt_count`加上`HARDIRQ_OFFSET`这个值，用来防止内核抢占，同时是判断是否在硬件中断中的一个判断，当内部定义的硬件中断结束后，会调用`irq_exit`函数来释放前面加的`HARDIRQ_OFFSET`变量，这个时候硬件的中断服务函数也就是上半部是执行完成了，但是在没有完全返回的时候会判断是否有软中断需要执行

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-4.png)

这个就是去掉`HARDIRQ_OFFSET`标志，判断是否有软中断存在，存在则调用`invoke_softirq`函数执行软中断，判断存在的软中断并执行对应的软中断，将所有的已将要发生的软中断处理完成再返回

#### tasklet

因为软中是全局变量并定死的，所以留了一个tasklet的软中断接口，用于处理一般大部分的中断函数，也就以另一种软中断的形式，同时不妨碍其他的软中断

也就是说，驱动要用的话就是设置对应的tasklet_struct并注册到内核中，在上半部标记对应的softirq的标志，这个下半部就会执行，但是这个存在一个缺点，因为是用标志的，所以存在执行次数和提交次数不一致的情况

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-5.png)

## 中断详细

### 上半部

```C
/* request_irq: 分配一条给定的中断线 */
int request_irq (unsigned int irq,
                                irq_handler_t handler,
                                unsigned long flags,
                                const char *name,
                                void *dev)
```

`flags`是用于初始化`struct irqaction`对象中`flags`成员的与中断相关标志

- 触发方式类
  - **`IRQF_TRIGGER_RISING`**：上升沿触发，即当信号从低电平跃变为高电平时触发中断。
  - **`IRQF_TRIGGER_FALLING`**：下降沿触发，信号从高电平跳变为低电平时触发中断。
  - **`IRQF_TRIGGER_HIGH`**：高电平触发，只要信号处于高电平状态就会触发中断。
  - **`IRQF_TRIGGER_LOW`**：低电平触发，信号处于低电平状态时触发中断。
- 中断处理行为类
  - **`IRQF_DISABLED`**：设置后内核在处理中断处理程序期间禁止所有其他中断。多数中断处理程序不设置，适用于希望快速执行的轻量级中断，是`SA_INTERRUPT`标志的现代表现形式 。在共享中断函数执行期间禁止中断的设置已废弃。
  - **`IRQF_SHARED`**：表明可以在多个中断处理程序之间共享中断线，共享中断必须设置此标志。
- 特殊用途类
  - **`IRQF_SAMPLE_RANDOM`**：表明设备产生的中断对内核熵池有贡献，设备中断间隔时间作为熵填充到熵池。设备中断速率可预知或可能受外部攻击者影响时，不建议设置 。
  - **`IRQF_TIMER`**：专为系统定时器的中断处理准备的标志，是定时器专用中断标志。

共享中断的服务程序的要求

1. **标志设置**：调用`request_irq()` 时，参数`flags`必须设置`IRQF_SHARED`标志 ，且 2.6 版后共享处理程序可混用`IRQF_DISABLED` 。设置此标志后，仅当中断线未注册，或线上已注册处理程序都指定该标志时，调用才可能成功。
2. **设备参数**：每个注册的中断处理程序的`dev`参数必须唯一，不能传递`NULL`值，常选用设备结构指针，因其唯一且处理程序可能会用到。
3. **中断区分**：中断处理程序需能判别自身设备是否产生中断，依赖硬件支持及程序内相关逻辑，多数硬件提供状态寄存器等类似机制辅助判断。内核接收中断后，会依次调用线上注册的处理程序，若设备未产生中断，对应处理程序应立即退出。 所有共享中断线的驱动程序都需满足上述要求，否则中断线无法共享。

| 函 数                | 说 明                                                        |
| -------------------- | ------------------------------------------------------------ |
| local_irq_disable()  | 禁止本地中断传递                                             |
| local_irq_enable()   | 激活本地中断传递                                             |
| local_irq_save()     | 保存本地中断传递的当前状态，然后禁止本地中断传递             |
| local_irq_restore()  | 恢复本地中断传递到给定的状态                                 |
| disable_irq()        | 禁止给定中断线，并确保该函数返回之前在该中断线上没有处理程序在运行 |
| disable_irq_nosync() | 禁止给定中断线                                               |
| enable_irq()         | 激活给定中断线                                               |
| irqs_disabled()      | 如果本地中断传递被禁止，则返回非 0；否则返回 0               |
| in_interrupt()       | 如果在中断上下文中，则返回非 0；如果在进程上下文中，则返回 0 |
| in_irq()             | 如果当前正在执行中断处理程序，则返回非 0；否则返回 0         |

### 下半部

软中断和tasklet是在中断上下文中执行的，不允许存在导致休眠的函数，工作队列是运行在进程上下文中，可以重新调度和睡眠

#### 软中断

1. **静态分配特性**：软中断在编译期间进行静态分配，这与可动态注册或注销的 tasklet 不同。其相关代码位于`kernel/softirq.c`文件。它由`softirq_action`结构体来表示，该结构体定义在`<linux/interrupt.h>`中，包含一个函数指针`action` ，用于指向软中断处理程序。在内核的`kernel/softirq.c`中定义了一个包含 32 个`softirq_action`结构体的数组`softirq_vec[NR_SOFTIRQS]` ，这意味着最多可存在 32 个软中断，但当前内核版本仅使用了其中 9 个，且注册软中断数目的最大值无法动态改变。
2. **处理程序细节**：软中断处理程序`action`的函数原型为`void softirq_handler(struct softirq_action *)` 。当内核运行软中断处理程序时，会执行这个`action`函数，并将对应的`softirq_action`结构体指针作为参数传递进去。这样设计的好处是，若结构体中将来加入新的域，无需变动所有软中断处理程序。此外，软中断之间不会相互抢占，不过中断处理程序可以抢占软中断 ，而且相同类型的软中断可在其他处理器上同时执行。
3. **执行流程剖析**：一个注册的软中断必须先被标记（触发软中断，即 raising the softirq）才会执行。通常，中断处理程序会在返回前标记它的软中断，使其在合适时刻运行。待处理的软中断会在从硬件中断代码处返回时、在`ksoftirqd`内核线程中、在一些显式检查和执行待处理软中断的代码（如网络子系统）中被检查和执行 。执行软中断的核心函数是`do_softirq()` ，它会检查并执行所有待处理的软中断。其主要步骤为：用`pending`变量保存`local_softirq_pending()`返回的 32 位软中断位图，该位图指示哪些软中断等待处理；将实际的软中断位图清零；把指针`h`指向`softirq_vec`数组首项；若`pending`首位为 1 ，则调用`h->action(h)` ；指针`h`后移一位，`pending`位图右移一位，重复上述步骤，直到`pending`变为 0 。

```C
u32 pending;
pending = local_softirq_pending();
if (pending) {
    struct softirq_action *h;
    /* 重设待处理的位图 */
    set_softirq_pending(0);
    h = softirq_vec;
    do {
        if (pending & 1)
            h->action(h);
        h++;
        pending >>= 1;
    } while (pending);
}
```

| tasklet         | 优先级 | 软中断描述            |
| --------------- | ------ | --------------------- |
| HI_SOFTIRQ      | 0      | 优先级高的 tasklets   |
| TIMER_SOFTIRQ   | 1      | 定时器的下半部        |
| NET_TX_SOFTIRQ  | 2      | 发送网络数据包        |
| NET_RX_SOFTIRQ  | 3      | 接收网络数据包        |
| BLOCK_SOFTIRQ   | 4      | BLOCK 装置            |
| TASKLET_SOFTIRQ | 5      | 正常优先权的 tasklets |
| SCHED_SOFTIRQ   | 6      | 调度程度              |
| HRTIMER_SOFTIRQ | 7      | 高分辨率定时器        |
| RCU_SOFTIRQ     | 8      | RCU 锁定              |

软中断的使用

1. **分配规则**：在编译期间，通过`<linux/interrupt.h>`中定义的枚举类型来静态声明软中断。内核使用从 0 开始的索引表示软中断的相对优先级，索引值小的软中断会在索引值大的之前执行 。添加新软中断时，需加入此枚举类型列表，依据期望优先级决定位置，习惯上`HI_SOFTIRQ`常作为第一项，`RCU_SOFTIRQ`作为最后一项 ，新项可能插在`BLOCK_SOFTIRQ`和`TASKLET_SOFTIRQ`之间。
2. **处理程序注册**：运行时，通过调用`open_softirq()`函数来注册软中断处理程序，该函数接收软中断的索引号和处理函数两个参数。以网络子系统为例，在`net/core/dev.c`中通过`open_softirq(NET_TX_SOFTIRQ, net_tx_action);`等方式注册软中断 。软中断处理程序执行时允许响应中断，但自身不能休眠。若多个处理器同时运行同一软中断处理程序，共享数据（即使是处理程序内部的全局变量）都需严格的锁保护，这也是 tasklet 更受青睐的原因之一，因为 tasklet 通过一些机制避免了复杂锁操作。
3. **触发机制**：新的软中断处理程序注册后，可通过`raise_softirq()`函数将其设置为挂起状态，以便下次调用`do_softirq()`函数时投入运行。例如网络子系统可能调用`raise_softirq(NET_TX_SOFTIRQ);`触发相应软中断 。在触发软中断前通常要禁止中断，触发后恢复原状态，若中断原本已被禁止，可调用`raise_softirq_irqoff()`优化。在中断处理程序中触发软中断是常见形式 ，即中断处理程序执行硬件设备相关操作后触发软中断，随后内核执行完中断处理程序后调用`do_softirq()` ，让软中断开始执行中断处理程序剩余任务。软中断主要用于对时间要求严格且重要的下半部处理 ，目前网络和 SCSI 子系统直接使用，内核定时器和 tasklet 也基于软中断实现。

#### tasklet

- **定义**：基于软中断实现的下半部机制，与进程无关联，本质和行为与软中断相似，但接口更简单，锁保护要求更低。
- **应用场景**：多数下半部处理优先选用 tasklet，软中断仅用于执行频率和连续性要求极高的场景（如网络子系统）。

##### **tasklet 实现**

##### **数据结构**

- `tasklet_struct`结构体（定义于<linux/interrupt.h>）：

```C
struct tasklet_struct {
    struct tasklet_struct *next;  // 指向下一个tasklet的指针
    unsigned long state;          // 状态：0（未调度）、TASKLET_STATE_SCHED（已调度）、TASKLET_STATE_RUN（正在运行）
    atomic_t count;               // 引用计数器（0表示启用，非0表示禁用）
    void (*func)(unsigned long);  // 处理函数指针
    unsigned long data;           // 传递给处理函数的参数
};
```

##### **调度机制**

- 链表存储：已调度的 tasklet 存储在两个全局链表中：
  - `tasklet_vec`：普通优先级 tasklet。
  - `tasklet_hi_vec`：高优先级 tasklet（通过`HI_SOFTIRQ`软中断执行）
- 调度函数：
  - `tasklet_schedule()`：调度普通 tasklet（触发`TASKLET_SOFTIRQ`软中断）。
  - `tasklet_hi_schedule()`：调度高优先级 tasklet（触发`HI_SOFTIRQ`软中断）。

**`tasklet_schedule()`****执行流程**：

```C
static inline void tasklet_schedule(struct tasklet_struct *t) {
    if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
        _tasklet_schedule(t);
}

void _tasklet_schedule(struct tasklet_struct *t) {
    unsigned long flags;
    local_irq_save(flags);          // 保存中断状态并禁用本地中断
    t->next = __this_cpu_read(tasklet_vec.head);
    __this_cpu_write(tasklet_vec.head, t);
    raise_softirq_irqoff(TASKLET_SOFTIRQ);  // 触发软中断
    local_irq_restore(flags);       // 恢复中断状态
}
```

##### **执行机制**

- **软中断处理**：所有 tasklet 通过`HI_SOFTIRQ`和`TASKLET_SOFTIRQ`软中断实现。
- 执行流程：
  - **软中断触发**：当`TASKLET_SOFTIRQ`或`HI_SOFTIRQ`被触发时，内核调用对应的软中断处理函数。
  - **链表遍历**：软中断处理函数遍历`tasklet_vec`或`tasklet_hi_vec`链表，执行每个 tasklet 的处理函数。
  - **状态检查**：处理前检查`state`和`count`，确保 tasklet 未在其他 CPU 上运行且未被禁用。

```C
// TASKLET_SOFTIRQ软中断处理函数
static void tasklet_action(struct softirq_action *a) {
    struct tasklet_struct *list;
    
    local_irq_disable();
    list = __this_cpu_read(tasklet_vec.head);
    __this_cpu_write(tasklet_vec.head, NULL);
    __this_cpu_write(tasklet_vec.tail, &__this_cpu_read(tasklet_vec.head));
    local_irq_enable();
    
    while (list) {
        struct tasklet_struct *t = list;
        list = list->next;
        
        if (tasklet_trylock(t)) {  // 检查TASKLET_STATE_RUN状态
            if (!atomic_read(&t->count)) {  // 检查引用计数器
                clear_bit(TASKLET_STATE_SCHED, &t->state);
                t->func(t->data);          // 执行处理函数
                tasklet_unlock(t);
                continue;
            }
            tasklet_unlock(t);
        }
        
        // 如果tasklet正在运行或被禁用，则重新调度
        local_irq_disable();
        t->next = __this_cpu_read(tasklet_vec.head);
        __this_cpu_write(tasklet_vec.head, t);
        __raise_softirq_irqoff(TASKLET_SOFTIRQ);
        local_irq_enable();
    }
}
```

| 函数名                  | 功能描述                        |
| ----------------------- | ------------------------------- |
| `tasklet_schedule()`    | 调度 tasklet 执行               |
| `tasklet_hi_schedule()` | 高优先级调度                    |
| `tasklet_disable()`     | 禁用 tasklet（增加引用计数）    |
| `tasklet_enable()`      | 启用 tasklet（减少引用计数）    |
| `tasklet_kill()`        | 从队列中移除 tasklet 并等待完成 |

##### ksoftirqd 内核线程

###### 基本概念与作用

ksoftirqd 是 Linux 内核中辅助处理软中断（包括基于软中断实现的 tasklet）的一组内核线程 。每个处理器都对应有一个这样的线程，命名规则为 ksoftirqd/n ，其中 n 是处理器的编号 ，例如在双 CPU 的机器上就有 ksoftirqd0 和 ksoftirqd1 。其主要作用是当内核中出现大量软中断时，辅助处理这些软中断，以避免软中断处理过度占用 CPU 资源，平衡系统负载，保证系统的稳定性和响应性 。

```C
for (;;) {
    if (!softirq_pending(cpu))
        schedule();

    set_current_state(TASK_RUNNING);
    while (softirq_pending(cpu)) {
        do_softirq();
        if (need_resched())
            schedule();
    }
    set_current_state(TASK_INTERRUPTIBLE);
}
```

在这个循环中，首先通过 `softirq_pending(cpu)` 函数检查当前处理器上是否有待处理的软中断 。如果没有，线程调用 `schedule()` 函数进入睡眠状态，让出 CPU 资源给其他进程

- **处理软中断**：当检测到有待处理的软中断时，线程将自身状态设置为 `TASK_RUNNING` ，然后进入内层循环 。在内层循环中，调用 `do_softirq()` 函数处理所有待处理的软中断 。`do_softirq()` 函数会遍历并执行所有注册的软中断处理程序 。在每次迭代后，通过 `need_resched()` 函数检查是否需要重新调度 。如果需要（比如系统中存在更高优先级的进程需要运行），则调用 `schedule()` 函数进行进程调度，以便更重要的进程得到处理机会 。
- **结束处理与状态设置**：当所有需要执行的软中断操作都完成以后，ksoftirqd 线程将自己设置为 `TASK_INTERRUPTIBLE` 状态，再次唤起调度程序选择其他可执行进程投入运行 。如果在处理过程中，`do_softirq()` 函数发现已经执行过的内核线程重新触发了它自己（即软中断被再次触发），那么软中断内核线程就会被唤醒，继续处理新的软中断任务 。

###### 设计考量与优势

- **避免资源抢占**：ksoftirqd 线程在最低的优先级上运行（nice 值是 19） ，这样能避免它们跟其他重要的任务抢夺资源 。在系统负载较高时，不会因为软中断处理线程的运行而影响到关键进程的执行 。
- **平衡负载与保证响应性**：通过 ksoftirqd 线程来辅助处理软中断，能够在软中断负载很重的时候，保证用户程序不会因为软中断的持续处理而长时间得不到执行机会 。同时，在空闲系统上，软中断也能得到迅速处理，因为仅存的内核线程会马上被调度来处理软中断 。它有效地平衡了系统在不同负载情况下软中断处理和其他任务执行之间的关系，提升了系统整体的稳定性和响应性 。

###### 触发与执行时机

- **触发条件**：当软中断负载过高时，内核会唤醒 ksoftirqd 线程来处理这些软中断 。比如在进行大流量的网络通信期间，网络子系统产生大量软中断，或者软中断处理函数自行重复触发（像网络子系统会在软中断执行时重新触发自己以便再次得到执行），导致软中断处理任务繁重，此时就会触发 ksoftirqd 线程 。
- **执行时机**：内核选择在几个特殊时机进行软中断处理 。在中断处理程序返回时是常见的软中断触发时机，此时内核会检查是否有待处理的软中断，若有且负载较高，就可能唤醒 ksoftirqd 线程 。

| 特性            | tasklet              | 软中断                       |
| --------------- | -------------------- | ---------------------------- |
| 执行时机        | 软中断上下文         | 软中断上下文                 |
| 静态 / 动态创建 | 支持静态和动态       | 仅支持静态（编译期注册）     |
| 并发限制        | 同类型不能并发       | 同类型可在多 CPU 并发执行    |
| 锁保护要求      | 较低（同类型无需锁） | 较高（需严格保护共享数据）   |
| 应用场景        | 大多数下半部处理     | 高性能场景（如网络、定时器） |

这个并发是软中断可以在不同的CPU上同时执行，即便是同一类型的软中断，也可以，这个就是中断可以重入，所以数据的相关性要强，但是软中断是查看其的state，这个在执行的时候要判断是否正在执行，若是则跳过这个任务，所以也就是一个tasklet不可能同时在多个处理器上执行，但是可以同时执行不同的tasklet，这个需要看这个软中断的处理机制，所以对数据的锁可以放松，不用考虑内部数据在tasklet机制中同时访问

#### 工作队列

工作队列是 Linux 内核中一种将工作推后执行的机制，与软中断、tasklet 等不同，它把工作交由内核线程执行，运行于进程上下文，允许任务重新调度和睡眠 。适用于需要睡眠、获取大量内存、获取信号量或执行阻塞式 I/O 操作的任务 。

##### 工作队列实现

###### 核心数据结构

1. `workqueue_struct`：
   1. 是工作队列子系统的核心数据结构，定义于`kernel/workqueue.c` 。它是一个由`cpu_workqueue_struct`组成的数组，数组每项对应系统中的一个处理器 。
   2. 结构体内包含每个 CPU 的工作队列数组`cpu_wq[NR_CPUS]` 、工作列表`list_head list` 、是否为单线程`singlethread` 、是否可冻结`freezable` 、实时标志`rt`等成员 。
   3. 每个工作者线程类型关联一个`workqueue_struct` ，在该结构体中，给每个线程分配一个`cpu_workqueue_struct` ，即每个处理器对应一个 。
2. `cpu_workqueue_struct`：
   1. 定义于`kernel/workqueue.c` ，包含用于保护结构的自旋锁`spinlock_t lock` 、工作列表`list_head worklist` 、等待队列头`wait_queue_head_t more_work` 、当前工作项指针`struct work_struct *current_struct` 、关联的工作队列结构指针`struct workqueue_struct *wq` 、关联线程`task_t *thread`等成员 。
   2. 每个处理器对应一个该结构体，每个工作者线程对应一个`cpu_workqueue_struct` ，工作者线程通过它管理和执行分配的工作 。
3. `work_struct`：
   1. 定义于`<linux/workqueue.h>` ，用于表示工作项 。包含原子长整型数据`atomic_long_t data` 、链表头`struct list_head entry` 、工作函数指针`work_func_t func`等成员 。
   2. 这些结构体被连接成链表，每个处理器上的每种类型队列都对应这样一个链表 。工作者线程被唤醒时，会执行链表上的所有工作，工作完成后将对应的`work_struct`对象从链表上移除 。

工作者线程位于最高层，系统允许存在多种类型的工作者线程 。对于指定类型，每个 CPU 上都有一个该类型的工作者线程 。默认情况下内核只有`events`类型的工作者线程 ，每个工作者线程由一个`cpu_workqueue_struct`结构体表示 。

```C
/*
 * 外部可见的工作队列抽象是
 * 由每个CPU的工作队列组成的数组
 */
struct workqueue_struct {
    struct cpu_workqueue_struct cpu_wq[NR_CPUS];
    struct list_head list;
    const char *name;
    int singlethread;
    int freezeable;
    int rt;
};

struct cpu_workqueue_struct {
    spinlock_t lock;          /* 锁保护这种结构 */
    struct list_head worklist;  /* 工作列表 */
    wait_queue_head_t more_work;
    struct work_struct*current_struct;
    struct workqueue_struct *wq;  /* 关联工作队列结构 */
    task_t *thread;            /* 关联线程 */
};

struct work_struct {
    atomic_long_t data;
    struct list_head entry;
    work_func_t func;
};
```

其实也就是在创建workqueue_struct的时候，会给每个处理器都创建一个cpu_orkqueue_struct结构，这个结构才是真正要执行的工作者线程，上面挂载的真正的任务work_strutc

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-6.png)

#### 选择

- **软中断**：运行于中断上下文，提供的执行序列化保障最少，多个相同类别软中断可在不同处理器上同时执行。适用于代码本身多线程化做得好、对时间要求严格且执行频率高的场景，如网络子系统 ，但需严格保障共享数据安全 ，且需静态创建。
- **tasklet**：基于软中断实现，运行于中断上下文，同类型 tasklet 不能同时执行。接口简单，实现相对容易，是较常用的选择。在代码多线程化考虑不充分时，比软中断更合适 ，多数驱动程序开发者应优先选 tasklet ，除非要在多处理器上并发运行且做好准备工作，才考虑软中断。
- **工作队列**：靠内核线程实现，运行于进程上下文，允许任务重新调度和睡眠。若任务需推后到进程上下文执行，特别是有睡眠需求时，工作队列是唯一选择 。其使用简单，默认的 events 队列使用方便，虽可能引入开销，但能满足大部分情况需求 。（如果进程上下文不是必须的，tasklet更合适，因为工作队列的开销比较大）

| 下半部   | 上下文 | 顺序执行保障                   |
| -------- | ------ | ------------------------------ |
| 软中断   | 中断   | 没有                           |
| tasklet  | 中断   | 同类型不能同时执行             |
| 工作队列 | 进程   | 没有（和进程上下文一样被调度） |

# 定时器和时间

## jiffies

- **类型与溢出**：`jiffies` 是无符号长整数（`unsigned long`），32 位体系结构下为 32 位，在 100HZ 和 1000HZ 时钟频率下分别约 497 天、49.7 天溢出；64 位体系结构下为 64 位，极难溢出。因性能、历史及内核代码兼容性，内核开发者希望其保持 `unsigned long` 类型。
- **变量定义**：`jiffies` 定义为 `extern unsigned long volatile jiffies;` ，`jiffies_64` 定义为 `extern u64 jiffies_64;` 。`ld(1)` 脚本用于连接主内核映像，用 `jiffies_64` 变量初值覆盖 `jiffies` 变量，`jiffies` 取 `jiffies_64` 的低 32 位。多数代码使用 `jiffies` 关注低 32 位，时间管理代码用 64 位防溢出。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-7.png)

- **访问方式**：多数代码通过 `jiffies` 读低 32 位，可通过 `get_jiffies_64()` 函数读取 64 位值，但需求少。64 位体系结构下，`jiffies_64` 和 `jiffies` 指同一变量，代码可读 `jiffies` 或调用 `get_jiffies_64()` ，作用相同。

也就是说在32位机上，这个数据是32位的，用u64来描述jiffies_64，再取低32位为jiffies，在64位上两个是一样。

### 回绕

```C
#define time_after(unknown,known)  ((long)(known) - (long)(unknown)<0)
#define time_before(unknown,known) ((long)(unknown) - (long)(known)<0)
#define time_after_eq(unknown,known) ((long)(unknown) - (long)(known)>=0)
#define time_before_eq(unknown,known) ((long)(known) - (long)(unknown)>=0)
```

这个是用来比对时间是否超时的宏，参数都是`jiffies`类型数据，这个会避免`jiffies`溢出导致回绕产生的问题

## 硬时钟和定时器

### 实时时钟

也就是`RTC`时钟，这个在系统掉电后也会使用主板的微型电池来保持系统计时，在系统启动的时候会读取这个时间来初始化墙上时钟（也就是现实时间，`xtime`变量），之后就不会再读取这个变量，但是有些架构会周期的将当前时间回写到`RTC`中

### 系统定时器

系统定时器是内核定时机制的关键，不同体系结构实现有别，但都旨在提供周期性触发中断机制，实现方式有电子晶振分频、使用衰减测量器（设置初值以固定频率递减，减为零触发中断 ）等，效果相同。

x86 体系结构主要采用可编程中断时钟（PIT），PIT 在 PC 机常见，自 DOS 时代就作为时钟中断源。内核启动时对其编程初始化，使其以 HZ / 秒频率产生时钟中断（中断 0 ），虽 PIT 简单功能有限但能满足需求，此外 x86 体系结构还有本地 APIC 时钟、时间戳计数（TSC）等时钟资源。

## 时钟中断处理程序

时钟中断处理程序分体系结构相关和无关两部分。体系结构相关例程注册到内核，作为系统定时器中断处理程序，在时钟中断产生时运行。

### 体系结构相关部分工作

- 获取`xtime_lock`锁，保护对`jiffies_64`和墙上时间`xtime`的访问。
- 应答或重新设置系统时钟。
- 用墙上时间周期性更新实时时钟。
- 调用体系结构无关的时钟例程`tick_periodic()` 。

### 体系结构无关部分工作（`tick_periodic()` ）

- 给`jiffies_64`变量加 1 。
- 更新资源消耗统计，如当前进程的系统时间和用户时间。
- 执行到期的动态定时器。
- 执行`scheduler_tick()` 函数。
- 更新墙上时间（存于`xtime`变量 ）。
- 计算平均负载值。

### 相关函数

- **`do_timer()`**：对`jiffies_64` 实际增加操作，更新墙上时间，更新系统平均负载统计值。
- **`update_process_times()`**：更新进程所耗费的各种节拍数，根据处理器模式区分是在用户空间还是内核空间 。
- **`account_process_tick()`**：对进程时间进行实质性更新，依处理器模式分类统计时间。
- **`run_local_timers()`**：标记软中断处理到期定时器。
- **`scheduler_tick()`**：减少当前运行进程时间计数值，必要时设置`need_resched`标志，在 SMP 机器中平衡处理器运行队列。

## 实际时间（xtime）

实际时间定义在`kernel/time/timekeeping.c`文件中

```C
// 定义timespec结构体
struct timespec{
    __kernel_time_t tv_sec;   // 秒
    long tv_nsec;             // 纳秒
};
struct timespec xtime;
```

包含`__kernel_time_t tv_sec`（记录自 1970 年 1 月 1 日（UTC）以来的秒数 ）和`long tv_nsec`（记录上一秒开始经过的纳秒数 ） 。

### 读写操作

- **锁机制**：读写`xtime`需使用`xtime_lock` ，它是 seqlock 锁而非普通自旋锁。
- **更新操作**：更新`xtime` 先申请 seqlock 锁，用`write_seqlock(&xtime_lock)` ，更新完后用`write_sequnlock(&xtime_lock)` 释放 。
- **读取操作**：读取时用`read_seqbegin()` 和`read_seqretry()` 函数。通过循环读取，若读取期间有写操作（时钟中断处理程序更新`xtime` ），`read_seqretry()` 返回无效序列号，循环继续等待，直到确认无写操作介入 。

```C
// 更新xtime相关操作
write_seqlock(&xtime_lock);
/* 更新xtime... */
write_sequnlock(&xtime_lock);

// 读取xtime操作
unsigned long seq;
do {
    unsigned long lost;
    seq = read_seqbegin(&xtime_lock);
    usec = timer->get_offset();
    lost = jiffies - wall_jiffies;
    if (lost)
        usec += lost * (1000000 / HZ);
    sec = xtime.tv_sec;
    usec += (xtime.tv_nsec / 1000);
} while (read_seqretry(&xtime_lock, seq));
```

## 定时器

内核使用的定时器是挂载在`TIMER_SOFTIRQ`这个软中断上，在每个定时中断到来后会标记这个软中断，后面在定时中断结束后会执行这个软中断，判断定时器等待队列中超时的定时器任务，开始执行这些超时的任务（定时器的`jiffies`大于等于当前的`jiffies`）

## 延迟执行

### 忙等待（jiffies）

**原理**：通过忙循环不断检查时钟节拍数，直到达到期望的延迟时间。仅适用于延迟时间是节拍整数倍或精确率要求不高的情况。

```C
// 延迟10个节拍
unsigned long timeout = jiffies + 10; 
while (time_before(jiffies, timeout))
    ;
// 延迟2秒（假设HZ = 1000）
unsigned long delay = jiffies + 2 * HZ; 
while (time_before(jiffies, delay))
    ;
// 改进版，允许内核重新调度执行其他任务
unsigned long delay = jiffies + 5 * HZ; 
while (time_before(jiffies, delay))
    cond_resched();
```

`cond_resched()`函数用于调度新程序运行，需先设置`need_resched`标志，即系统存在更重要任务待运行时才起作用。因调用调度程序，只能用于进程上下文，不能在中断上下文使用 。

代码等待时处理器只能空转，效率低，不适合系统其他部分，持有锁或禁止中断时也不应使用。同时，为保证`jiffies`变量每次循环能重新从主存读取（因`jiffies`会随时钟中断变化 ），该变量用`volatile`关键字标记。

### 短延迟（指令循环次数）

内核提供`udelay()`、`ndelay()`和`mdelay()`函数处理更短、更精确延迟，定义在`<linux/delay.h>`和`<asm/delay.h>`中，不依赖`jiffies` 。`udelay()`利用忙循环，根据处理器 BogoMIPS 值（记录在`loops_per_jiffy` ，可从`/proc/cpuinfo`读取 ）计算循环次数实现延迟指定微秒数；`mdelay()`通过`udelay()`实现延迟指定毫秒数 。

BogoMIPS 这个就是处理器的每秒执行的指令的次数，用来衡量处理器的运行速度，这个不是为了展示性能，主要是给`udelay()`使用，这个实际记录的是处理器在给定时间内忙循环执行的次数，存在`loops_per_jiffy`，内核在启动的时候使用`calibrate_delay()`来计算这个数据，延迟循环函数根据这个数据来计算出要循环多少次达到延时

**使用限制**：`udelay()`一般用于小于 1ms 的延迟，大延迟可能溢出；`mdelay()`用于较长延迟，但除非必要，长延迟不建议使用这两个函数，因为持锁或禁止中断时使用会影响系统响应时间和性能。

### `schedule_timeout()`（jiffies）

让需要延迟执行的任务睡眠到指定延迟时间耗尽后再重新运行，是内核定时器的应用。通过设置任务状态为可中断睡眠状态（`TASK_INTERRUPTIBLE` ）或不可中断睡眠状态（`TASK_UNINTERRUPTIBLE` ），利用定时器在超时后唤醒任务并放回运行队列。

```C
// 将任务设置为可中断睡眠状态
set_current_state(TASK_INTERRUPTIBLE); 
// 小睡一会儿，s秒后唤醒
schedule_timeout(s * HZ); 

signed long schedule_timeout(signed long timeout)
{
    timer_t timer;
    unsigned long expire;

    switch (timeout)
    {
    case MAX_SCHEDULE_TIMEOUT:
        schedule();
        goto out;
    default:
        if (timeout < 0)
        {
            printk(KERN_ERR "schedule_timeout: wrong timeout "
                   "value %lx from %p\n", timeout,
                   __builtin_return_address(0));
            current->state = TASK_RUNNING;
            goto out;
        }
    }

    expire = timeout + jiffies;

    init_timer(&timer);
    timer.expires = expire;
    timer.data = (unsigned long)current;
    timer.function = process_timeout;

    add_timer(&timer);
    schedule();
    del_timer_sync(&timer);

    timeout = expire - jiffies;

out:
    return timeout < 0? 0 : timeout;
}

void process_timeout(unsigned long data)
{
    wake_up_process((task_t *)data);
}
```

**使用注意事项**：调用代码必须处于进程上下文且不能持有锁，因为该函数需调用调度程序。任务可能因信号提前唤醒，此时定时器撤销，`process_timeout()`返回剩余时间。特殊情况（如`MAX_SCHEDULE_TIMEOUT` ）用于检查任务是否无限期睡眠 。也可用于设置超时时间，让进程在等待队列上睡眠，等待特定事件发生，同时结合`wake_up()`函数唤醒睡眠任务 。

# 同步机制

## 原子操作

| 原子整数操作                                  | 描述                                                 |
| --------------------------------------------- | ---------------------------------------------------- |
| `ATOMIC_INIT(int i)`                          | 在声明一个`atomic_t`变量时，将它初始化为`i`          |
| `int atomic_read(atomic_t *v)`                | 原子地读取整数变量`v`                                |
| `void atomic_set(atomic_t *v, int i)`         | 原子地设置`v`值为`i`                                 |
| `void atomic_add(int i, atomic_t *v)`         | 原子地给`v`加`i`                                     |
| `void atomic_sub(int i, atomic_t *v)`         | 原子地从`v`减`i`                                     |
| `void atomic_inc(atomic_t *v)`                | 原子地给`v`加 1                                      |
| `void atomic_dec(atomic_t *v)`                | 原子地从`v`减 1                                      |
| `int atomic_sub_and_test(int i, atomic_t *v)` | 原子地从`v`减`i`，如果结果等于 0，返回真；否则返回假 |
| `int atomic_add_negative(int i, atomic_t *v)` | 原子地给`v`加`i`，如果结果是负数，返回真；否则返回假 |
| `int atomic_add_return(int i, atomic_t *v)`   | 原子地给`v`加`i`，且返回结果                         |
| `int atomic_sub_return(int i, atomic_t *v)`   | 原子地从`v`减`i`，且返回结果                         |
| `int atomic_inc_return(int i, atomic_t *v)`   | 原子地给`v`加 1，且返回结果                          |
| `int atomic_dec_return(int i, atomic_t *v)`   | 原子地从`v`减 1，且返回结果                          |
| `int atomic_dec_and_test(atomic_t *v)`        | 原子地从`v`减 1，如果结果等于 0，返回真；否则返回假  |
| `int atomic_inc_and_test(atomic_t *v)`        | 原子地给`v`加 1，如果结果等于 0，返回真；否则返回假  |

| 原子位操作                                    | 描述                                               |
| --------------------------------------------- | -------------------------------------------------- |
| `void set_bit(int nr, void *addr)`            | 原子地设置`addr`所指对象的第`nr`位                 |
| `void clear_bit(int nr, void *addr)`          | 原子地清空`addr`所指对象的第`nr`位                 |
| `void change_bit(int nr, void *addr)`         | 原子地翻转`addr`所指对象的第`nr`位                 |
| `int test_and_set_bit(int nr, void *addr)`    | 原子地设置`addr`所指对象的第`nr`位，并返回原先的值 |
| `int test_and_clear_bit(int nr, void *addr)`  | 原子地清空`addr`所指对象的第`nr`位，并返回原先的值 |
| `int test_and_change_bit(int nr, void *addr)` | 原子地翻转`addr`所指对象的第`nr`位，并返回原先的值 |
| `int test_bit(int nr, void *addr)`            | 原子地返回`addr`所指对象的第`nr`位                 |

原子位函数前加`__`如`__set_bit`，这个位操作就不原子，但是这个的执行要比原子快

## 自旋锁（不会睡眠）

| 方法                     | 描述                                                 |
| ------------------------ | ---------------------------------------------------- |
| spin_lock()              | 获取指定的自旋锁                                     |
| spin_lock_irq()          | 禁止本地中断并获取指定的锁                           |
| spin_lock_irqsave()      | 保存本地中断的当前状态，禁止本地中断，并获取指定的锁 |
| spin_unlock()            | 释放指定的锁                                         |
| spin_unlock_irq()        | 释放指定的锁，并激活本地中断                         |
| spin_unlock_irqrestore() | 释放指定的锁，并让本地中断恢复到以前状态             |
| spin_lock_init()         | 动态初始化指定的 spinlock_t                          |
| spin_trylock()           | 试图获取指定的锁，如果未获取，则返回非 0             |
| spin_is_locked()         | 如果指定的锁当前正在被获取，则返回非 0，否则返回 0   |

### 锁和下半部

- `spin_lock_bh()`用于获取指定锁并禁止所有下半部执行，`spin_unlock_bh()`执行相反操作。当下半部和进程上下文共享数据，加锁同时需禁止下半部执行；中断处理程序和下半部共享数据时，获取锁同时要禁止中断。
- 同类 tasklet 不会同时运行，其共享数据无需保护；不同种类 tasklet 共享数据，访问前需获普通自旋锁，无需禁止下半部。
- 同种软中断可能在多处理器上同时运行，共享数据需互斥保护；同一处理器上软中断不会相互抢占，不必禁止下半部。

### 读写锁

| 方法                      | 描述                                               |
| ------------------------- | -------------------------------------------------- |
| read_lock()               | 获得指定的读锁                                     |
| read_lock_irq()           | 禁止本地中断并获得指定读锁                         |
| read_lock_irqsave()       | 存储本地中断的当前状态，禁止本地中断并获得指定读锁 |
| read_unlock()             | 释放指定的读锁                                     |
| read_unlock_irq()         | 释放指定的读锁并激活本地中断                       |
| read_unlock_irqrestore()  | 释放指定的读锁并将本地中断恢复到指定的前状态       |
| write_lock()              | 获得指定的写锁                                     |
| write_lock_irq()          | 禁止本地中断并获得指定写锁                         |
| write_lock_irqsave()      | 存储本地中断的当前状态，禁止本地中断并获得指定写锁 |
| write_unlock()            | 释放指定的写锁                                     |
| write_unlock_irq()        | 释放指定的写锁并激活本地中断                       |
| write_unlock_irqrestore() | 释放指定的写锁并将本地中断恢复到指定的前状态       |
| write_trylock()           | 试图获得指定的写锁；如果写锁不可用，返回非 0 值    |
| rwlock_init()             | 初始化指定的 r wlock_t                             |

## 信号量（睡眠锁）

| 方法                                   | 描述                                                         |
| -------------------------------------- | ------------------------------------------------------------ |
| sema_init(struct semaphore *,int)      | 以指定的计数值初始化动态创建的信号量                         |
| init_MUTEX(struct semaphore *)         | 以计数值 1 初始化动态创建的信号量                            |
| init_MUTEX_LOCKED(struct semaphore *)  | 以计数值 0 初始化动态创建的信号量（初始为加锁状态）          |
| down_interruptible(struct semaphore *) | 以试图获得指定的信号量，如果信号量已被争用，则进入可中断睡眠状态 |
| down(struct semaphore *)               | 以试图获得指定的信号量，如果信号量已被争用，则进入不可中断睡眠状态 |
| down_trylock(struct semaphore *)       | 以试图获得指定的信号量，如果信号量已被争用，则立刻返回非 0 值 |
| up(struct semaphore *)                 | 以释放指定的信号量，如果睡眠队列不空，则唤醒其中一个任务     |

### 读写信号量

读 - 写信号量属于互斥信号量，引用计数为 1 ，仅对写者互斥，对读者不互斥。无写者时，持有读锁的读者数量不受限制；仅有一个写者可在无读者时获取写锁。读 - 写锁睡眠不会被信号打断，仅有一个版本的`down()`操作 。

## 互斥体（mutex）

实现 “互相排斥” 同步的简单形式，用于保护共享的易变代码（如全局或静态数据），避免多个进程或线程同时进入临界区 。

| 方法                            | 描述                                                         |
| ------------------------------- | ------------------------------------------------------------ |
| mutex_lock(struct mutex *)      | 为指定的 mutex 上锁，如果锁不可用则睡眠                      |
| mutex_unlock(struct mutex *)    | 为指定的 mutex 解锁                                          |
| mutex_trylock(struct mutex *)   | 试图获取指定的 mutex，如果成功则返回 1；否则锁被获取，返回值是 0 |
| mutex_is_locked(struct mutex *) | 如果锁已被使用，则返回 1；否则返回 0                         |

### 特点

- 任何时刻中只有一个任务可以持有 mutex，也就是说，mutex 的使用计数永远是 1。
- 给 mutex 上锁者必须负责给其再解锁 —— 你不能在一个上下文中锁定一个 mutex，而在另一个上下文中给它解锁。这个限制使得 mutex 不适合内核同用户空间复杂的同步场景。最常使用的方式是：在同一上下文中上锁和解锁。
- 递归地上锁和解锁是不允许的。也就是说，你不能递归地持有同一个锁，同样你也不能再去解锁一个已经被解开的 mutex。
- 当持有一个 mutex 时，进程不可以退出。
- mutex 不能在中断或者下半部中使用，即使使用 mutex_trylock () 也不行。
- mutex 只能通过官方 API 管理：它只能使用上节中描述的方法初始化，不可被拷贝、手动初始化或者重复初始化。

| 需求             | 建议的加锁方法 |
| ---------------- | -------------- |
| 低开销加锁       | 优先使用自旋锁 |
| 短期锁定         | 优先使用自旋锁 |
| 长期加锁         | 优先使用互斥体 |
| 中断上下文中加锁 | 使用自旋锁     |
| 持有锁需要睡眠   | 使用互斥体     |

### 互斥体、互斥信号量和计数为1的信号量

#### 概念

- **互斥体**：是一种用于实现线程或进程间互斥访问共享资源的机制，确保同一时刻只有一个执行单元（线程或进程）能够访问特定的资源或代码段。
- **互斥信号量**：是一种特殊的信号量，它在信号量的基础上增加了互斥的特性，用于控制对共享资源的互斥访问，同一时刻只允许一个进程或线程访问资源。
- **计数为 1 的信号量**：是信号量的一种特殊情况，信号量的计数值被初始化为 1，用于控制对单个资源的访问，当计数值为 1 时表示资源可用，为 0 时表示资源被占用。

#### 特性

- 所有权
  - **互斥体**：具有明确的所有权概念，只有获取互斥体的线程或进程才能释放它，这能有效防止其他线程或进程误释放锁，保证了操作的安全性和有序性。
  - **互斥信号量**：通常也具有所有权特性，与互斥体类似，只有获得互斥信号量的进程或线程才有权利释放它。
  - **计数为 1 的信号量**：一般没有所有权的概念，任何线程或进程都可以执行释放操作，这在某些情况下可能导致误操作，但也为一些特殊的同步需求提供了便利。
- 优先级处理
  - **互斥体**：在很多操作系统中支持优先级继承机制。当高优先级线程等待低优先级线程持有的互斥体时，低优先级线程的优先级会被提升到与高优先级线程相同，以防止高优先级线程长时间等待，减少优先级翻转的影响。
  - **互斥信号量**：部分系统中也支持类似的优先级继承或相关的优先级保护机制，以确保互斥访问的公平性和高效性，避免高优先级任务被低优先级任务长时间阻塞。
  - **计数为 1 的信号量**：通常不具备优先级继承机制，线程或进程等待信号量时按照先来先服务的原则进行排队，可能会出现优先级翻转问题，即高优先级进程或线程因等待低优先级进程或线程释放信号量而被阻塞。

#### 实现原理

- **互斥体**：通常基于简单的数据结构实现，如在 Linux 系统中，常基于 futex（快速用户态互斥锁）机制。在非竞态情况下，futex 操作完全在用户态下进行，只有在竞态条件下才会调用内核态的系统调用来仲裁，这样可以减少用户态与内核态之间的切换开销，提高效率。互斥体的操作主要围绕获取（acquire）和释放（release），通过检查持有线程的身份来保证操作的安全性。
- **互斥信号量**：底层实现常涉及等待队列和自旋锁等机制，以确保对内部状态的原子性操作。当一个进程或线程获取互斥信号量时，如果信号量已经被占用，那么该进程或线程会被放入等待队列中睡眠，直到持有信号量的进程或线程释放它。自旋锁则用于在短时间内保护对信号量状态的修改，避免多个进程或线程同时修改信号量的值。
- **计数为 1 的信号量**：本质是一个计数器为 1 的信号量，通过 P 操作（通常对应`down`等操作，使信号量值减 1）和 V 操作（通常对应`up`等操作，使信号量值加 1）来控制资源的访问。当信号量的值为 1 时，表示资源可用，进程或线程可以执行 P 操作获取信号量并进入临界区；当信号量的值为 0 时，表示资源被占用，执行 P 操作的进程或线程会被加入等待队列睡眠，直到有其他进程或线程执行 V 操作释放信号量将其唤醒。

#### 应用场景

- **互斥体**：主要用于对共享资源的严格互斥访问，强调一次只有一个线程或进程能够进入临界区操作资源，常用于保护共享数据结构、硬件外设等，防止数据竞争，确保数据的一致性和完整性。
- **互斥信号量**：与互斥体类似，主要用于实现进程或线程之间对共享资源的互斥访问，但在一些复杂的系统中，互斥信号量可能会提供更多的功能和灵活性，例如可以在不同的进程之间进行同步和互斥控制，适用于多个进程或线程需要访问同一共享资源的场景。
- **计数为 1 的信号量**：除了实现互斥功能外，还可用于更灵活的任务间同步场景。例如，在一些需要任务按特定顺序执行或进行事件通知的场景中，计数为 1 的信号量可以作为简单的同步机制使用，通过 P 和 V 操作来协调不同任务之间的执行顺序和时机。

## 顺序锁

依靠序列计数器实现读写共享数据。写操作时获取锁并增加序列值；读操作前后读取序列号，若相同且为偶数，说明读过程无写操作打断。

适用于多读者、少数写者场景，写者优先且不允许读者使写者饥饿，也适用于简单数据结构（如无法用原子量的情况 ） 。

### 读写操作对序列的影响

- **读操作**：读操作不会增加顺序锁的序列值 。在进行读操作时，读取顺序值并记录，读完临界区内容后再次读取顺序值并比较 。若两次值相等，读操作有效；若不等，说明读的过程中有写操作，读操作无效，需重试 。
- **写操作**：写操作会增加顺序锁的序列值 。每次进入写临界区，先上锁自旋锁，然后增加顺序值；离开写临界区时，会再次增加顺序值 。也就是说，一次完整写操作会使顺序值增加两次，增加次数为偶数 。

### 奇数时的含义

当顺序值为奇数时，表明正在进行写操作 。写者在进入写临界区后，将顺序值加 1 ，此时顺序值变为奇数，这就向读者表明有写操作正在进行 。写者完成写操作离开临界区时，再次增加顺序值，使其变回偶数，代表写操作完成，读者可以继续进行有效读操作 。

如内核的`jiffies`的变量：

```C
u64 get_jiffies_64(void)
{
    unsigned long seq;
    u64 ret;

    do {
        seq = read_seqbegin(&xtime_lock);
        ret = jiffies_64;
    } while (read_seqretry(&xtime_lock, seq));
    return ret;
}

write_seqlock(&xtime_lock);
jiffies_64 += 1;
write_sequnlock(&xtime_lock);
```

## 禁止抢占

也就是禁止内核在这个区域内发生这个高进程抢占低进程的现象

| 函数                        | 描述                                                         |
| --------------------------- | ------------------------------------------------------------ |
| preempt_disable()           | 增加抢占计数值，从而禁止内核抢占                             |
| preempt_enable()            | 减少抢占计数，并当该值降为 0 时检查和执行被挂起的需调度的任务 |
| preempt_enable_no_resched() | 激活内核抢占但不再检查任何被挂起的需调度任务                 |
| preempt_count()             | 返回抢占计数                                                 |

可以直接是使用下面的函数，作用是相同的：

```C
int cpu;
/* 禁止内核抢占,并将CPU设置为当前处理器 */
cpu = get_cpu();
/* 对每个处理器的数据进行操作...*/
/* 再给予内核抢占性,"CPU"可改变故它不再有效 */
put_cpu();
```

## 顺序和屏障

编译器和处理器在执行的时候为了提升效率可能会对读写指令进行重新排序，但是对于有明确数据相关性的数据不会进行排序，但是没有明确数据相关的指令会排序，内存屏障机制就是可以使得部分数据分两半进行重排，也就是在一些存取指令前设置屏障保证在这个屏障前的存取执行完毕。

| 屏障                       | 描述                                                         |
| -------------------------- | ------------------------------------------------------------ |
| rmb()                      | 阻止跨越屏障的载入动作发生重排序                             |
| read_barrier_depends()     | 阻止跨越屏障的具有数据依赖关系的载入动作重排序               |
| wmb()                      | 阻止跨越屏障的存储动作发生重排序                             |
| mb()                       | 阻止跨越屏障的载入和存储动作重新排序                         |
| smp_rmb()                  | 在 SMP 上提供 rmb () 功能，在 UP 上提供 barrier () 功能      |
| smp_read_barrier_depends() | 在 SMP 上提供 read_barrier_depends () 功能，在 UP 上提供 barrier () 功能 |
| smp_wmb()                  | 在 SMP 上提供 wmb () 功能，在 UP 上提供 barrier () 功能      |
| smp_mb()                   | 在 SMP 上提供 mb () 功能，在 UP 上提供 barrier () 功能       |
| barrier()                  | 阻止编译器对载入或存储操作进行优化                           |

例子解释：

假设初始时 `a = 1`，`b = 2` 。有线程 1 和线程 2 两个线程，操作如下：

- 线程 1：
  - `a = 3;` ：尝试将 `a` 的值更新为 `3` 。
  - （可能存在指令重排或并发问题）
  - `b = 4;` ：尝试将 `b` 的值更新为 `4` 。
- 线程 2：
  - `c = b;` ：尝试读取 `b` 的值并赋给 `c` 。
  - （可能存在指令重排或并发问题）
  - `rmb();` ：读内存屏障（这里先不考虑它此时没起作用的情况 ）
  - `d = a;` ：尝试读取 `a` 的值并赋给 `d` 。

由于编译器和处理器可能对指令进行重排，并且两个线程是并发执行的，可能出现这样的情况：线程 2 在执行 `c = b;` 时，读到了线程 1 更新后的 `b` 的值 `4` （因为线程 1 可能已经执行了 `b = 4;` ） ，但在线程 2 执行 `d = a;` 时，读到的却是线程 1 还未更新的 `a` 的值 `1` （因为线程 1 的 `a = 3;` 可能还没执行 ，或者虽然执行了但由于指令重排、缓存等原因，线程 2 没读到更新后的值 ） 。所以可能出现 `c = 4` ，`d = 1` 这种不符合预期的结果

使用内存屏障 `mb()` 和 `rmb()` 的作用

- **线程 1 中的** **`mb()`** **（通用****内存****屏障）** ： `mb()` 确保了在它之前的写操作（这里是 `a = 3;` ）一定在它之后的写操作（这里是 `b = 4;` ）之前完成。也就是说，当线程 1 执行到 `mb()` 时，它保证了 `a` 的值一定已经更新为 `3` ，然后才会去执行 `b = 4;` 。
- **线程 2 中的** **`rmb()`** **（读****内存****屏障）** ： `rmb()` 确保了在它之前的读操作（这里是 `c = b;` ）一定在它之后的读操作（这里是 `d = a;` ）之前完成。并且，它还能保证在执行 `d = a;` 时，读到的 `a` 的值是最新的（前提是其他线程已经完成了对 `a` 的更新 ） 。

综合起来，使用了 `mb()` 和 `rmb()` 后，就能保证线程 2 在读取 `c` 和 `d` 的值时，读到的是符合预期的更新后的值 。即如果 `c` 读到了 `b` 的新值 `4` ，那么 `d` 一定能读到 `a` 的新值 `3` ，避免了出现 `c` 是新值而 `d` 是旧值这种错误情况 。

内存屏障在多线程并发读写内存的场景下，通过规定操作顺序，保证了不同线程之间数据读写的一致性和正确性 。

# 内存管理

## 内存管理概念

内存页分配函数

| 标志                             | 描述                                                 |
| -------------------------------- | ---------------------------------------------------- |
| alloc_page(gfp_mask)             | 只分配一页，返回指向页结构的指针                     |
| alloc_pages(gfp_mask,order)      | 分配2*or****d****er*个页，返回指向第一页页结构的指针 |
| __get_free_page(gfp_mask)        | 只分配一页，返回指向其逻辑地址的指针                 |
| __get_free_pages(gfp_mask,order) | 分配2*or****d****er*页，返回指向第一页逻辑地址的指针 |
| get_zeroed_page(gfp_mask)        | 只分配一页，让其内容填充 0，返回指向其逻辑地址的指针 |

### gfp_mask标志

#### 行为修饰符

| 标志          | 描述                                                         |
| ------------- | ------------------------------------------------------------ |
| __GFP_WAIT    | 分配器可以睡眠                                               |
| __GFP_HIGH    | 分配器可以访问紧急事件缓冲池                                 |
| __GFP_IO      | 分配器可以启动磁盘 I/O                                       |
| __GFP_FS      | 分配器可以启动文件系统 I/O                                   |
| __GFP_COLD    | 分配器应该使用高速缓存中快要淘汰出去的页                     |
| __GFP_NOWARN  | 分配器将不打印失败警告                                       |
| __GFP_REPEAT  | 分配器在分配失败时重复进行分配，但是这次分配还存在失败的可能 |
| __GFP_NOFALL  | 分配器将无限地重复进行分配。分配不能失败                     |
| __GFP_NORETRY | 分配器在分配失败时绝不会重新分配                             |
| __GFP_NO_GROW | 由 slab 层内部使用                                           |
| __GFP_COMP    | 添加混合页元数据，在 hugetlb 的代码内部使用                  |

#### 区修饰符

区修饰符的功能在于明确内存区的分配来源。正常情况下，内存分配可起始于任何区，但内核为确保其他区在有需求时存有充足的空闲页，会优先从 ZONE_NORMAL 区开始进行内存分配操作。

| 标志          | 描述                                |
| ------------- | ----------------------------------- |
| __GFP_DMA     | 从 ZONE_DMA 分配                    |
| __GFP_DMA32   | 只在 ZONE_DMA32 分配                |
| __GFP_HIGHMEM | 从 ZONE_HIGHMEM 或 ZONE_NORMAL 分配 |

- **__GFP_DMA**：该标志强制内核从 ZONE_DMA 区进行内存分配。当使用此标志时，意味着对用于直接内存访问（DMA）的内存有特定需求，确保相关内存来自 ZONE_DMA 区。
- **__GFP_DMA32**：限定内存分配仅在 ZONE_DMA32 区进行，是一种更为特定区域的分配限定。
- **__GFP_HIGHMEM**：使用此标志时，内核优先从 ZONE_HIGHMEM 区分配内存，若 ZONE_HIGHMEM 区资源不足，则会从 ZONE_NORMAL 区分配。适用于对高端内存有使用需求的场景。

get_free_pages () 和 kalloc () 这两个函数在内存分配时存在限制，由于它们返回的是逻辑地址，且所分配内存可能尚未映射到内核虚拟地址空间，甚至可能不存在逻辑地址，所以不能为其指定 ZONE_HIGHMEM 修饰符。alloc_pages () 函数具备分配高端内存（ZONE_HIGHMEM）的能力。

#### 类型

| 标志         | 描述                                                         |
| ------------ | ------------------------------------------------------------ |
| GFP_ATOMIC   | 这个标志用在中断处理程序、下半部、持有自旋锁以及其他不能睡眠的地方 |
| GFP_NOWAIT   | 与 GFP_ATOMIC 类似，不同之处在于，调用不会退给紧急内存池。这就增加了内存分配失败的可能性 |
| GFP_NOIO     | 这种分配可以阻塞，但不会启动磁盘 I/O。这个标志在不能引发更多磁盘 I/O 时能阻塞 I/O 代码，这可能导致令人不愉快的递归 |
| GFP_NOFS     | 这种分配不能启动阻塞，也可能启动磁盘 I/O，但是不会启动文件系统操作。这个标志在你不能再启动另一个文件系统的操作时，用在文件系统部分的代码中 |
| GFP_KERNEL   | 这是一种常规分配方式，可能会阻塞。这个标志在睡眠安全时用在进程上下文代码中。为了获得调用者所需的内存，内核会尽力而为。这个标志应当是首选标志 |
| GFP_USER     | 这是一种常规分配方式，可能会阻塞。这个标志用于为用户空间进程分配内存时 |
| GFP_HIGHUSER | 这是从 ZONE_HIGHMEM 进行分配，可能会阻塞。这个标志用于为用户空间进程分配内存 |
| GFP_DMA      | 这是从 ZONE_DMA 进行分配。需要获取能供 DMA 使用的内存的设备驱动程序使用这个标志，通常与以上的某个标志组合在一起使用 |

| 标志         | 修饰符标志                                      |
| ------------ | ----------------------------------------------- |
| GFP_ATOMIC   | __GFP_HIGH                                      |
| GFP_NOWAIT   | 0                                               |
| GFP_NOIO     | __GFP_WAIT                                      |
| GFP_NOFS     | (__GFP_WAIT\|__GFP_IO)                          |
| GFP_KERNEL   | (__GFP_WAIT\|__GFP_IO\|__GFP_FS)                |
| GFP_USER     | (__GFP_WAIT\|__GFP_IO\|__GFP_FS)                |
| GFP_HIGHUSER | (__GFP_WAIT\|__GFP_IO\|__GFP_FS\|__GFP_HIGHMEM) |
| GFP_DMA      | __GFP_DMA                                       |

| 情形                            | 相应标志                                                     |
| ------------------------------- | ------------------------------------------------------------ |
| 进程上下文，可以睡眠            | 使用 GFP_KERNEL                                              |
| 进程上下文，不可以睡眠          | 使用 GFP_ATOMIC，在你睡眠之前或之后以 GFP_KERNEL 执行内存分配 |
| 中断处理程序                    | 使用 GFP_ATOMIC                                              |
| 软中断                          | 使用 GFP_ATOMIC                                              |
| tasklet                         | 使用 GFP_ATOMIC                                              |
| 需要用于 DMA 的内存，可以睡眠   | 使用 (GFP_DMA \| GFP_KERNEL)                                 |
| 需要用于 DMA 的内存，不可以睡眠 | 使用 (GFP_DMA \| GFP_ATOMIC)，或在你睡眠之前执行内存分配     |

最常用标志是 GFP_KERNEL 和 GFP_ATOMIC 。GFP_KERNEL 用于可睡眠的进程上下文；GFP_ATOMIC 用于不可睡眠场景。GFP_NOIO 和 GFP_NOFS 用于避免特定操作的低级别块 I/O 或文件系统代码。并给出不同情形下适用标志的列表，强调分配时需检查和处理错误。

### vmalloc kmalloc malloc

#### 函数功能及内存地址特性

- **`malloc`**：用户空间内存分配函数，在 C 标准库中定义。它返回的内存块在进程的虚拟地址空间内是连续的，但并不保证在物理 RAM 中连续 。主要用于为用户空间程序分配内存，满足一般程序运行时对内存的动态需求，比如动态数组、链表节点等内存的分配。
- **`kmalloc`**：内核空间内存分配函数。它确保分配的内存不仅虚拟地址连续，物理地址也连续。这是因为在内核中，很多硬件设备（如 DMA 设备）要求操作的内存物理地址连续，`kmalloc` 分配的内存能满足此类需求，常用于内核中为设备驱动程序分配缓冲区等场景 。
- **`vmalloc`**：同样是内核空间内存分配函数。`vmalloc` 分配的内存虚拟地址是连续的，但物理地址无需连续。它通过分配非连续的物理内存块，再修正页表，将内存映射到逻辑地址空间的连续区域 。适用于为内核中仅需虚拟地址连续的内存需求场景分配内存，如为动态插入内核的模块分配较大内存块 。

#### 性能及使用场景差异

- **`malloc`**：在用户空间使用，性能方面依赖于操作系统的虚拟内存管理机制。在一般的用户程序开发中，对内存物理连续性要求不高时广泛使用，开发过程中无需过多考虑硬件对内存物理地址的特殊要求。
- **`kmalloc`**：由于保证物理地址连续，在分配时可能会受到物理连续内存块数量的限制。但对于硬件设备相关的内存操作，因其能满足物理连续要求，所以性能表现稳定且符合硬件需求。在设备驱动开发中，为与硬件交互的缓冲区分配内存时经常使用 。
- **`vmalloc`**：性能相对较差，因为它需要专门建立页表项来将物理上不连续的页转换为虚拟地址空间上连续的页，这个过程会导致 TLB（Translation Lookaside Buffer，转换后援缓冲器）抖动。仅在需要大块内存且对物理地址连续性无要求，如加载较大的内核模块等特定场景下使用 。

#### 调用限制

- **`malloc`**：在用户空间程序正常流程中调用，只要程序有足够的虚拟内存空间，且符合操作系统的内存管理规则即可调用。
- **`kmalloc`**：可在合适的内核上下文（满足睡眠等条件）中调用，相对来说调用限制较少，但要注意满足内核内存管理的相关规则。
- **`vmalloc`**：函数可能睡眠，因此不能从中断上下文中进行调用，也不能从其他不允许阻塞的情况下进行调用。释放内存使用 `vfree` 函数，同样也不能从中断上下文中调用 。



## slab

内存是按照页进行分配的，但是页对于日常使用来说太大了，所以出现了slab实现小内存的管理。

### 结构组成与工作原理

- **高速缓存与 slab**：将不同对象划分到高速缓存组，每个高速缓存存放特定类型对象，如进程描述符、索引节点对象等。高速缓存由一个或多个 slab 组成，slab 由物理上连续的页构成，一般由一页组成 。slab 有满、部分满、空三种状态，内核分配对象时优先从部分满的 slab 开始，若无则从空的 slab 分配，都无则创建新 slab 。
- **数据结构**：高速缓存用 `kmem_cache` 结构表示，含 `slabs_full`、`slabs_partial` 和 `slabs_empty` 三个链表存放所有 slab 。`slab` 描述符 `struct slab` 用于描述每个 slab，包含链表指针、着色偏移量、已分配对象数等信息 ，描述符可在 slab 外或内分配 。
- **内存分配与释放**：创建新 slab 通过 `kmem_getpages()` 函数，该函数使用 `__get_free_pages()` 为高速缓存分配内存，在与 NUMA 相关代码影响下较复杂，简化版本忽略 NUMA 相关代码 。释放内存调用 `kmem_freepages()` ，最终调用 `free_pages()` ，仅在特定内存紧张或高速缓存显式释放时才调用 。

![image-20250518170108547](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250518170108547.png)

### 实现原理

- **对象分类与缓存**：Slab 将内核中经常使用的小对象按照类型和大小进行分类，为每一类对象创建一个专门的 Slab 缓存。每个缓存由多个 Slab 组成，每个 Slab 由一个或多个物理页组成。例如，对于 `task_struct`、`inode` 等不同类型的结构体，都有各自对应的 Slab 缓存。这样的设计可以将相同类型的对象集中管理，提高分配和释放的效率。
- **Slab** **状态管理**：Slab 有三种状态，分别是满（full）、部分满（partial）和空（empty）。内核在分配对象时，优先从部分满的 Slab 中查找空闲对象。如果没有部分满的 Slab，则选择空 Slab 进行分配。当一个 Slab 中的所有对象都被分配出去，它就变成了满 Slab；而当所有对象都被释放后，Slab 就变为空 Slab。通过这种状态管理，Slab 分配器可以快速定位到适合分配对象的 Slab。
- **内存分配与回收**：当内核请求分配一个小对象时，Slab 分配器首先在对应的 Slab 缓存中查找空闲对象。如果缓存中没有可用的对象，Slab 分配器会向伙伴系统申请分配新的物理页来创建一个新的 Slab。当对象被释放时，Slab 分配器将其标记为空闲，并将其放回原来的 Slab 中，而不是立即将内存归还给伙伴系统。这样，后续的分配请求可以直接从 Slab 中获取空闲对象，减少了内存分配和释放的开销。
- **缓存优化**：Slab 分配器采用了多种缓存优化技术。例如，颜色缓存机制通过对 Slab 进行不同的颜色标记，使不同的 Slab 在内存中的对齐方式不同，从而尽可能均匀地分布在 CPU 缓存中，减少缓存冲突，提高缓存命中率。另外，Slab 分配器还支持对象构造和析构函数，在对象分配和释放时可以自动调用这些函数，对对象进行初始化和清理工作，方便内核对特殊对象的管理。

|                      | Slab                                                         | 伙伴系统                                                     |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **管理对象**         | 侧重于管理内核中频繁使用的小对象，如进程描述符、文件系统的 inode 节点、各种内核数据结构等。 | 主要管理以页为单位的大块连续内存空间，通常用于满足较大内存分配请求，如为内核模块、驱动程序分配较大的缓冲区等。 |
| **分配策略**         | 将内存划分为多个 Slab 缓存，每个缓存针对特定类型和大小的对象。分配对象时，优先从部分满的 Slab 中查找空闲对象，如果没有则从空 Slab 中分配，当缓存中没有可用的 Slab 时，才向伙伴系统请求新的物理页来创建 Slab。 | 按照 2 的幂次方大小来划分和分配内存块。当有内存分配请求时，它会查找最接近请求大小的 2 的幂次方的空闲内存块进行分配，如果没有合适大小的空闲块，就从更大的块中拆分 |
| **内存****碎片处理** | 由于针对特定类型的小对象进行管理，在小对象的分配和释放过程中，通过 Slab 缓存机制可以有效减少内部碎片，但对于 Slab 缓存之间的外部碎片管理相对较弱。 | 通过合并相邻的空闲内存块来减少内存碎片。当内存块被释放时，会检查其相邻的内存块是否为伙伴（大小相同且地址连续），如果是则将它们合并成更大的内存块。 |

### 关系

- **互补关系**：伙伴系统为 Slab 提供了底层的内存支持，当 Slab 需要更多内存来创建新的 Slab 时，会向伙伴系统请求分配物理页。而 Slab 是对伙伴系统分配的内存进行进一步的细分和管理，针对小对象的频繁分配和释放进行优化，避免了频繁地向伙伴系统申请和释放内存，从而提高了整个内存管理系统的效率。
- **协同工作**：在内存分配过程中，两者协同工作。例如，当内核需要分配一个小对象时，首先由 Slab 分配器在其缓存中查找空闲对象，如果 Slab 缓存中没有可用的对象，则 Slab 分配器向伙伴系统申请内存来创建新的 Slab，以满足小对象的分配需求。在内存释放时，Slab 将释放的对象标记为空闲并放回原 Slab，当 Slab 中的对象全部释放后，Slab 可能会被销毁，其占用的内存会归还给伙伴系统。

## 内存和DMA

### 内存组织

#### UMA（统一内存访问）

- **架构特点**：在 UMA 架构中，所有处理器访问内存的速度是一致的。内存被视为一个统一的资源，均匀地分布在各个处理器之间，处理器通过共享的总线或高速缓存一致性协议来访问内存。
- 优点
  - **编程简单**：由于处理器访问内存的方式相同，软件开发人员无需考虑内存访问的差异性，编程模型相对简单，易于理解和实现。
  - **数据一致性容易维护**：共享内存空间使得多个处理器对数据的访问和修改能够通过统一的机制进行管理，保证了数据的一致性。
- 缺点
  - **可扩展性有限**：随着处理器数量的增加，共享总线或一致性协议会成为瓶颈，限制系统性能的提升。
  - **内存****带宽竞争**：多个处理器同时访问内存时，容易产生内存带宽的竞争，导致性能下降。

#### NUMA（非统一内存访问）

- **架构特点**：NUMA 架构将内存分布在不同的节点上，每个节点都有自己的本地内存和处理器。处理器访问本地内存的速度比访问其他节点的内存速度快，因此内存访问时间是非均匀的。节点之间通过高速互联网络进行通信。
- 优点
  - **良好的可扩展性**：通过增加节点，可以有效地扩展系统的处理器和内存容量，避免了 UMA 架构中共享总线的瓶颈问题，适合构建大规模的多处理器系统。
  - **减少****内存****带宽竞争**：处理器优先访问本地内存，减少了对全局内存带宽的竞争，提高了系统的整体性能。
- 缺点
  - **编程复杂**：开发人员需要考虑数据在不同节点上的分布情况，以优化内存访问性能，这增加了编程的复杂性。
  - **数据一致性管理困难**：由于数据可能分布在不同节点的内存中，需要更复杂的机制来保证数据的一致性，增加了系统设计和维护的难度。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-8.png)

Linux在内存组织中将最高层次定义为内存节点，所以可以将UMA看为特殊的NUMA系统，也就是只有一个内存节点的系统，这样在分配内存的时候，优先考虑在CPU的本地内存对应的节点分配空间，如果不行就从非本地的内存分配

下来将内存按区进行划分（这个看得选考虑虚拟内存映射的关系）：

1. ZONE_DMA
   1. **适用场景**：适合 DMA（Direct Memory Access，直接内存访问）操作的内存区。
   2. **x86** **系统情况**：在 x86 系统上，受 ISA 总线地址总线宽度限制，它只能访问最低的 16MB 区域内存，所以 DMA 内存区被限制在这 16MB 内存区域内。
   3. **SoC** **系统情况**：像 ARM 这样的 SoC（System on Chip，片上系统）系统，DMA 内存区通常无此限制。
2. ZONE_DMA32
   1. **适用场景**：在 64 位系统上，供使用 32 位地址寻址且适合 DMA 操作的内存区。
   2. **举例**：在 AMD64 系统上，该区域为低 4GB 的空间。在 32 位系统上，这个区域通常为空 。
3. ZONE_NORMAL
   1. **定义**：常规内存区域，可直接映射到内核空间的内存。直接映射指物理地址和虚拟地址间存在简单关系，物理地址加上固定偏移就得到虚拟内存地址。
   2. **32 位系统情况**：若用户空间和内核空间以 3GB 为界划分，内核空间仅 1GB 。除去特殊用途的一段内核内存空间（通常是高 128MB 内存空间），常规内存区域一般指低于 896MB 的物理内存，是内核空间中使用最频繁的内存段。
4. ZONE_HIGHMEM
   1. **32 位系统情况**：在 32 位系统上，通常指高于 896MB 的物理内存。
   2. **64 位系统情况**：64 位系统内核空间大，一般不存在高端内存概念。
   3. **映射特点**：要将这段物理内存映射到内核空间，需单独映射，且不能保证物理地址和虚拟地址间有常规内存那样的固定对应关系。
5. ZONE_MOVABLE
   1. **性质**：伪内存区域。
   2. **用途**：主要用于防止物理内存碎片化。

最后是将区按照页进行划分，也就是操作系统能管理的page，这个的大小是由MMU来决定的，内核最基本的内存分配和释放是按照页来进行的

### DMA

这个是实现数据的传输，让CPU去做其他事情

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-9.png)

DMA工作需要的数据是源地址、目的地址、传输字节数和传输方向，因为DMAC数据传输不过MMU，所以这个地址需要时物理地址，不能是虚拟地址，所以在驱动中使用DMA进行数据传输需要将虚拟地址转为物理地址再进行DMA的设置

上图可以看出CPU一般是从cache中拿数据，DMA只能对内存的数据进行传输，所以存在数据一致性的问题，所以驱动程序必须保证用于DMA操作的内存关闭高速缓存的特点 [DMA传输的一个合理的例子](https://github.com/huabowen/linux_kernel_study/blob/master/dma/ex1/memcpy.c)

## 高端内存

**特性**：在 x86 体系中，高于 896MB 的物理内存多为高端内存，其页不能永久映射到内核地址空间，通过 `alloc_pages()` 函数以 `__GFP_HIGHMEM` 标志获取的页无逻辑地址，且 x86 处理器可寻址物理 RAM 范围达 4GB（启用 PAE 可达 64GB） ，高端内存页映射到 3GB - 4GB 。

### 永久映射

- **映射函数**：使用 `<linux/highmem.h>` 中定义的 `void *kmap(struct page *page)` 函数，可用于高端或低端内存。若为低端内存页，直接返回虚拟地址；若是高端内存页，建立永久映射后返回地址，该函数会睡眠，只能用于进程上下文 。
- **解除映射**：通过 `void kunmap(struct page *page)` 函数解除映射，因允许的永久映射数量有限，不再需要高端内存时需解除 。

### 临时映射（原子映射）

- **映射函数**：通过 `void *kmap_atomic(struct page *page, enum km_type type)` 建立临时映射，`type` 是 `<asm/kmap_types.h>` 中定义的枚举类型，描述映射目的 。此函数不会阻塞，可用于中断上下文等不能睡眠的场景，且禁止内核抢占，因为映射对每个处理器唯一 。
- **取消映射**：使用 `void kunmap_atomic(void *kvaddr, enum km_type type)` 取消映射，该函数也不阻塞，在多数体系结构中，若未激活内核抢占，`kmap_atomic()` 映射在新映射到来前有效，`kunmap_atomic()` 通常无需实际操作 。

## CPU数据

### 旧的

使用`get_cpu()`这个获得CPU的编号，用来作为数组下标来访问数据，也就是用数组来抽象每个CPU的数据

```C
unsigned long my_percpu[NR_CPUS];

int cpu;

cpu = get_cpu();      /* 获得当前处理器,并禁止内核抢占 */
my_percpu[cpu]++;     /* ... 或者无论什么 */
printk("my_percpu on cpu=%d is %lu\n", cpu, my_percpu[cpu]);
put_cpu();            /* 激活内核抢占 */
```

### 新的

#### 编译时（静态）

- **变量定义**：使用 `DEFINE_PER_CPU(type, name);` 为每个处理器创建指定类型和名字的变量实例，`DECLARE_PER_CPU(type, name);` 可用于别处声明变量防编译警告 。
- **变量操作**：通过 `get_cpu_var()` 获取当前处理器指定变量并禁止抢占，`put_cpu_var()` 重新激活抢占，如 `get_cpu_var(name)++; put_cpu_var(name);` ；也可用 `per_cpu(name, cpu)++;` 获取指定处理器数据，但该函数不禁止抢占，需额外处理并发问题 。且编译时每个 CPU 数据不能在模块内直接使用 。

#### 运行时（动态）

- **内存分配**：类似于 `kmalloc()` ，通过 `alloc_percpu(type)` 或底层宏 `__alloc_percpu(size_t size, size_t align)` 为每个处理器创建指定类型实例，`alloc_percpu()` 按单字节对齐，如 

  ```c
  struct rabid_cheetah = alloc_percpu(struct rabid_cheetah);
  //等价于
  struct rabid_cheetah = __alloc_percpu(sizeof (struct rabid_cheetah), __alignof__ (struct rabid_cheetah));
  
  ```

  `__alignof__` 用于获取对齐字节数 。

- **数据获取与释放**：分配后通过 `get_cpu_var(ptr)` 获取当前处理器数据并禁止抢占，`put_cpu_var(ptr)` 重新激活抢占；释放用 `free_percpu()` 释放所有处理器指定的每个 CPU 数据 ，如 

  ```C
  void *percpu_ptr;
  unsigned long *foo;
  
  percpu_ptr = alloc_percpu(unsigned long);
  if (!ptr)
      /* 内存分配错误... */
  foo = get_cpu_var(percpu_ptr);
  /* 操作foo... */
  put_cpu_var(percpu_ptr);
  ```

## 内存管理方式选择

- **需连续物理页时**：可选用低级页分配器或 `kmalloc()` 。常用标志有 `GFP_ATOMIC` 和 `GFP_KERNEL` 。`GFP_ATOMIC` 用于中断处理程序等不能睡眠的代码段，执行不睡眠的高优先级分配；`GFP_KERNEL` 用于可睡眠的进程上下文代码，分配时必要情况下可睡眠 。
- **从高端内存分配时**：使用 `alloc_pages()` ，该函数返回指向 `struct page` 结构的指针。因高端内存可能未映射，要获得真正可用指针，需调用 `kmap()` 将高端内存映射到内核逻辑地址空间 。
- **仅需虚拟地址连续页时**：采用 `vmalloc()` ，其分配的内存虚拟地址连续，但物理地址不一定连续。不过与 `kmalloc()` 相比，`vmalloc()` 存在一定性能损失 。
- **创建和撤销大量数据结构时**：考虑构建 slab 高速缓存。slab 层为每个处理器维护对象高速缓存（空闲链表），预先分配对象存于缓存，需存放数据结构时，无需重新分配内存，从高速缓存获取对象即可，可提升对象分配和回收性能 。

-----

# 虚拟文件系统















# 内核模块

Linux的驱动开发教程代码：https://github.com/huabowen/linux_kernel_study.git

## 内核模块编译

linux是宏内核，其和微内核的区别是将所有的内核功能整体编译，形成单个镜像文件，后面加载到内存中跑起来，这个的优点是效率高，各个功能模块的交互是直接调用函数的。微内核的是实现内核的关键和核心函数，其他的模块是单独编译的，功能模块之间的交互需要使用微内核提供的通信机制。

宏内核的缺点是在更改时要重新编译整个内核，在重新启动系统，所以后续开发了内核模块，用于动态增加和卸载内核模块，系统不必重启，开发快速

```C
insmode ; depmod | modprobe          //加载内核模块
modinfo                                                 //显示内核模块的信息
rmmod                                                         //卸载内核模块

MODULE_LICENSE                                        //代表相应的许可证协议  一般是GPL
MODULE_AUTHOR                                        //描述模块作者的信息
MODULE_DESCRIPTION                                //模块的详细说明描述
MODULE_ALIAS                                        //提供给用户空间一个更合适的别名，也就是给这个模块去取一个别名
```

在模块的初始化函数和卸载函数只会调用一次，所以在前面加`__init`和`__exit`标签，这个模块在编译时会将这个函数放在ELF文件的特定代码段，然后加载时会单独分配内存，在函数调用后，模块的加载程序会释放这个部分的内存，而`__exit`修饰的函数是在模块卸载时才加载这个函数，如果模块不允许卸载，这个代码就不会被加载

### 多文件编译内核模块

多个文件编译成一个ko文件，实现是在makefile中定义的

```Makefile
#单个文件编译出ko
obj-m := vser.o

#多个文件编译出ko
obj-m := vser.o
vser-objs = foo.o bar.o
```

重要的更改是`vser-objs = foo.o bar.o`，这个说明了vser模块需要foo.o和bar.o这两个文件共同生成

```Makefile
# 同时指定两个要编译成内核模块的目标文件
obj-m := vser.o another_module.o

# 定义构成 vser.o 的子目标文件
vser-objs := vser.o foo.o bar.o
# 定义构成 another_module.o 的子目标文件
another_module-objs := another_module.o helper.o

# 获取当前内核源码的构建目录
KDIR := /lib/modules/$(shell uname -r)/build
# 获取当前工作目录
PWD := $(shell pwd)

# 默认目标，编译内核模块
default:
    $(MAKE) -C $(KDIR) M=$(PWD) modules

# 清理目标，清除编译生成的文件
clean:
    $(MAKE) -C $(KDIR) M=$(PWD) clean
```

`obj-m` 是一个在编译内核模块时使用的特殊变量，其作用在于告知内核构建系统需要将哪些目标文件编译成内核模块。

`*-objs` 是一个自定义的变量，其用途是指定构成 `vser.o` 目标文件的子目标文件列表。

### 内核模块参数

就是内核模块在加载的时候获取命令行的参数

```C
module_param(name,type,perm)
module_param_array(name,type,nump,perm)
```

上面是用来将那个变量声明为模块参数，name是变量名，type是变量类型，nump是数组的个数，perm是在sysfs文件系统中对应文件的权限属性

```C
//这个是命令行加载的格式，name3是数组，传参用','分割
modprobe vser
modprobe vser name1=* name2=* name3=1,2,3,4
```

### 内核模块依赖

`EXPORT_SYMBOL`这个宏是将函数导出，这个是生成特定结构放在ELF文件的特定段，内核启动的时候，会将这个符号的确切地址放在这个结构的特定成员，模块在加载时，对于未决符号就是在特殊段中去找这个符号的名字，找到就将获得的地址填充在被加载模块的相应段中，也就时连接步骤后移，在加载内核模块的时候，对之前编译未确定的符号或函数进行连接，故前面编译的ko文件只是编译，并没有连接。

同时使用这个宏进行导出，就是产生这个模块的依赖关系（使用这个函数的模块和定义这个函数的模块）

在存在依赖关系的两个模块的编译时需要放在一起进行编译，或者先将被依赖的模块加载到内核中，在编译另一个模块，不然会产生找不到的情况

内核模块在编译的时候会保留这边编译平台和版本的信息，在加载的时候内核会核对这些信息，不一致则认为非法模块，加载失败

## 字符设备驱动

字符设备驱动：设备对数据是按照字节流进行控制的，可以支持随机访问，也可以不支持随机访问，由于数据流量不大，一般没有页高速缓存

块设备驱动：设备对数据的处理是按照块进行的，一个块有固定的大小，这个类设备都支持随机访问，为了提高效率可以将之前用到的数据缓存起来

网络设备驱动：就是针对网络设备的一类驱动，主要就是进行网络数据的收发

设备的在内核的区分是设备号，主设备号是区分一类设备的，次设备号是区分一类设备的不同个体或不同分区，具体的设备名称是用户进行设备区分的

驱动文件是在/dev下，以文件的形式存在，这个文件描述的结构是同一的，所以用户能通过文件来操作设备，同时文件的描述存在`i_block`这个变量，如果是普通文件，这个是存对应的放文件数据的块号，如果是设备文件，这个放的是设备号，后续根据这个来判断这个是操作设备文件还是普通文件

### 从用户调到底层驱动程序

上层`open`函数的调用

```Shell
sys_open(fs/open.c)
  |- do_sys_open(fs/open.c)
     |- getname(fs/namei.c)
     |- get_unused_fd_flags(fs/file.c)
     |- do_filp_open(fs/namei.c)
     |  |- path_openat(fs/namei.c)
     |     |- get_empty_filp(fs/file_table.c)
     |     |- link_path_walk(fs/namei.c)
     |     |- do_last(fs/namei.c)
     |        |- lookup_fast(fs/namei.c)
     |        |- lookup_open(fs/namei.c)
     |           |- lookup_dcache(fs/namei.c)
     |           |- lookup_real(fs/namei.c)
     |              |- ext2_lookup(fs/ext2/namei.c)
     |                 |- ext2_iget(fs/ext2/inode.c)
     |                    |- init_special_inode(fs/inode.c)
     |                      |- inode->i_fop = &def_chr_fops;
     |- fd_install(fs/namei.c)
```

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-10.png)

用户调用`open`函数会调用到`sys_open`函数，这个调用到`do_sys_open`函数，这个内部先调用`getname`将文件名称拷贝到内核空间，然后使用`get_unused_fd_flags`在files_struct的fd_array中找一个没有用的文件描述符，返回对应的下标，下来就是使用`do_file_open`构造一个file接口并初始化这个结构，这个里面重要的是将这个f_op和设备驱动的file_operations这个进行关联，并初始化设备，最后调用`fd_install`让这个fd_array获取的下标执行这个新生成的file文件，最后系统用将这个新的下标返回给用户层

`do_file_open`是重要的操作，这个是会根据传入的文件路径，解析这个path，最后不断的找到对应这个文件对应的inode（磁盘上的inode），最后会获取到这个mknod命令创建的这个设备文件的信息，判断这个文件的类型，如果对于字符设备驱动，最重要的是将文件类型和设备号要填充到对应的内存的inode，同时将f_op指向def_chr_fops，然后这个里面存在`chrdev_open`函数，这个会根据设备号在cdev_map这个散列表中找到当时注册的cdev，这个cdev会驱动自己构造的file_operation结构进行关联，然后就是将驱动实现的file_operations替换了file的f_op，调用对应这个设备的open函数执行，初始化设备

```C
//初始化cdev结构，主要是将fops和这个cdev就进行关联
void cdev_init(struct cdev* cdev, const struct file_operations * fops);
//将cdev添加到散列表中，这个主要的工作是将主设备号对255取余，余数作为cdev_map下标的索引，构造一个probe的对象，添加到链表中，其中的data指向的就是cdev的地址
int cdev_add(struct cdev* p, dev_t dev, unsigned count);
```

### 一个驱动支持多个设备

因为是一个驱动需要适配多个设备同时使用，所以是使用file的private变量进行区分的，根据open函数的inode的信息，区分不同的设备，对不同设备的file的private变量赋予不同的值在其他的f_op的操作函数中，使用file的private变量来进行对单个设备的操作

例子：[一个cdev控制多个设备](https://github.com/huabowen/linux_kernel_study/blob/master/chrdev/ex4/vser.c) [一个cdev控制一个设备，多次注册](https://github.com/huabowen/linux_kernel_study/blob/master/chrdev/ex5/vser.c)

## IO操作

### ioctl

对于一个设备来说，read和write是一般走的数据，但是对于设备的控制来说，应该走ioctl通道（可以使用write实现）

`ioctl`系统调用的驱动接口可以是`unlocked_ioctl`和`compat_ioctl`，`compat_ioctl`是为了兼容32位和64位机

#### `_IO(type, nr)`

- **用途**：该宏用于定义一个无参数的`ioctl`命令。
- 参数解释：
  - `type`：这是一个表示设备类型的魔数（magic number），通常是一个字符，用于区分不同类型的设备。魔数的取值范围一般在`0x00 - 0xFF`之间，要确保其唯一性，避免与其他设备的魔数冲突。
  - `nr`：是一个命令编号，同样需要保证其在该设备类型内的唯一性，一般取值范围是`0 - 255`。

#### `_IOR(type, nr, size)`

- **用途**：用于定义一个从内核空间向用户空间传递数据的`ioctl`命令。
- 参数解释：
  - `type`：设备类型魔数，作用同`_IO`宏中的`type`。
  - `nr`：命令编号，作用同`_IO`宏中的`nr`。
  - `size`：要传递的数据的大小（以字节为单位），内核会根据这个大小来检查传递数据的合法性。

#### `_IOW(type, nr, size)`

- **用途**：用于定义一个从用户空间向内核空间传递数据的`ioctl`命令。
- 参数解释：
  - `type`：设备类型魔数。
  - `nr`：命令编号。
  - `size`：要传递的数据的大小（以字节为单位）。

#### `_IOWR(type, nr, size)`

- **用途**：用于定义一个既可以从用户空间向内核空间传递数据，又可以从内核空间向用户空间传递数据的`ioctl`命令。
- 参数解释：
  - `type`：设备类型魔数。
  - `nr`：命令编号。
  - `size`：要传递的数据的大小（以字节为单位）。

### proc

proc是一种伪文件系统，只存在内存中，只有内核运行才会动态生成内部内容，这个可以类似/dev下的节点，创建对应的`file_operations`然后再/proc下创建对应的文件，最后使用文件来更改这个内核模块的内容 [proc接口使用例子](https://github.com/huabowen/linux_kernel_study/blob/master/advio/ex2/vser.c)

### IO分类

#### 阻塞 IO（Blocking IO）

- **概念**：在进行 I/O 操作时，若数据未准备好或无法立即读写，程序会被阻塞（即调用被挂起），一直等待数据准备好或操作完成，才能继续执行后续操作 。这是一种同步 I/O 操作方式。
- **优点**：编程模型简单易懂，对于简单的 I/O 操作场景，逻辑清晰，容易实现和维护。
- **缺点**：在等待 I/O 操作完成期间，进程或线程会被阻塞，无法执行其他任务，严重影响程序的并发处理能力。当有大量 I/O 操作且 I/O 操作耗时较长时，会导致系统资源浪费和性能低下。

#### 非阻塞 IO（Non - Blocking IO）

- **概念**：进行 I/O 操作时，如果数据没有准备好或无法立即读写，程序不会等待，而是立即返回，并继续执行后续操作。之后程序通过轮询（不断重复检查）或使用选择函数（如`select`、`poll`、`epoll` 等）来检查是否有 I/O 操作可进行。
- **优点**：程序不会被 I/O 操作阻塞，可在等待 I/O 的同时执行其他任务，提高了程序的并发性能，能更高效地利用 CPU 资源。
- **缺点**：需要不断轮询检查 I/O 状态，会消耗一定的 CPU 资源。而且编程复杂度相对较高，需要处理各种返回状态和错误情况。

#### 异步 IO（Asynchronous IO）

- **概念**：程序发起 I/O 请求后，无需等待 I/O 操作完成，可继续执行其他任务。当 I/O 操作完成后，系统会通过特定机制（如回调函数、信号等）通知程序，程序再去获取或处理 I/O 操作的结果 。例如在一些支持异步 I/O 的系统中，通过注册回调函数，当 I/O 操作完成时，操作系统会自动调用该回调函数来处理结果。
- **优点**：极大地提高了程序的并发性和响应性能，能充分利用系统资源，让程序在 I/O 操作的同时执行其他计算任务，适合处理大量 I/O 操作的高并发场景。
- **缺点**：编程模型复杂，需要处理异步事件的回调逻辑，涉及到多线程、多进程或异步编程框架等知识，增加了代码编写和调试的难度。同时，需要处理好异步操作中的竞态条件和资源管理问题。

#### IO 多路复用（IO Multiplexing）

- **概念**：通过一种特殊的函数（如`select`、`poll`、`epoll` 等），让一个或少数几个线程能够同时监控多个文件描述符（fd，如网络连接的 socket 描述符）的 I/O 状态变化 。当被监控的 fd 中至少有一个有数据准备就绪（可读、可写或有异常等）时，函数返回，程序再针对就绪的 fd 进行相应的 I/O 操作（如调用`recvfrom`读取数据）。
- **优点**：相比为每个 I/O 操作创建一个线程或进程，能显著减少系统资源消耗（如线程或进程的创建和维护开销），提高系统并发处理能力，适用于处理大量并发连接的网络服务器等场景。
- **缺点**：`select`和`poll`函数存在一定性能瓶颈（如`select`对监控的 fd 数量有限制，且采用轮询方式效率较低），虽然`epoll`在性能上有很大提升，但整体编程复杂度较高，需要对这些函数的使用和原理有深入理解。

### 异步通知

前面的IO都是应用层来驱动主动获取数据，而异步通知则是驱动给应用层信号，说明有信息可以获取，应用层再来驱动层拿

## RCU（read copy update）

RCU机制就是读—复制—更新，RCU对共享内存是通过指针来实现的，读是对指针解引用来访问，写是先将这部分内存复制一份进行更改，当写结束后，等所有的读操作完成对原数据的读取后，将读指针更新为写的新指针，后续读到的就是新数据

```C
#include <linux/init.h>
#include <linux/module.h>
#include <linux/rcupdate.h>
#include <linux/slab.h>

// 定义一个简单的数据结构
struct my_data {
    int value;
};

// 全局指针，指向共享数据
static struct my_data *my_shared_data;

// 读函数，使用 RCU 读取数据
static int read_data(void)
{
    struct my_data *data;
    int value;

    rcu_read_lock();
    data = rcu_dereference(my_shared_data);
    if (data)
        value = data->value;
    else
        value = -1;
    rcu_read_unlock();

    return value;
}

// 写函数，使用 RCU 更新数据
static void write_data(int new_value)
{
    struct my_data *new_data, *old_data;

    // 分配新的数据结构并初始化
    new_data = kmalloc(sizeof(*new_data), GFP_KERNEL);
    if (!new_data)
        return;
    new_data->value = new_value;

    // 使用 RCU 进行更新
    old_data = xchg(&my_shared_data, new_data);
    if (old_data) {
        // 等待宽限期结束
        synchronize_rcu();
        // 释放旧数据
        kfree(old_data);
    }
}

// 模块初始化函数
static int __init rcu_example_init(void)
{
    // 初始化共享数据
    my_shared_data = kmalloc(sizeof(*my_shared_data), GFP_KERNEL);
    if (!my_shared_data)
        return -ENOMEM;
    my_shared_data->value = 0;

    printk(KERN_INFO "RCU example module initialized, initial value: %d\n", read_data());

    return 0;
}

// 模块退出函数
static void __exit rcu_example_exit(void)
{
    struct my_data *data;

    // 等待所有 RCU 读临界区完成
    synchronize_rcu();

    data = my_shared_data;
    if (data) {
        // 释放共享数据
        kfree(data);
    }
    printk(KERN_INFO "RCU example module exited.\n");
}

module_init(rcu_example_init);
module_exit(rcu_example_exit);

MODULE_LICENSE("GPL");    
```

## Linux设备模型

### kobject kset

kobject注册到内核中会在sysfs文件系统中创建一个目录，kobject的attr属性则是创建对应的文件，kset则是对多个kobject对象的一个集合，kset内部内嵌了一个kobject，所以这个也会以文件的形式展示，内嵌的kobject的attr属性是这个目录的文件，然后这个用来管理多个kobject的，内嵌的kobject就是其管理的kobject的父节点，所以就会形成树形结构，同意kset管理的kobject就是兄弟关系

```C
#include <linux/init.h>
#include <linux/kernel.h>
#include <linux/module.h>

#include <linux/slab.h>
#include <linux/kobject.h>

static struct kset *kset;
static struct kobject *kobj1;
static struct kobject *kobj2;
static unsigned int val = 0;

static ssize_t val_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
        return snprintf(buf, PAGE_SIZE, "%d\n", val);
}

static ssize_t val_store(struct kobject *kobj, struct kobj_attribute *attr, const char *buf, size_t count)
{
        char *endp;

        printk("size = %zu\n", count);
        val = simple_strtoul(buf, &endp, 10);

        return count;
}

static struct kobj_attribute kobj1_val_attr = __ATTR(val, 0444, val_show, val_store);
static struct attribute *kobj1_attrs[] = {
        &kobj1_val_attr.attr,
        NULL,
};

static struct attribute_group kobj1_attr_group = { 
                .attrs = kobj1_attrs,
};

static int __init model_init(void)
{
        int ret;

        kset = kset_create_and_add("kset", NULL, NULL);
        kobj1 = kobject_create_and_add("kobj1", &kset->kobj);
        kobj2 = kobject_create_and_add("kobj2", &kset->kobj);

        ret = sysfs_create_group(kobj1, &kobj1_attr_group);
        ret = sysfs_create_link(kobj2, kobj1, "kobj1");

        return 0;
}

static void __exit model_exit(void)
{
        sysfs_remove_link(kobj2, "kobj1");
        sysfs_remove_group(kobj1, &kobj1_attr_group);
        kobject_del(kobj2);
        kobject_del(kobj1);
        kset_unregister(kset);
}

module_init(model_init);
module_exit(model_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kevin Jiang <jiangxg@farsight.com.cn>");
MODULE_DESCRIPTION("A simple module for device model");
```

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-11.png)

### 动态加载

Linux通过udev来实现内核模块的动态加载，udev是一个用户线程，任何设备的添加和删除都会导致内核给用户空间发送响应的通知，事件叫uevent，用户空间的udev捕获这些信息来实现自动加载驱动、自动创建和删除设备节点等，这个也可以自己定义规则

所以Linux的内核模块并不都是内核启动就加载上去的，当设备添加时再加载对应的驱动，但是一个驱动可以控制多个设备，所以内核模块的别名就启了很大的作用，下面是module.alias文件的信息，前面的是对应模块别名信息（一般是能和这个驱动匹配的设备的信息，可以有多个别名），后面的是模块的信息，也就是内核再识别到对应的端口的设备后，会添加对应的一个设备的信息，设备的名称会被udev获取，udev再根据这个名称（驱动的别名）来找到对应的驱动，这样就可以实现多个设备只要有一个插入就可以加载对应的驱动，如下：多个pci端口的设备就匹配一个atyfd的设备

```Shell
alias pci:v00001002d00004750sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004749sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004744sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004742sv*sd*bc*sc*i* atyfb
alias pci:v00001002d0000475Asv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004759sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004757sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004756sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00005656sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004C47sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004755sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00005655sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004754sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00005654sv*sd*bc*sc*i* atyfb
```

## 总线类

### I2C

基本就不说了，I2C上可以有多个主机控制器（Master Adapter）和从设备（Slave Client），多个从设备是根据设备地址来区分的，地址分为7位和10位，但是一般是7位

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-12.png)

- 开始位：当 SCL 为高电平时，SDA 由高电平变为低电平的期间，如图 10.2 中最左边的 START condition 所标记的区域，这表示主机控制器要开始对从机发起访问了。
- 地址位：接下来的 7 个时钟周期，主机控制器将会发送从机的 7 位地址（如果是 10 位地址需要分两次发送），如图 10.2 中 ADDRESS 所标记的区域。
- 读 / 写位：在第 8 个时钟周期，如果 SDA 为高电平则表示接下来要读取从机的数据，如果是低电平则表示主机要写数据到从机，如图 10.2 中 R/W 所标记的区域。
- 应答位：在第 9 个时钟周期由从机进行应答，低电平为 ACK，高电平为 NACK，如果从机响应，应该发 ACK。
- 数据位：在接下来的若干个周期内，主机可以持续读取数据（如果读 / 写位为读），或写数据（如果读 / 写位为写），每次数据传输完成（如图 10.2 中的 DATA 所标记的区域）也要进行应答，是读则由主机控制器来应答，是写则由从机来应答，只是在主机读完最后一个字节的数据后应该以 NACK 来应答。
- 停止位：当 SCL 为高电平时，SDA 由低电平变为高电平的期间，如图 10.2 中最右边的 STOP condition 所标记的区域，这表示主机控制器结束了对从机的访问。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-13.png)

#### I2C 总线的随机读时序及解释

1. **起始信号与写命令发送**：主机先产生并发送起始信号给从机，同时将写控制命令发给从机，此时读写控制位设为低电平，表明要对从机进行写操作。写控制命令按高位在前、低位在后的顺序发送。比如在对一个 I2C 设备进行操作时，这一步就像敲门并告知设备接下来要写入内容。
2. **从机应答判断**：从机收到读控制指令后，若回传非应答信号，会输出 I2C 通信错误信号；若回传应答信号，主机收到应答信号后，开始字地址写入。这里的应答信号就像设备回应 “我准备好了” 。
3. 字地址写入：
   1. **单字节地址**：按高位在前、低位在后顺序写入单字节存储地址。
   2. **双字节地址**：先向从机写入高 8 位地址（高位在前、低位在后），收到从机应答信号后，再写入低 8 位地址（高位在前、低位在后） 。例如，访问有双字节地址的存储芯片时，需分两步完成地址写入。
4. **再次起始信号**：字地址写入完成且主机收到从机应答信号后，主机再次向从机发送一个起始信号，相当于重新建立通信准备读取数据。
5. **读命令发送**：主机向从机发送读控制命令，此时读写控制位设为高电平，表示要对从机进行读操作。
6. **数据接收**：主机收到从机回传的应答信号后，开始接收从机传回的单字节数据。
7. **无应答信号发送**：数据接收完成后，主机产生一个时钟的高电平无应答信号，告诉从机数据接收完毕。
8. **停止信号发送**：主机向从机发送停止信号，单字节读操作完成。

#### I2C 总线的随机写时序及解释

1. **起始信号与写命令发送**：主机产生并发送起始信号到从机，同时将写控制命令发送给从机，读写控制位设为低电平，写控制命令高位在前、低位在后发送。
2. **从机应答判断**：从机接收到写控制指令后，回传应答信号（若回传非应答信号，则表示通信有问题），主机收到应答信号后，准备写入数据。
3. **数据写入**：主机按照约定顺序，将数据逐位或逐字节写入从机。每写入一个数据，等待从机应答。例如向 I2C 的 EEPROM 设备写入数据时，按其存储单元顺序写入。
4. **停止信号发送**：数据全部写入完成，且收到从机对应答后，主机发送停止信号，结束此次随机写操作。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-14.png)

- I2C 主机驱动：I2C 主机控制器的驱动，一般由 SoC 芯片厂商负责设计实现，用于控制 I2C 主机控制器发出时序信号。
- I2C Core：为上层提供统一的 API 接口和对其他模块进行注册和注销等管理等。
- I2C 设备驱动：调用 I2C Core 提供的统一 API，根据 I2C 设备的访问规范，控制 I2C 主机控制器发出不同的时序信号，对 I2C 设备进行访问。该驱动称为内核层 I2C 设备驱动。
- i2c - dev：将 I2C 主机控制器实现为一个字符设备，应用程序可以直接访问 /dev/i2c - N 来访问 I2C 主机控制器，从而对 I2C 设备发起访问，该应用程序称为应用层 I2C 设备驱动。

I2C Core 为屏蔽不同的 I2C 主机控制器驱动提供了可能，可以使 I2C 设备驱动仅关心如何操作 I2C 设备，而不需要了解 I2C 主机控制器的细节，从而使 I2C 设备驱动可以独立的存在，适用于各种不同的硬件平台。

现在的Client是由设备树和内核实现的，会将这个节点添加到内核中，所以现在需要实现的是驱动也就是和这个Client匹配的driver

### SPI

和I2C一样，内核有具体的实现，驱动编写只需要使用对应的接口，同时将SPI的主机设备也抽象为一个字符设备

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-15.png)

### USB

这个usb的管理结构和网络内部的管理类似，USB主机就是Host，其会和一个根hub（集线器进行连接），剩下的设备都是和这个集线器进行连接的，其的拓扑图如下：

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035734-16.png)

一个USB物理设备由一个或多个USB逻辑设备组成，一个USB逻辑设备体现为一个”接口“，接口由一组“端点”组成，每个USB物理设备需要用地址进行区分，一个逻辑设备的端点也有地址进行区分。所以主机通过设备地址和端点地址来寻找USB设备上的具体端点，物理0地址和端点0都是特殊的

传输分类：

1. 控制传输：突发的、非周期性的、用于请求 / 响应的通信。主要用于命令和状态的操作，如前面提到的枚举过程中的数据传输。只有端点 0 默认用于控制传输，所以端点 0 也叫作控制端点（通常用于什么传输的端点就叫什么端点，如用于控制传输的端点就叫控制端点）。USB 协议定义了很多标准的命令（请求）以及这些命令的响应，这些数据就是通过控制传输来完成的。另外，USB 协议允许厂商自定义命令，也用控制传输来完成。
2. 等时传输：有的也叫作同步传输，用于主机和设备之间周期性、连续的通信，通常用于对时间性要求高，但不太关心数据正确性的场合，比如音频数据的传输，如果传输速率不能满足要求，声音会出现停顿，但少量的数据错误，并不会太影响声音所提供的信息。
3. 中断传输：周期性的、确保延迟不超过一个规定值的传输。这里的中断并不是我们之前所说的中断，其更像是轮询。比如对于 100ms 周期的中断传输，主机会保证在 100ms 内发起一次传输。键盘、鼠标等设备通常使用这种传输模式，主机会定期获取设备的按键信息。
4. 块传输：也叫批量传输，非周期性的大数据传输。主要用于大量数据的传输，且对传输的延时不特别限制的情况，比如磁盘设备等。

## 块设备

VFS（Virtual File System，虚拟文件系统）：为应用程序提供统一的文件访问接口，屏蔽了各个具体文件系统的操作细节，是对所有文件系统的一个抽象。

Disk Caches：硬盘高速缓存，用于缓存最近访问的文件数据，如果能在高速缓存中找到，就不必去访问硬盘，毕竟硬盘的访问速度要慢很多。

Disk Filesystem：文件系统，属于映射层（Mapping Layer）。在应用程序开发者的眼中，一个文件是线性存储的，但实际上它们很有可能是分散存放在硬盘的不同扇区上的。文件系统最主要的作用就是要把对文件从某个位置开始的若干个字节访问转换为对磁盘上某些扇区的访问。它是文件在应用层的逻辑视图到磁盘上的物理视图的一个映射。

Generic Block Layer：通用块层，用于启动具体的块 I/O 操作的层。它是对具体硬盘设备的抽象，使得内核的上层不用关心磁盘硬件上的细节信息。

I/O Scheduler Layer：I/O 调度层，负责将通用块层的块 I/O 操作进行调度、排序和合并操作，使对硬盘的访问更高效。这在后面还会进一步进行说明。

Block Device Driver：块设备驱动，也就是块设备驱动开发者写的驱动程序

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-17.png)

但是实际内核的组件已经实现了很多的功能，所以需要注意的是gendisk这个结构体，这个结构体在sd_probe中申请并注册的，这个结构是一个硬盘对上层的一个抽象显示

### bio

对块设备的读写操作基本是由`submit_bio`函数提交的申请，描述这个任务的详细信息是bio结构体描述的

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-18.png)

这个是主要的一个描述形式，`bi_io_vec`数组所维护的`bio_vec`是用来描述具体信息的，page是物理内存页的管理对象指针，len是字节数，offset是偏移量

```C
// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
enum rw_hint bio_data_dir(struct bio *bio);

// bvl：struct bio_vec 结构体指针，用于接收每次迭代得到的 bio_vec。
// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
// iter：struct bio_vec_iter 类型的迭代器，用于遍历 bio 中的 bio_vec。
#define bio_for_each_segment(bvl, bio, iter) \
    for (bio_iter_init(iter, bio); \
         (bvl = bio_iter_iovec(&iter)) != NULL; \
         bio_iter_advance(&iter))

// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
// iter：struct bio_vec_iter 类型的迭代器，指定要映射的 bio_vec。
void *__bio_kmap_atomic(struct bio *bio, struct bio_vec_iter iter);

// addr：之前通过 __bio_kmap_atomic 映射得到的虚拟地址。
void __bio_kunmap_atomic(void *addr);

// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
// error：错误码，0 表示 bio 正常结束，其他为错误码。
void bio_endio(struct bio *bio, int error);
```

bio_data_dir：获取本次块 I/O 操作的读写方向，在同一个 bio 中的 bio_vec 的读写都是一致的，要么全是读，要么全是写。 bio_for_each_segment：用迭代器 iter 遍历 bio 中的每一个 bio_vec，得到的 bio_vec 是 bvl。 bio_kmap_atomic：将迭代器 iter 对应的 bio_vec 中的物理页面映射，支持高端内存，返回的是映射后再加上 bv_offset 偏移的虚拟地址。bio_kunmap_atomic：解除前面的映射。 bio_endio：结束一个 bio，error 为 0 表示 bio 正常结束，否则是其他错误码，比如 - EIO。

### request

bio是可以合并并排序的，将多个bio合并排序后整合到一个request中

```C
struct request {
        struct list_head queuelist;
        struct request_queue *q;
        struct bio *bio;
        struct bio *biotail;
    ...
};
```

- queuelist：链表成员，用于将多个请求组织成一个链表。
- q：当前请求属于的请求队列。
- bio：请求中包含的第一个 bio 对象。
- biotail：请求中包含的最后一个 bio 对象。

### request_queue

请求最后放入到请求队列中

```C
struct request_queue {
        struct list_head queue_head;
        request_fn_proc *request_fn;
        make_request_fn *make_request_fn;
    ...
};
```

- queue_head：请求队列的链表头。
- request_fn：指向由块设备驱动开发者提供的请求处理函数。采用这种方式内核会把块 I/O 请求（bio）首先经过排序、合并等手段来形成请求（struct request），然后再把请求放入到请求队列中，最后调用块设备驱动开发者提供的由该函数指针指向的请求处理函数来处理这个队列中的请求。
- make_request_fn：指向用于构造请求的函数（将 bio 排序、合并的函数），该函数可以由内核提供默认的请求构造函数，那么驱动开发者就应该提供请求处理函数来处理请求，这通常用于磁盘这类设备。还可以由驱动开发者来实现一个用于构造请求的函数，然后该指针指向这个函数，这通常适用对 bio 的排序和合并操作有特殊要求的设备，或者根本不需要将 bio 排序和合并的设备（驱动开发者也就不需要提供请求处理函数了），如固态硬盘、U 盘和 SD 卡等。

# 网络驱动

网络驱动设备需要两个硬件设备：MAC和PHY，MAC就和I2C控制器的功能类似，一般情况下SOC会内置MAC，只需要在外部增加PHY就可以联网，如果SOC内部没有内置这个MAC，就需要在外部增加MAC和PHY

## SOC内部不带MAC

如果SOC内部不带MAC，则需要在外部增加一个带MAC和PHY集成的一个模块，这个模块和SPC的通信可能就简单使用一些SPI SRAM等协议进行通信

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-19.png)

## SOC内部自带MAC

还有的SOC内部自带MAC，所以只需要在外部增加PHY的模块的，就能进行网络通信，由于是集成在SOC内部的，所以就会有专门的网络加速引擎，比如网络专用的DMA，这样的网速就会快，同时PHY的成本就低，可选项就多

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-20.png)

## MII/RMII和MDIO

### MII/RMII

这个是一个通信的协议，用于MAC和PHY进行通信的协议，这个是进行网络数据传输的协议，但是主控还需要读取PHY内部的寄存器，这个是MDIO

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-21.png)

这个是MII的通信协议的线，这里的时钟线都是PHY产生给MAC的，如果网速为 100M 的话时钟频率为 25MHz，10M 网速的话时钟频率 为 2.5MHz，特别说明：56

- TX_ER：发送错误信号，高电平有效，表示 TX_ER 有效期内传输的数据无效。
- RX_DV：接收数据有效，作用类似 TX_EN。
- CRS：载波侦听信号。
- COL：冲突检测信号。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-22.png)

这个上RMII的通信协议线，这个是MII的简化，比MII能少9根线，特别说明：

- CRS_DV：相当于 MII 接口中的 RX_DV 和 CRS 这两个信号的混合。
- REF_CLK：参考时钟，由外部时钟源提供， 频率为 50MHz。这里与 MII 不同，MII 的接 收和发送时钟是独立分开的，而且都是由 PHY 芯片提供的。

这里就简述两个，还有其他的GMII、RGMII等，原理基本相同，大同小异

### MDIO

Management Data Input/Output，就两根线，和I2C很像，一个MDIO数据线，一个MDC时钟线，MDIO的接口可以支持32个PHY，但是同一时刻只能和一个PHY进行操作，这个很和I2C很像，需要对PHY就行编址

## RJ45

插网线硬件接口，这个是和PHY进行沟通，但是这个中间需要一个网络变压器，这个变压器用于进行隔离和滤波，当前市面的这个座子基本都集成了变压器

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-23.png)

## PHY

PHY的芯片寄存器地址空间是5位，也就是32个寄存器，IEEE定义了0~15寄存器，这个就是所有的PHY都是一样的，使用这个16个寄存器就能驱动PHY，另外的16个寄存器是厂商定义。但是后面厂商进行了丰富，所以Linux开源PHY驱动可能驱动不起来，所以厂商提供驱动代码并commit到Linux的开源源码中，但是一定根据具体的硬件手册一定能驱动起来，主要也就是控制这个BCR（Basic Control Rgsister）寄存器进行PHY的设置，地址一般是0，和读取BSR（Basic Status Register）寄存器数据进行PHY状态的判断，地址一般是0，还有就是两个ID寄存器，地址一般是2 3

## 网络设备驱动

### 网络层次结构

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-24.png)

网络设备就是将设备的数据读上来，或将数据给设备给发送出去，没有设备节点，使用的是套接字编程接口。

### sk_buff

在数据传输的过程中`sk_buff`结构很重要（套接字缓冲区），这个用于各层之间传递数据包，各层是共享这个这一个缓冲区的，层和层之间传递的是缓冲区的指针。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-25.png)

```C
struct sk_buff *alloc_skb(unsigned int size, gfp_t priority);
struct sk_buff *dev_alloc_skb(unsigned int length);
void kfree_skb(struct sk_buff *skb);
dev_kfree_skb(a);
```

- alloc_skb、dev_alloc_skb：用于分配并初始化 skb，size 或 length 是缓冲区的大小，priority 是内存分配掩码。dev_alloc_skb 用于不能休眠的上下文中。
- kfree_skb、dev_kfree_skb：释放 skb，kfree_skb 和 alloc_skb 配对使用，dev_kfree_skb 和 dev_alloc_skb 配对使用。

新分配的sk_buff如下：

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-26.png)

```C
static inline void skb_reserve(struct sk_buff *skb, int len)
{
        skb->data += len;
        skb->tail += len;
}
```

这个函数是将data和tail同时向end方向偏移len字节，也就是用在刚分配好的skb后为了预留足够的协议头空间或对齐操作

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-27.png)

```C
unsigned char *skb_put(struct sk_buff *skb, unsigned int len)
{
        ……
        skb->tail += len;
        skb->len += len;
        ……
}
```

将 tail 向 end 方向偏移 len 个字节，在 put 操作之前，tail 处于实线箭头的位置；在 put 操作之后，tail 在虚线箭头的位置。函数返回 put 操作之前的 tail 指针，通常用于添加尾部数据。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-28.png)

```C
unsigned char *skb_push(struct sk_buff *skb, unsigned int len)
{
        skb->data -= len;
        skb->len += len;
        ……
}
```

将 data 向 head 方向偏移 len 个字节，在 push 操作之前，data 处于实线箭头的位置；在 push 操作之后，data 在虚线箭头的位置。函数返回 push 操作之后的 data 指针，通常用于添加协议头数据。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-29.png)

```C
unsigned char *skb_pull(struct sk_buff *skb, unsigned int len)
{
        ……
        skb->len -= len;
        ……
        return skb->data += len;
}
```

将 data 向 end 方向偏移 len 个字节，在 pull 操作之前，data 处于实线箭头的位置；在 push 操作之后，data 在虚线箭头的位置。函数返回 pull 操作之后的 data 指针，通常用于去掉协议头数据。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-30.png)

## 代码框架

### 设备申请和注册

在网络驱动的中主要注册net_device结构体，可以使用`alloc_netdev`函数来看申请这个节点，但是申请的本质是调用的`alloc_netdev_mqs`函数，函数的定义如下：

```C
struct net_device * alloc_netdev_mqs ( int sizeof_priv, 
                                                                                 const char *name, 
                                                                                 void (*setup) (struct net_device *)) 
                                                                                 unsigned int txqs, 
                                                                                 unsigned int rxqs); 
//新的
struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,
                unsigned char name_assign_type,
                void (*setup)(struct net_device *),
                unsigned int txqs, unsigned int rxqs)
    
//删除的话直接调用
void free_netdev(struct net_device *dev) 
```

这个就是函数调用的原型

- sizeof_priv：私有数据块大小。这个是需要传入的，是这个网路设备自己维护的一些私有数据。
- name_assign_type：新的参数，是和名字的相关的
- name：设备名字。
- setup：回调函数，初始化设备的设备后调用此函数，在`alloc_netdev_mqs`函数中使用到这个回调函数，用于设置net_device的部分变量
- txqs：分配的发送队列数量。
- rxqs：分配的接收队列数量。

但是该函数是根本调用的函数，后续开源内核对该函数进行了封装，可以直接调用封装好的函数，如`alloc_etherdev`初始化以太网的函数

```C
//注册net_device设备
int register_netdev(struct net_device *dev) 
    
//注销net_device设备
void unregister_netdev(struct net_device *dev) 
```

前面申请以太网的eth设备时，申请的名称是eth%d，这个名称是在`register_netdev -> register_netdevice -> dev_get_valid_name`中确定的

### 数据发送

由于网络是分层的，所以不必过于关心底层是怎么实现的，所以需要注意的是发送的数据包应该怎么封装，怎么发送出去，发送的话主要是通过`dev_queue_xmit`函数进行发送数据，接受的话使用的是`netif_rx`函数

网络数据主要是`sk_buff`结构体进行保存，各层的协议都是在这个结构体的中加自己的协议头，然乎最后层层往下传，最后由底层的驱动将数据发送出去，接受也是同理，底层接受后将其整理为`sk_buff`数据，返回给上层，上层解析，最后将数据返给用户

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-31.png)

sk_buff中存在head data tail end，head 指向缓冲区的头部，data 指向实际数据的头部。data 和 tail 指向实际数据 的头部和尾部，head 和 end 指向缓冲区的头部和尾部。

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-32.png)

```C
//发送数据
static inline int dev_queue_xmit(struct sk_buff *skb) //返回值：0 发送成功，负值 发送失败。

//接受数据
int netif_rx(struct sk_buff *skb) //返回值：NET_RX_SUCCESS 成功，NET_RX_DROP 数据包丢弃。
```

sk_buff申请和释放函数

```C
//申请函数
static inline struct sk_buff *alloc_skb(unsigned int size, gfp_t priority) 
//但是一般使用netdev_alloc_skb来给对应的net_deivce申请sk_buff
static inline struct sk_buff *netdev_alloc_skb(struct net_device *dev, unsigned int length) 
//返回值：分配成功的话就返回申请到的 sk_buff 首地址，失败的话就返回 NULL。
    
//释放函数
void kfree_skb(struct sk_buff *skb) 
//对于网络设备，最好使用这个函数
void dev_kfree_skb (struct sk_buff *skb) 
```

sk_buff的变更函数

`skb_put`在sbk_buff的尾部添加数据，就是将skb_buff的tail后移n个字节

```C
unsigned char *skb_put(struct sk_buff *skb, unsigned int len) 
//返回值：扩展出来的那一段数据区首地址。 
```

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-33.png)

`skb_push`在头部增加数据区

```C
unsigned char *skb_push(struct sk_buff *skb, unsigned int len) 
//返回值：扩展完成以后新的数据区首地址。
```

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035735-34.png)

`sbk_pull`从sk_buff的起始位置删除数据

```C
unsigned char *skb_pull(struct sk_buff *skb, unsigned int len)
//返回值：删除以后新的数据区首地址。 
```

![img](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/1747556035736-35.png)

`sbk_reserve`调整缓冲区的头部大小，将 skb_buff 的 data 和 tail 同时后 移 n 个字节即可

```C
static inline void skb_reserve(struct sk_buff *skb, int len) 
```

## NAPI机制

传统的网络数据包接收方式是基于中断的，当网络设备接收到数据包时，会产生中断通知 CPU 进行处理。然而，频繁的中断会导致较高的上下文切换开销，影响系统性能。NAPI 机制采用了中断与轮询相结合的方式。在有数据包到达时，网络设备先通过中断通知内核，但之后内核会在一个相对稳定的上下文环境中以轮询的方式从设备接收多个数据包，而不是每次都依赖中断，这样可以减少中断的频率，降低上下文切换开销。

NAPI 利用软中断来实现数据包的异步处理。当网络设备产生中断时，会触发一个软中断，将数据包接收任务添加到软中断的任务队列中。内核会在合适的时机（例如在系统空闲时或其他高优先级任务执行完毕后）来处理软中断任务队列中的数据包接收任务，这样可以保证数据包的处理不会立即抢占当前正在执行的重要任务，同时又能及时地处理网络数据。

```C
//初始化napi_struct
void netif_napi_add(struct net_device *dev, struct napi_struct *napi, int (*poll)(struct napi_struct *, int),  int weight) 
//dev：每个 NAPI 必须关联一个网络设备，此参数指定 NAPI 要关联的网络设备。 
//napi：要初始化的 NAPI 实例。 
//poll：NAPI 所使用的轮询函数，非常重要，一般在此轮询函数中完成网络数据接收的工作。 
//weight：NAPI 默认权重(weight)，一般为 NAPI_POLL_WEIGHT。 
    
//删除napi_struct
void netif_napi_del(struct napi_struct *napi) 
    
//使能napi
inline void napi_enable(struct napi_struct *n) 
  
//关闭napi
void napi_disable(struct napi_struct *n) 
    
//检查napi是否可以进行调度
inline bool napi_schedule_prep(struct napi_struct *n) 
//返回值：如果可以调度就返回真，如果不可调度就返回假。 
 
//napi调度
void __napi_schedule(struct napi_struct *n) 

//调度封装
static inline void napi_schedule(struct napi_struct *n) 2 { 
        if (napi_schedule_prep(n)) 
                __napi_schedule(n); 
} 

//napi处理完成，NAPI 处理完成以后需要调用 napi_complete 函数来标记 NAPI 处理完成
inline void napi_complete(struct napi_struct *n) 
```

工作流程：

1. **设备注册与初始化**：在网络设备驱动程序的初始化阶段，会调用`napi_register`函数将设备的`napi_struct`注册到内核中。这个函数会初始化`napi_struct`的各个字段，并将其添加到系统的 NAPI 管理列表中。同时，驱动程序还会设置设备的中断处理函数，以便在数据包到达时能够触发 NAPI 的处理流程。
2. **中断触发**：当网络设备接收到数据包时，会产生硬件中断。设备驱动程序中的中断处理函数会被调用，在这个函数中，会通过`napi_schedule`函数将 NAPI 实例添加到软中断的任务队列中，标记该设备有数据包需要处理，并触发软中断。
3. **软中断处理**：内核在处理软中断时，会遍历软中断任务队列中的 NAPI 实例。对于每个 NAPI 实例，会调用其轮询函数（通常是`poll`函数）来从设备接收数据包。在轮询函数中，会根据设备的状态和接收能力，尽可能多地从设备接收数据包，并将其提交给内核的网络协议栈进行进一步处理。
4. **数据包处理**：轮询函数从设备接收数据包后，会将数据包封装成`struct sk_buff`结构体（用于表示网络数据包的内核数据结构），并通过一系列的函数调用将其传递给内核的网络协议栈进行处理。协议栈会根据数据包的协议类型（如 IP、TCP、UDP 等）进行相应的解析和处理，最终将数据传递到应用层或进行其他的网络操作。
5. **结束处理**：当轮询函数完成一轮数据包的接收和处理后，会检查是否还有更多的数据包需要处理。如果没有，则会将 NAPI 实例从软中断任务队列中移除，并将设备的状态设置为空闲，等待下一次数据包的到达和中断触发。

# 电子书籍网

https://github.com/YoungLC/ebooks.git

# 内核源码在线阅读网站

https://elixir.bootlin.com/
