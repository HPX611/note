# 网络驱动

网络驱动设备需要两个硬件设备：MAC和PHY，MAC就和I2C控制器的功能类似，一般情况下SOC会内置MAC，只需要在外部增加PHY就可以联网，如果SOC内部没有内置这个MAC，就需要在外部增加MAC和PHY

##  SOC内部不带MAC

如果SOC内部不带MAC，则需要在外部增加一个带MAC和PHY集成的一个模块，这个模块和SPC的通信可能就简单使用一些SPI  SRAM等协议进行通信

![image-20250422161749865](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422161749865.png)

## SOC内部自带MAC

还有的SOC内部自带MAC，所以只需要在外部增加PHY的模块的，就能进行网络通信，由于是集成在SOC内部的，所以就会有专门的网络加速引擎，比如网络专用的DMA，这样的网速就会快，同时PHY的成本就低，可选项就多

![image-20250422161840198](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422161840198.png)

## MII/RMII和MDIO

### MII/RMII

这个是一个通信的协议，用于MAC和PHY进行通信的协议，这个是进行网络数据传输的协议，但是主控还需要读取PHY内部的寄存器，这个是MDIO

![image-20250422161958891](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422161958891.png)

这个是MII的通信协议的线，这里的时钟线都是PHY产生给MAC的，如果网速为 100M 的话时钟频率为 25MHz，10M 网速的话时钟频率 为 2.5MHz，特别说明：

* TX_ER：发送错误信号，高电平有效，表示 TX_ER 有效期内传输的数据无效。
* RX_DV：接收数据有效，作用类似 TX_EN。
* CRS：载波侦听信号。
* COL：冲突检测信号。

![image-20250422162543568](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422162543568.png)

这个上RMII的通信协议线，这个是MII的简化，比MII能少9根线，特别说明：

* CRS_DV：相当于 MII 接口中的 RX_DV 和 CRS 这两个信号的混合。
* REF_CLK：参考时钟，由外部时钟源提供， 频率为 50MHz。这里与 MII 不同，MII 的接 收和发送时钟是独立分开的，而且都是由 PHY 芯片提供的。

这里就简述两个，还有其他的GMII、RGMII等，原理基本相同，大同小异

### MDIO

Management Data Input/Output，就两根线，和I2C很像，一个MDIO数据线，一个MDC时钟线，MDIO的接口可以支持32个PHY，但是同一时刻只能和一个PHY进行操作，这个很和I2C很像，需要对PHY就行编址

## RJ45

插网线硬件接口，这个是和PHY进行沟通，但是这个中间需要一个网络变压器，这个变压器用于进行隔离和滤波，当前市面的这个座子基本都集成了变压器

![image-20250422163629891](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422163629891.png)

## PHY

PHY的芯片寄存器地址空间是5位，也就是32个寄存器，IEEE定义了0~15寄存器，这个就是所有的PHY都是一样的，使用这个16个寄存器就能驱动PHY，另外的16个寄存器是厂商定义。但是后面厂商进行了丰富，所以Linux开源PHY驱动可能驱动不起来，所以厂商提供驱动代码并commit到Linux的开源源码中，但是一定根据具体的硬件手册一定能驱动起来，主要也就是控制这个BCR（Basic Control Rgsister）寄存器进行PHY的设置，地址一般是0，和读取BSR（Basic Status Register）寄存器数据进行PHY状态的判断，地址一般是0，还有就是两个ID寄存器，地址一般是2 3

## 代码框架

### 设备申请和注册

在网络驱动的中主要注册net_device结构体，可以使用`alloc_netdev`函数来看申请这个节点，但是申请的本质是调用的`alloc_netdev_mqs`函数，函数的定义如下：

```c
struct net_device * alloc_netdev_mqs ( int sizeof_priv, 
 										const char *name, 
										 void (*setup) (struct net_device *)) 
										 unsigned int txqs, 
										 unsigned int rxqs); 
//新的
struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,
		unsigned char name_assign_type,
		void (*setup)(struct net_device *),
		unsigned int txqs, unsigned int rxqs)
    
//删除的话直接调用
void free_netdev(struct net_device *dev) 
```

这个就是函数调用的原型

* sizeof_priv：私有数据块大小。这个是需要传入的，是这个网路设备自己维护的一些私有数据。
* name_assign_type：新的参数，是和名字的相关的
* name：设备名字。 
* setup：回调函数，初始化设备的设备后调用此函数，在`alloc_netdev_mqs`函数中使用到这个回调函数，用于设置net_device的部分变量
* txqs：分配的发送队列数量。
* rxqs：分配的接收队列数量。

但是该函数是根本调用的函数，后续开源内核对该函数进行了封装，可以直接调用封装好的函数，如`alloc_etherdev`初始化以太网的函数

```c
//注册net_device设备
int register_netdev(struct net_device *dev) 
    
//注销net_device设备
void unregister_netdev(struct net_device *dev) 
```

前面申请以太网的eth设备时，申请的名称是eth%d，这个名称是在`register_netdev -> register_netdevice -> dev_get_valid_name`中确定的

### 数据发送

由于网络是分层的，所以不必过于关心底层是怎么实现的，所以需要注意的是发送的数据包应该怎么封装，怎么发送出去，发送的话主要是通过`dev_queue_xmit`函数进行发送数据，接受的话使用的是`netif_rx`函数

网络数据主要是`sk_buff`结构体进行保存，各层的协议都是在这个结构体的中加自己的协议头，然乎最后层层往下传，最后由底层的驱动将数据发送出去，接受也是同理，底层接受后将其整理为`sk_buff`数据，返回给上层，上层解析，最后将数据返给用户

![image-20250422201234133](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422201234133.png)

sk_buff中存在head data tail end，head 指向缓冲区的头部，data 指向实际数据的头部。data 和 tail 指向实际数据 的头部和尾部，head 和 end 指向缓冲区的头部和尾部。

![image-20250422210400991](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422210400991.png)

```c
//发送数据
static inline int dev_queue_xmit(struct sk_buff *skb) //返回值：0 发送成功，负值 发送失败。

//接受数据
int netif_rx(struct sk_buff *skb) //返回值：NET_RX_SUCCESS 成功，NET_RX_DROP 数据包丢弃。
```

sk_buff申请和释放函数

```c
//申请函数
static inline struct sk_buff *alloc_skb(unsigned int size, gfp_t priority) 
//但是一般使用netdev_alloc_skb来给对应的net_deivce申请sk_buff
static inline struct sk_buff *netdev_alloc_skb(struct net_device *dev, unsigned int length) 
//返回值：分配成功的话就返回申请到的 sk_buff 首地址，失败的话就返回 NULL。
    
//释放函数
void kfree_skb(struct sk_buff *skb) 
//对于网络设备，最好使用这个函数
void dev_kfree_skb (struct sk_buff *skb) 
```

sk_buff的变更函数

`skb_put`在sbk_buff的尾部添加数据，就是将skb_buff的tail后移n个字节

```C
unsigned char *skb_put(struct sk_buff *skb, unsigned int len) 
//返回值：扩展出来的那一段数据区首地址。 
```

![image-20250422210414422](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422210414422.png)

`skb_push`在头部增加数据区

```C
unsigned char *skb_push(struct sk_buff *skb, unsigned int len) 
//返回值：扩展完成以后新的数据区首地址。
```

![image-20250422210540418](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422210540418.png)

`sbk_pull`从sk_buff的起始位置删除数据

```C
unsigned char *skb_pull(struct sk_buff *skb, unsigned int len)
//返回值：删除以后新的数据区首地址。 
```

![image-20250422210646514](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250422210646514.png)

`sbk_reserve`调整缓冲区的头部大小，将 skb_buff 的 data 和 tail 同时后 移 n 个字节即可

```	C
static inline void skb_reserve(struct sk_buff *skb, int len) 
```

## NAPI机制

传统的网络数据包接收方式是基于中断的，当网络设备接收到数据包时，会产生中断通知 CPU 进行处理。然而，频繁的中断会导致较高的上下文切换开销，影响系统性能。NAPI 机制采用了中断与轮询相结合的方式。在有数据包到达时，网络设备先通过中断通知内核，但之后内核会在一个相对稳定的上下文环境中以轮询的方式从设备接收多个数据包，而不是每次都依赖中断，这样可以减少中断的频率，降低上下文切换开销。

NAPI 利用软中断来实现数据包的异步处理。当网络设备产生中断时，会触发一个软中断，将数据包接收任务添加到软中断的任务队列中。内核会在合适的时机（例如在系统空闲时或其他高优先级任务执行完毕后）来处理软中断任务队列中的数据包接收任务，这样可以保证数据包的处理不会立即抢占当前正在执行的重要任务，同时又能及时地处理网络数据。

```c
//初始化napi_struct
void netif_napi_add(struct net_device *dev, struct napi_struct *napi, int (*poll)(struct napi_struct *, int),  int weight) 
//dev：每个 NAPI 必须关联一个网络设备，此参数指定 NAPI 要关联的网络设备。 
//napi：要初始化的 NAPI 实例。 
//poll：NAPI 所使用的轮询函数，非常重要，一般在此轮询函数中完成网络数据接收的工作。 
//weight：NAPI 默认权重(weight)，一般为 NAPI_POLL_WEIGHT。 
    
//删除napi_struct
void netif_napi_del(struct napi_struct *napi) 
    
//使能napi
inline void napi_enable(struct napi_struct *n) 
  
//关闭napi
void napi_disable(struct napi_struct *n) 
    
//检查napi是否可以进行调度
inline bool napi_schedule_prep(struct napi_struct *n) 
//返回值：如果可以调度就返回真，如果不可调度就返回假。 
 
//napi调度
void __napi_schedule(struct napi_struct *n) 

//调度封装
static inline void napi_schedule(struct napi_struct *n) 2 { 
	if (napi_schedule_prep(n)) 
		__napi_schedule(n); 
} 

//napi处理完成，NAPI 处理完成以后需要调用 napi_complete 函数来标记 NAPI 处理完成
inline void napi_complete(struct napi_struct *n) 
```



工作流程：

1. **设备注册与初始化**：在网络设备驱动程序的初始化阶段，会调用`napi_register`函数将设备的`napi_struct`注册到内核中。这个函数会初始化`napi_struct`的各个字段，并将其添加到系统的 NAPI 管理列表中。同时，驱动程序还会设置设备的中断处理函数，以便在数据包到达时能够触发 NAPI 的处理流程。
2. **中断触发**：当网络设备接收到数据包时，会产生硬件中断。设备驱动程序中的中断处理函数会被调用，在这个函数中，会通过`napi_schedule`函数将 NAPI 实例添加到软中断的任务队列中，标记该设备有数据包需要处理，并触发软中断。
3. **软中断处理**：内核在处理软中断时，会遍历软中断任务队列中的 NAPI 实例。对于每个 NAPI 实例，会调用其轮询函数（通常是`poll`函数）来从设备接收数据包。在轮询函数中，会根据设备的状态和接收能力，尽可能多地从设备接收数据包，并将其提交给内核的网络协议栈进行进一步处理。
4. **数据包处理**：轮询函数从设备接收数据包后，会将数据包封装成`struct sk_buff`结构体（用于表示网络数据包的内核数据结构），并通过一系列的函数调用将其传递给内核的网络协议栈进行处理。协议栈会根据数据包的协议类型（如 IP、TCP、UDP 等）进行相应的解析和处理，最终将数据传递到应用层或进行其他的网络操作。
5. **结束处理**：当轮询函数完成一轮数据包的接收和处理后，会检查是否还有更多的数据包需要处理。如果没有，则会将 NAPI 实例从软中断任务队列中移除，并将设备的状态设置为空闲，等待下一次数据包的到达和中断触发。



-------



# Linux驱动开发教程

Linux的驱动开发教程代码：https://github.com/huabowen/linux_kernel_study.git

## 内核模块

linux是宏内核，其和微内核的区别是将所有的内核功能整体编译，形成单个镜像文件，后面加载到内存中跑起来，这个的优点是效率高，各个功能模块的交互是直接调用函数的。微内核的是实现内核的关键和核心函数，其他的模块是单独编译的，功能模块之间的交互需要使用微内核提供的通信机制。

宏内核的缺点是在更改时要重新编译整个内核，在重新启动系统，所以后续开发了内核模块，用于动态增加和卸载内核模块，系统不必重启，开发快速

```c
insmode ; depmod | modprobe  	//加载内核模块
modinfo 						//显示内核模块的信息
rmmod 							//卸载内核模块

MODULE_LICENSE					//代表相应的许可证协议  一般是GPL
MODULE_AUTHOR					//描述模块作者的信息
MODULE_DESCRIPTION				//模块的详细说明描述
MODULE_ALIAS					//提供给用户空间一个更合适的别名，也就是给这个模块去取一个别名
```

在模块的初始化函数和卸载函数只会调用一次，所以在前面加`__init`和`__exit`标签，这个模块在编译时会将这个函数放在ELF文件的特定代码段，然后加载时会单独分配内存，在函数调用后，模块的加载程序会释放这个部分的内存，而`__exit`修饰的函数是在模块卸载时才加载这个函数，如果模块不允许卸载，这个代码就不会被加载

### 多文件编译内核模块

多个文件编译成一个ko文件，实现是在makefile中定义的

```makefile
#单个文件编译出ko
obj-m := vser.o

#多个文件编译出ko
obj-m := vser.o
vser-objs = foo.o bar.o
```

重要的更改是`vser-objs = foo.o bar.o`，这个说明了vser模块需要foo.o和bar.o这两个文件共同生成

```makefile
# 同时指定两个要编译成内核模块的目标文件
obj-m := vser.o another_module.o

# 定义构成 vser.o 的子目标文件
vser-objs := vser.o foo.o bar.o
# 定义构成 another_module.o 的子目标文件
another_module-objs := another_module.o helper.o

# 获取当前内核源码的构建目录
KDIR := /lib/modules/$(shell uname -r)/build
# 获取当前工作目录
PWD := $(shell pwd)

# 默认目标，编译内核模块
default:
    $(MAKE) -C $(KDIR) M=$(PWD) modules

# 清理目标，清除编译生成的文件
clean:
    $(MAKE) -C $(KDIR) M=$(PWD) clean
```

`obj-m` 是一个在编译内核模块时使用的特殊变量，其作用在于告知内核构建系统需要将哪些目标文件编译成内核模块。

`*-objs` 是一个自定义的变量，其用途是指定构成 `vser.o` 目标文件的子目标文件列表。

### 内核模块参数

就是内核模块在加载的时候获取命令行的参数

```c
module_param(name,type,perm)
module_param_array(name,type,nump,perm)
```

上面是用来将那个变量声明为模块参数，name是变量名，type是变量类型，nump是数组的个数，perm是在sysfs文件系统中对应文件的权限属性

```c
//这个是命令行加载的格式，name3是数组，传参用','分割
modprobe vser
modprobe vser name1=* name2=* name3=1,2,3,4
```

### 内核模块依赖

`EXPORT_SYMBOL`这个宏是将函数导出，这个是生成特定结构放在ELF文件的特定段，内核启动的时候，会将这个符号的确切地址放在这个结构的特定成员，模块在加载时，对于未决符号就是在特殊段中去找这个符号的名字，找到就将获得的地址填充在被加载模块的相应段中，也就时连接步骤后移，在加载内核模块的时候，对之前编译未确定的符号或函数进行连接，故前面编译的ko文件只是编译，并没有连接。

同时使用这个宏进行导出，就是产生这个模块的依赖关系（使用这个函数的模块和定义这个函数的模块）

在存在依赖关系的两个模块的编译时需要放在一起进行编译，或者先将被依赖的模块加载到内核中，在编译另一个模块，不然会产生找不到的情况

内核模块在编译的时候会保留这边编译平台和版本的信息，在加载的时候内核会核对这些信息，不一致则认为非法模块，加载失败



## 字符设备驱动

字符设备驱动：设备对数据是按照字节流进行控制的，可以支持随机访问，也可以不支持随机访问，由于数据流量不大，一般没有页高速缓存

块设备驱动：设备对数据的处理是按照块进行的，一个块有固定的大小，这个类设备都支持随机访问，为了提高效率可以将之前用到的数据缓存起来

网络设备驱动：就是针对网络设备的一类驱动，主要就是进行网络数据的收发

设备的在内核的区分是设备号，主设备号是区分一类设备的，次设备号是区分一类设备的不同个体或不同分区，具体的设备名称是用户进行设备区分的

驱动文件是在/dev下，以文件的形式存在，这个文件描述的结构是同一的，所以用户能通过文件来操作设备，同时文件的描述存在`i_block`这个变量，如果是普通文件，这个是存对应的放文件数据的块号，如果是设备文件，这个放的是设备号，后续根据这个来判断这个是操作设备文件还是普通文件

### 从用户调到底层驱动程序

上层`open`函数的调用

```shell
sys_open(fs/open.c)
  |- do_sys_open(fs/open.c)
     |- getname(fs/namei.c)
     |- get_unused_fd_flags(fs/file.c)
     |- do_filp_open(fs/namei.c)
     |  |- path_openat(fs/namei.c)
     |     |- get_empty_filp(fs/file_table.c)
     |     |- link_path_walk(fs/namei.c)
     |     |- do_last(fs/namei.c)
     |        |- lookup_fast(fs/namei.c)
     |        |- lookup_open(fs/namei.c)
     |           |- lookup_dcache(fs/namei.c)
     |           |- lookup_real(fs/namei.c)
     |              |- ext2_lookup(fs/ext2/namei.c)
     |                 |- ext2_iget(fs/ext2/inode.c)
     |                    |- init_special_inode(fs/inode.c)
     |                      |- inode->i_fop = &def_chr_fops;
     |- fd_install(fs/namei.c)
```

![image-20250424125050608](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250424125050608.png)

用户调用`open`函数会调用到`sys_open`函数，这个调用到`do_sys_open`函数，这个内部先调用`getname`将文件名称拷贝到内核空间，然后使用`get_unused_fd_flags`在files_struct的fd_array中找一个没有用的文件描述符，返回对应的下标，下来就是使用`do_file_open`构造一个file接口并初始化这个结构，这个里面重要的是将这个f_op和设备驱动的file_operations这个进行关联，并初始化设备，最后调用`fd_install`让这个fd_array获取的下标执行这个新生成的file文件，最后系统用将这个新的下标返回给用户层

`do_file_open`是重要的操作，这个是会根据传入的文件路径，解析这个path，最后不断的找到对应这个文件对应的inode（磁盘上的inode），最后会获取到这个mknod命令创建的这个设备文件的信息，判断这个文件的类型，如果对于字符设备驱动，最重要的是将文件类型和设备号要填充到对应的内存的inode，同时将f_op指向def_chr_fops，然后这个里面存在`chrdev_open`函数，这个会根据设备号在cdev_map这个散列表中找到当时注册的cdev，这个cdev会驱动自己构造的file_operation结构进行关联，然后就是将驱动实现的file_operations替换了file的f_op，调用对应这个设备的open函数执行，初始化设备

```C
//初始化cdev结构，主要是将fops和这个cdev就进行关联
void cdev_init(struct cdev* cdev, const struct file_operations * fops);
//将cdev添加到散列表中，这个主要的工作是将主设备号对255取余，余数作为cdev_map下标的索引，构造一个probe的对象，添加到链表中，其中的data指向的就是cdev的地址
int cdev_add(struct cdev* p, dev_t dev, unsigned count);
```

### 一个驱动支持多个设备

因为是一个驱动需要适配多个设备同时使用，所以是使用file的private变量进行区分的，根据open函数的inode的信息，区分不同的设备，对不同设备的file的private变量赋予不同的值在其他的f_op的操作函数中，使用file的private变量来进行对单个设备的操作

例子：[一个cdev控制多个设备](https://github.com/huabowen/linux_kernel_study/blob/master/chrdev/ex4/vser.c)       [一个cdev控制一个设备，多次注册](https://github.com/huabowen/linux_kernel_study/blob/master/chrdev/ex5/vser.c)



## IO操作

### ioctl

对于一个设备来说，read和write是一般走的数据，但是对于设备的控制来说，应该走ioctl通道（可以使用write实现）

`ioctl`系统调用的驱动接口可以是`unlocked_ioctl`和`compat_ioctl`，`compat_ioctl`是为了兼容32位和64位机

#### `_IO(type, nr)`

- **用途**：该宏用于定义一个无参数的`ioctl`命令。
- 参数解释：
  - `type`：这是一个表示设备类型的魔数（magic number），通常是一个字符，用于区分不同类型的设备。魔数的取值范围一般在`0x00 - 0xFF`之间，要确保其唯一性，避免与其他设备的魔数冲突。
  - `nr`：是一个命令编号，同样需要保证其在该设备类型内的唯一性，一般取值范围是`0 - 255`。

#### `_IOR(type, nr, size)`

- **用途**：用于定义一个从内核空间向用户空间传递数据的`ioctl`命令。
- 参数解释：
  - `type`：设备类型魔数，作用同`_IO`宏中的`type`。
  - `nr`：命令编号，作用同`_IO`宏中的`nr`。
  - `size`：要传递的数据的大小（以字节为单位），内核会根据这个大小来检查传递数据的合法性。

#### `_IOW(type, nr, size)`

- **用途**：用于定义一个从用户空间向内核空间传递数据的`ioctl`命令。
- 参数解释：
  - `type`：设备类型魔数。
  - `nr`：命令编号。
  - `size`：要传递的数据的大小（以字节为单位）。

#### `_IOWR(type, nr, size)`

- **用途**：用于定义一个既可以从用户空间向内核空间传递数据，又可以从内核空间向用户空间传递数据的`ioctl`命令。
- 参数解释：
  - `type`：设备类型魔数。
  - `nr`：命令编号。
  - `size`：要传递的数据的大小（以字节为单位）。

### proc

proc是一种伪文件系统，只存在内存中，只有内核运行才会动态生成内部内容，这个可以类似/dev下的节点，创建对应的`file_operations`然后再/proc下创建对应的文件，最后使用文件来更改这个内核模块的内容 [proc接口使用例子](https://github.com/huabowen/linux_kernel_study/blob/master/advio/ex2/vser.c)

### IO分类

#### 阻塞 IO（Blocking IO）

- **概念**：在进行 I/O 操作时，若数据未准备好或无法立即读写，程序会被阻塞（即调用被挂起），一直等待数据准备好或操作完成，才能继续执行后续操作 。这是一种同步 I/O 操作方式。
- **优点**：编程模型简单易懂，对于简单的 I/O 操作场景，逻辑清晰，容易实现和维护。
- **缺点**：在等待 I/O 操作完成期间，进程或线程会被阻塞，无法执行其他任务，严重影响程序的并发处理能力。当有大量 I/O 操作且 I/O 操作耗时较长时，会导致系统资源浪费和性能低下。

#### 非阻塞 IO（Non - Blocking IO）

- **概念**：进行 I/O 操作时，如果数据没有准备好或无法立即读写，程序不会等待，而是立即返回，并继续执行后续操作。之后程序通过轮询（不断重复检查）或使用选择函数（如`select`、`poll`、`epoll` 等）来检查是否有 I/O 操作可进行。
- **优点**：程序不会被 I/O 操作阻塞，可在等待 I/O 的同时执行其他任务，提高了程序的并发性能，能更高效地利用 CPU 资源。
- **缺点**：需要不断轮询检查 I/O 状态，会消耗一定的 CPU 资源。而且编程复杂度相对较高，需要处理各种返回状态和错误情况。

#### 异步 IO（Asynchronous IO）

- **概念**：程序发起 I/O 请求后，无需等待 I/O 操作完成，可继续执行其他任务。当 I/O 操作完成后，系统会通过特定机制（如回调函数、信号等）通知程序，程序再去获取或处理 I/O 操作的结果 。例如在一些支持异步 I/O 的系统中，通过注册回调函数，当 I/O 操作完成时，操作系统会自动调用该回调函数来处理结果。
- **优点**：极大地提高了程序的并发性和响应性能，能充分利用系统资源，让程序在 I/O 操作的同时执行其他计算任务，适合处理大量 I/O 操作的高并发场景。
- **缺点**：编程模型复杂，需要处理异步事件的回调逻辑，涉及到多线程、多进程或异步编程框架等知识，增加了代码编写和调试的难度。同时，需要处理好异步操作中的竞态条件和资源管理问题。

#### IO 多路复用（IO Multiplexing）

- **概念**：通过一种特殊的函数（如`select`、`poll`、`epoll` 等），让一个或少数几个线程能够同时监控多个文件描述符（fd，如网络连接的 socket 描述符）的 I/O 状态变化 。当被监控的 fd 中至少有一个有数据准备就绪（可读、可写或有异常等）时，函数返回，程序再针对就绪的 fd 进行相应的 I/O 操作（如调用`recvfrom`读取数据）。
- **优点**：相比为每个 I/O 操作创建一个线程或进程，能显著减少系统资源消耗（如线程或进程的创建和维护开销），提高系统并发处理能力，适用于处理大量并发连接的网络服务器等场景。
- **缺点**：`select`和`poll`函数存在一定性能瓶颈（如`select`对监控的 fd 数量有限制，且采用轮询方式效率较低），虽然`epoll`在性能上有很大提升，但整体编程复杂度较高，需要对这些函数的使用和原理有深入理解。

### 异步通知

前面的IO都是应用层来驱动主动获取数据，而异步通知则是驱动给应用层信号，说明有信息可以获取，应用层再来驱动层拿   



## 中断和时间

Linux的中断处理是一个很长的过程，汇编获得硬件中断号后会转到内核对应的中断号，然后根据中断号转化出`irq_desc`这个结构体，这个结构体在内核的存在形式是数组或基数树，这个根据获得的这个结构体找到对应的`action`这个结构是一个指向`irqaction`结构体的指针，会调用里面的函数，然后这个是一个链表结构，会不断的执行后续的中断，直到链表遍历完毕。

也就是说，多个中断会共享一个中断号，在这个中断号发生中断时，内部的中断函数都会执行，所以需要驱动判断中断是否是自己的管理的设备发生的

![image-20250425173956686](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250425173956686.png)

### 驱动的中断

也就是说，驱动实现的中断也就是构造`irqaction`这个结构体并注册到内核中，发生中断会调到handle对应的中断服务函数

```C
int request_irq(unsigned int irq, irqreturn_t (*handler)(int, void *, struct pt_regs *), 
                unsigned long irqflags, const char *devname, void *dev_id); 
void free_irq(unsigned int irq, void *dev_id);
```

* irq：设备使用的中断号，不是硬件手册上的，是内核对应的IRQ的号，这个号决定了`irqaction`结构会插入到那个链表中
* handler：注册的中断服务函数
* irqflags：中断标志，是初始化`irqaction`的flags变量，描述中断如何触发等
* name： 在/proc结构中显示的名字，也是`irqaction`的名字
* dev_id：设备号，在链表中用于区分这个各个设备的中断，这个变量会传回中断服务函数

### 中断下半部

#### 软中断

这个是内核支持的一个中断，存在一个全局变量和一个软中断的一个函数数组

![image-20250425181227921](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250425181227921.png)

这个是硬件最后调用的`handle_IRQ`函数，`irq_enter`函数会给中断进程中的抢占计数器`preempt_count`加上`HARDIRQ_OFFSET`这个值，用来防止内核抢占，同时是判断是否在硬件中断中的一个判断，当内部定义的硬件中断结束后，会调用`irq_exit`函数来释放前面加的`HARDIRQ_OFFSET`变量，这个时候硬件的中断服务函数也就是上半部是执行完成了，但是在没有完全返回的时候会判断是否有软中断需要执行

![image-20250425181735424](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250425181735424.png)

这个就是去掉`HARDIRQ_OFFSET`标志，判断是否有软中断存在，存在则调用`invoke_softirq`函数执行软中断，判断存在的软中断并执行对应的软中断，将所有的已将要发生的软中断处理完成再返回

#### tasklet

因为软中是全局变量并定死的，所以留了一个tasklet的软中断接口，用于处理一般大部分的中断函数，也就以另一种软中断的形式，同时不妨碍其他的软中断

也就是说，驱动要用的话就是设置对应的tasklet_struct并注册到内核中，在上半部标记对应的softirq的标志，这个下半部就会执行，但是这个存在一个缺点，因为是用标志的，所以存在执行次数和提交次数不一致的情况

![image-20250425182136940](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250425182136940.png)

## RCU（read copy update）

RCU机制就是读—复制—更新，RCU对共享内存是通过指针来实现的，读是对指针解引用来访问，写是先将这部分内存复制一份进行更改，当写结束后，等所有的读操作完成对原数据的读取后，将读指针更新为写的新指针，后续读到的就是新数据

```C
#include <linux/init.h>
#include <linux/module.h>
#include <linux/rcupdate.h>
#include <linux/slab.h>

// 定义一个简单的数据结构
struct my_data {
    int value;
};

// 全局指针，指向共享数据
static struct my_data *my_shared_data;

// 读函数，使用 RCU 读取数据
static int read_data(void)
{
    struct my_data *data;
    int value;

    rcu_read_lock();
    data = rcu_dereference(my_shared_data);
    if (data)
        value = data->value;
    else
        value = -1;
    rcu_read_unlock();

    return value;
}

// 写函数，使用 RCU 更新数据
static void write_data(int new_value)
{
    struct my_data *new_data, *old_data;

    // 分配新的数据结构并初始化
    new_data = kmalloc(sizeof(*new_data), GFP_KERNEL);
    if (!new_data)
        return;
    new_data->value = new_value;

    // 使用 RCU 进行更新
    old_data = xchg(&my_shared_data, new_data);
    if (old_data) {
        // 等待宽限期结束
        synchronize_rcu();
        // 释放旧数据
        kfree(old_data);
    }
}

// 模块初始化函数
static int __init rcu_example_init(void)
{
    // 初始化共享数据
    my_shared_data = kmalloc(sizeof(*my_shared_data), GFP_KERNEL);
    if (!my_shared_data)
        return -ENOMEM;
    my_shared_data->value = 0;

    printk(KERN_INFO "RCU example module initialized, initial value: %d\n", read_data());

    return 0;
}

// 模块退出函数
static void __exit rcu_example_exit(void)
{
    struct my_data *data;

    // 等待所有 RCU 读临界区完成
    synchronize_rcu();

    data = my_shared_data;
    if (data) {
        // 释放共享数据
        kfree(data);
    }
    printk(KERN_INFO "RCU example module exited.\n");
}

module_init(rcu_example_init);
module_exit(rcu_example_exit);

MODULE_LICENSE("GPL");    
```



## 内存和DMA

### 内存组织

#### UMA（统一内存访问）

- **架构特点**：在 UMA 架构中，所有处理器访问内存的速度是一致的。内存被视为一个统一的资源，均匀地分布在各个处理器之间，处理器通过共享的总线或高速缓存一致性协议来访问内存。
- 优点
  - **编程简单**：由于处理器访问内存的方式相同，软件开发人员无需考虑内存访问的差异性，编程模型相对简单，易于理解和实现。
  - **数据一致性容易维护**：共享内存空间使得多个处理器对数据的访问和修改能够通过统一的机制进行管理，保证了数据的一致性。
- 缺点
  - **可扩展性有限**：随着处理器数量的增加，共享总线或一致性协议会成为瓶颈，限制系统性能的提升。
  - **内存带宽竞争**：多个处理器同时访问内存时，容易产生内存带宽的竞争，导致性能下降。

#### NUMA（非统一内存访问）

- **架构特点**：NUMA 架构将内存分布在不同的节点上，每个节点都有自己的本地内存和处理器。处理器访问本地内存的速度比访问其他节点的内存速度快，因此内存访问时间是非均匀的。节点之间通过高速互联网络进行通信。
- 优点
  - **良好的可扩展性**：通过增加节点，可以有效地扩展系统的处理器和内存容量，避免了 UMA 架构中共享总线的瓶颈问题，适合构建大规模的多处理器系统。
  - **减少内存带宽竞争**：处理器优先访问本地内存，减少了对全局内存带宽的竞争，提高了系统的整体性能。
- 缺点
  - **编程复杂**：开发人员需要考虑数据在不同节点上的分布情况，以优化内存访问性能，这增加了编程的复杂性。
  - **数据一致性管理困难**：由于数据可能分布在不同节点的内存中，需要更复杂的机制来保证数据的一致性，增加了系统设计和维护的难度。

![image-20250425210126214](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250425210126214.png)

Linux在内存组织中将最高层次定义为内存节点，所以可以将UMA看为特殊的NUMA系统，也就是只有一个内存节点的系统，这样在分配内存的时候，优先考虑在CPU的本地内存对应的节点分配空间，如果不行就从非本地的内存分配

下来将内存按区进行划分（这个看得选考虑虚拟内存映射的关系）：

1. ZONE_DMA
   - **适用场景**：适合 DMA（Direct Memory Access，直接内存访问）操作的内存区。
   - **x86 系统情况**：在 x86 系统上，受 ISA 总线地址总线宽度限制，它只能访问最低的 16MB 区域内存，所以 DMA 内存区被限制在这 16MB 内存区域内。
   - **SoC 系统情况**：像 ARM 这样的 SoC（System on Chip，片上系统）系统，DMA 内存区通常无此限制。
2. ZONE_DMA32
   - **适用场景**：在 64 位系统上，供使用 32 位地址寻址且适合 DMA 操作的内存区。
   - **举例**：在 AMD64 系统上，该区域为低 4GB 的空间。在 32 位系统上，这个区域通常为空 。
3. ZONE_NORMAL
   - **定义**：常规内存区域，可直接映射到内核空间的内存。直接映射指物理地址和虚拟地址间存在简单关系，物理地址加上固定偏移就得到虚拟内存地址。
   - **32 位系统情况**：若用户空间和内核空间以 3GB 为界划分，内核空间仅 1GB 。除去特殊用途的一段内核内存空间（通常是高 128MB 内存空间），常规内存区域一般指低于 896MB 的物理内存，是内核空间中使用最频繁的内存段。
4. ZONE_HIGHMEM
   - **32 位系统情况**：在 32 位系统上，通常指高于 896MB 的物理内存。
   - **64 位系统情况**：64 位系统内核空间大，一般不存在高端内存概念。
   - **映射特点**：要将这段物理内存映射到内核空间，需单独映射，且不能保证物理地址和虚拟地址间有常规内存那样的固定对应关系。
5. ZONE_MOVABLE
   - **性质**：伪内存区域。
   - **用途**：主要用于防止物理内存碎片化。

最后是将区按照页进行划分，也就是操作系统能管理的page，这个的大小是由MMU来决定的，内核最基本的内存分配和释放是按照页来进行的

### slab

内存是按照页进行分配的，但是页对于日常使用来说太大了，所以出现了slab实现小内存的管理。

#### 实现原理

- **对象分类与缓存**：Slab 将内核中经常使用的小对象按照类型和大小进行分类，为每一类对象创建一个专门的 Slab 缓存。每个缓存由多个 Slab 组成，每个 Slab 由一个或多个物理页组成。例如，对于 `task_struct`、`inode` 等不同类型的结构体，都有各自对应的 Slab 缓存。这样的设计可以将相同类型的对象集中管理，提高分配和释放的效率。
- **Slab 状态管理**：Slab 有三种状态，分别是满（full）、部分满（partial）和空（empty）。内核在分配对象时，优先从部分满的 Slab 中查找空闲对象。如果没有部分满的 Slab，则选择空 Slab 进行分配。当一个 Slab 中的所有对象都被分配出去，它就变成了满 Slab；而当所有对象都被释放后，Slab 就变为空 Slab。通过这种状态管理，Slab 分配器可以快速定位到适合分配对象的 Slab。
- **内存分配与回收**：当内核请求分配一个小对象时，Slab 分配器首先在对应的 Slab 缓存中查找空闲对象。如果缓存中没有可用的对象，Slab 分配器会向伙伴系统申请分配新的物理页来创建一个新的 Slab。当对象被释放时，Slab 分配器将其标记为空闲，并将其放回原来的 Slab 中，而不是立即将内存归还给伙伴系统。这样，后续的分配请求可以直接从 Slab 中获取空闲对象，减少了内存分配和释放的开销。
- **缓存优化**：Slab 分配器采用了多种缓存优化技术。例如，颜色缓存机制通过对 Slab 进行不同的颜色标记，使不同的 Slab 在内存中的对齐方式不同，从而尽可能均匀地分布在 CPU 缓存中，减少缓存冲突，提高缓存命中率。另外，Slab 分配器还支持对象构造和析构函数，在对象分配和释放时可以自动调用这些函数，对对象进行初始化和清理工作，方便内核对特殊对象的管理。

|                  | Slab                                                         | 伙伴系统                                                     |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **管理对象**     | 侧重于管理内核中频繁使用的小对象，如进程描述符、文件系统的 inode 节点、各种内核数据结构等。 | 主要管理以页为单位的大块连续内存空间，通常用于满足较大内存分配请求，如为内核模块、驱动程序分配较大的缓冲区等。 |
| **分配策略**     | 将内存划分为多个 Slab 缓存，每个缓存针对特定类型和大小的对象。分配对象时，优先从部分满的 Slab 中查找空闲对象，如果没有则从空 Slab 中分配，当缓存中没有可用的 Slab 时，才向伙伴系统请求新的物理页来创建 Slab。 | 按照 2 的幂次方大小来划分和分配内存块。当有内存分配请求时，它会查找最接近请求大小的 2 的幂次方的空闲内存块进行分配，如果没有合适大小的空闲块，就从更大的块中拆分 |
| **内存碎片处理** | 由于针对特定类型的小对象进行管理，在小对象的分配和释放过程中，通过 Slab 缓存机制可以有效减少内部碎片，但对于 Slab 缓存之间的外部碎片管理相对较弱。 | 通过合并相邻的空闲内存块来减少内存碎片。当内存块被释放时，会检查其相邻的内存块是否为伙伴（大小相同且地址连续），如果是则将它们合并成更大的内存块。 |

#### 关系

- **互补关系**：伙伴系统为 Slab 提供了底层的内存支持，当 Slab 需要更多内存来创建新的 Slab 时，会向伙伴系统请求分配物理页。而 Slab 是对伙伴系统分配的内存进行进一步的细分和管理，针对小对象的频繁分配和释放进行优化，避免了频繁地向伙伴系统申请和释放内存，从而提高了整个内存管理系统的效率。
- **协同工作**：在内存分配过程中，两者协同工作。例如，当内核需要分配一个小对象时，首先由 Slab 分配器在其缓存中查找空闲对象，如果 Slab 缓存中没有可用的对象，则 Slab 分配器向伙伴系统申请内存来创建新的 Slab，以满足小对象的分配需求。在内存释放时，Slab 将释放的对象标记为空闲并放回原 Slab，当 Slab 中的对象全部释放后，Slab 可能会被销毁，其占用的内存会归还给伙伴系统。



### DMA

这个是实现数据的传输，让CPU去做其他事情

![image-20250426125239798](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250426125239798.png)

DMA工作需要的数据是源地址、目的地址、传输字节数和传输方向，因为DMAC数据传输不过MMU，所以这个地址需要时物理地址，不能是虚拟地址，所以在驱动中使用DMA进行数据传输需要将虚拟地址转为物理地址再进行DMA的设置

上图可以看出CPU一般是从cache中拿数据，DMA只能对内存的数据进行传输，所以存在数据一致性的问题，所以驱动程序必须保证用于DMA操作的内存关闭高速缓存的特点     [DMA传输的一个合理的例子](https://github.com/huabowen/linux_kernel_study/blob/master/dma/ex1/memcpy.c)



## Linux设备模型

### kobject kset

kobject注册到内核中会在sysfs文件系统中创建一个目录，kobject的attr属性则是创建对应的文件，kset则是对多个kobject对象的一个集合，kset内部内嵌了一个kobject，所以这个也会以文件的形式展示，内嵌的kobject的attr属性是这个目录的文件，然后这个用来管理多个kobject的，内嵌的kobject就是其管理的kobject的父节点，所以就会形成树形结构，同意kset管理的kobject就是兄弟关系

```C
#include <linux/init.h>
#include <linux/kernel.h>
#include <linux/module.h>

#include <linux/slab.h>
#include <linux/kobject.h>

static struct kset *kset;
static struct kobject *kobj1;
static struct kobject *kobj2;
static unsigned int val = 0;

static ssize_t val_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
	return snprintf(buf, PAGE_SIZE, "%d\n", val);
}

static ssize_t val_store(struct kobject *kobj, struct kobj_attribute *attr, const char *buf, size_t count)
{
	char *endp;

	printk("size = %zu\n", count);
	val = simple_strtoul(buf, &endp, 10);

	return count;
}

static struct kobj_attribute kobj1_val_attr = __ATTR(val, 0444, val_show, val_store);
static struct attribute *kobj1_attrs[] = {
	&kobj1_val_attr.attr,
	NULL,
};

static struct attribute_group kobj1_attr_group = { 
	        .attrs = kobj1_attrs,
};

static int __init model_init(void)
{
	int ret;

	kset = kset_create_and_add("kset", NULL, NULL);
	kobj1 = kobject_create_and_add("kobj1", &kset->kobj);
	kobj2 = kobject_create_and_add("kobj2", &kset->kobj);

	ret = sysfs_create_group(kobj1, &kobj1_attr_group);
	ret = sysfs_create_link(kobj2, kobj1, "kobj1");

	return 0;
}

static void __exit model_exit(void)
{
	sysfs_remove_link(kobj2, "kobj1");
	sysfs_remove_group(kobj1, &kobj1_attr_group);
	kobject_del(kobj2);
	kobject_del(kobj1);
	kset_unregister(kset);
}

module_init(model_init);
module_exit(model_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Kevin Jiang <jiangxg@farsight.com.cn>");
MODULE_DESCRIPTION("A simple module for device model");
```

![image-20250426135325929](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250426135325929.png)

### 动态加载

Linux通过udev来实现内核模块的动态加载，udev是一个用户线程，任何设备的添加和删除都会导致内核给用户空间发送响应的通知，事件叫uevent，用户空间的udev捕获这些信息来实现自动加载驱动、自动创建和删除设备节点等，这个也可以自己定义规则

所以Linux的内核模块并不都是内核启动就加载上去的，当设备添加时再加载对应的驱动，但是一个驱动可以控制多个设备，所以内核模块的别名就启了很大的作用，下面是module.alias文件的信息，前面的是对应模块别名信息（一般是能和这个驱动匹配的设备的信息，可以有多个别名），后面的是模块的信息，也就是内核再识别到对应的端口的设备后，会添加对应的一个设备的信息，设备的名称会被udev获取，udev再根据这个名称（驱动的别名）来找到对应的驱动，这样就可以实现多个设备只要有一个插入就可以加载对应的驱动，如下：多个pci端口的设备就匹配一个atyfd的设备

```shell
alias pci:v00001002d00004750sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004749sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004744sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004742sv*sd*bc*sc*i* atyfb
alias pci:v00001002d0000475Asv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004759sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004757sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004756sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00005656sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004C47sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004755sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00005655sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00004754sv*sd*bc*sc*i* atyfb
alias pci:v00001002d00005654sv*sd*bc*sc*i* atyfb
```



## 总线类

### I2C

基本就不说了，I2C上可以有多个主机控制器（Master   Adapter）和从设备（Slave   Client），多个从设备是根据设备地址来区分的，地址分为7位和10位，但是一般是7位

![image-20250426210323881](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250426210323881.png)

* 开始位：当 SCL 为高电平时，SDA 由高电平变为低电平的期间，如图 10.2 中最左边的 START condition 所标记的区域，这表示主机控制器要开始对从机发起访问了。
* 地址位：接下来的 7 个时钟周期，主机控制器将会发送从机的 7 位地址（如果是 10 位地址需要分两次发送），如图 10.2 中 ADDRESS 所标记的区域。
* 读 / 写位：在第 8 个时钟周期，如果 SDA 为高电平则表示接下来要读取从机的数据，如果是低电平则表示主机要写数据到从机，如图 10.2 中 R/W 所标记的区域。
* 应答位：在第 9 个时钟周期由从机进行应答，低电平为 ACK，高电平为 NACK，如果从机响应，应该发 ACK。
* 数据位：在接下来的若干个周期内，主机可以持续读取数据（如果读 / 写位为读），或写数据（如果读 / 写位为写），每次数据传输完成（如图 10.2 中的 DATA 所标记的区域）也要进行应答，是读则由主机控制器来应答，是写则由从机来应答，只是在主机读完最后一个字节的数据后应该以 NACK 来应答。
* 停止位：当 SCL 为高电平时，SDA 由低电平变为高电平的期间，如图 10.2 中最右边的 STOP condition 所标记的区域，这表示主机控制器结束了对从机的访问。

![image-20250426210920992](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250426210920992.png)

#### I2C 总线的随机读时序及解释

1. **起始信号与写命令发送**：主机先产生并发送起始信号给从机，同时将写控制命令发给从机，此时读写控制位设为低电平，表明要对从机进行写操作。写控制命令按高位在前、低位在后的顺序发送。比如在对一个 I2C 设备进行操作时，这一步就像敲门并告知设备接下来要写入内容。
2. **从机应答判断**：从机收到读控制指令后，若回传非应答信号，会输出 I2C 通信错误信号；若回传应答信号，主机收到应答信号后，开始字地址写入。这里的应答信号就像设备回应 “我准备好了” 。
3. 字地址写入：
   - **单字节地址**：按高位在前、低位在后顺序写入单字节存储地址。
   - **双字节地址**：先向从机写入高 8 位地址（高位在前、低位在后），收到从机应答信号后，再写入低 8 位地址（高位在前、低位在后） 。例如，访问有双字节地址的存储芯片时，需分两步完成地址写入。
4. **再次起始信号**：字地址写入完成且主机收到从机应答信号后，主机再次向从机发送一个起始信号，相当于重新建立通信准备读取数据。
5. **读命令发送**：主机向从机发送读控制命令，此时读写控制位设为高电平，表示要对从机进行读操作。
6. **数据接收**：主机收到从机回传的应答信号后，开始接收从机传回的单字节数据。
7. **无应答信号发送**：数据接收完成后，主机产生一个时钟的高电平无应答信号，告诉从机数据接收完毕。
8. **停止信号发送**：主机向从机发送停止信号，单字节读操作完成。

#### I2C 总线的随机写时序及解释

1. **起始信号与写命令发送**：主机产生并发送起始信号到从机，同时将写控制命令发送给从机，读写控制位设为低电平，写控制命令高位在前、低位在后发送。
2. **从机应答判断**：从机接收到写控制指令后，回传应答信号（若回传非应答信号，则表示通信有问题），主机收到应答信号后，准备写入数据。
3. **数据写入**：主机按照约定顺序，将数据逐位或逐字节写入从机。每写入一个数据，等待从机应答。例如向 I2C 的 EEPROM 设备写入数据时，按其存储单元顺序写入。
4. **停止信号发送**：数据全部写入完成，且收到从机对应答后，主机发送停止信号，结束此次随机写操作。

![image-20250426212546386](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250426212546386.png)

- I2C 主机驱动：I2C 主机控制器的驱动，一般由 SoC 芯片厂商负责设计实现，用于控制 I2C 主机控制器发出时序信号。
- I2C Core：为上层提供统一的 API 接口和对其他模块进行注册和注销等管理等。
- I2C 设备驱动：调用 I2C Core 提供的统一 API，根据 I2C 设备的访问规范，控制 I2C 主机控制器发出不同的时序信号，对 I2C 设备进行访问。该驱动称为内核层 I2C 设备驱动。
- i2c - dev：将 I2C 主机控制器实现为一个字符设备，应用程序可以直接访问 /dev/i2c - N 来访问 I2C 主机控制器，从而对 I2C 设备发起访问，该应用程序称为应用层 I2C 设备驱动。

I2C Core 为屏蔽不同的 I2C 主机控制器驱动提供了可能，可以使 I2C 设备驱动仅关心如何操作 I2C 设备，而不需要了解 I2C 主机控制器的细节，从而使 I2C 设备驱动可以独立的存在，适用于各种不同的硬件平台。

现在的Client是由设备树和内核实现的，会将这个节点添加到内核中，所以现在需要实现的是驱动也就是和这个Client匹配的driver

### SPI

和I2C一样，内核有具体的实现，驱动编写只需要使用对应的接口，同时将SPI的主机设备也抽象为一个字符设备

![image-20250427164744983](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250427164744983.png)

###  USB

这个usb的管理结构和网络内部的管理类似，USB主机就是Host，其会和一个根hub（集线器进行连接），剩下的设备都是和这个集线器进行连接的，其的拓扑图如下：

![image-20250427184535205](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250427184535205.png)

一个USB物理设备由一个或多个USB逻辑设备组成，一个USB逻辑设备体现为一个”接口“，接口由一组“端点”组成，每个USB物理设备需要用地址进行区分，一个逻辑设备的端点也有地址进行区分。所以主机通过设备地址和端点地址来寻找USB设备上的具体端点，物理0地址和端点0都是特殊的

传输分类：

1. 控制传输：突发的、非周期性的、用于请求 / 响应的通信。主要用于命令和状态的操作，如前面提到的枚举过程中的数据传输。只有端点 0 默认用于控制传输，所以端点 0 也叫作控制端点（通常用于什么传输的端点就叫什么端点，如用于控制传输的端点就叫控制端点）。USB 协议定义了很多标准的命令（请求）以及这些命令的响应，这些数据就是通过控制传输来完成的。另外，USB 协议允许厂商自定义命令，也用控制传输来完成。
2. 等时传输：有的也叫作同步传输，用于主机和设备之间周期性、连续的通信，通常用于对时间性要求高，但不太关心数据正确性的场合，比如音频数据的传输，如果传输速率不能满足要求，声音会出现停顿，但少量的数据错误，并不会太影响声音所提供的信息。
3. 中断传输：周期性的、确保延迟不超过一个规定值的传输。这里的中断并不是我们之前所说的中断，其更像是轮询。比如对于 100ms 周期的中断传输，主机会保证在 100ms 内发起一次传输。键盘、鼠标等设备通常使用这种传输模式，主机会定期获取设备的按键信息。
4. 块传输：也叫批量传输，非周期性的大数据传输。主要用于大量数据的传输，且对传输的延时不特别限制的情况，比如磁盘设备等。



## 块设备

VFS（Virtual File System，虚拟文件系统）：为应用程序提供统一的文件访问接口，屏蔽了各个具体文件系统的操作细节，是对所有文件系统的一个抽象。

Disk Caches：硬盘高速缓存，用于缓存最近访问的文件数据，如果能在高速缓存中找到，就不必去访问硬盘，毕竟硬盘的访问速度要慢很多。

Disk Filesystem：文件系统，属于映射层（Mapping Layer）。在应用程序开发者的眼中，一个文件是线性存储的，但实际上它们很有可能是分散存放在硬盘的不同扇区上的。文件系统最主要的作用就是要把对文件从某个位置开始的若干个字节访问转换为对磁盘上某些扇区的访问。它是文件在应用层的逻辑视图到磁盘上的物理视图的一个映射。

Generic Block Layer：通用块层，用于启动具体的块 I/O 操作的层。它是对具体硬盘设备的抽象，使得内核的上层不用关心磁盘硬件上的细节信息。

I/O Scheduler Layer：I/O 调度层，负责将通用块层的块 I/O 操作进行调度、排序和合并操作，使对硬盘的访问更高效。这在后面还会进一步进行说明。

Block Device Driver：块设备驱动，也就是块设备驱动开发者写的驱动程序

![image-20250428153715012](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428153715012.png)

但是实际内核的组件已经实现了很多的功能，所以需要注意的是gendisk这个结构体，这个结构体在sd_probe中申请并注册的，这个结构是一个硬盘对上层的一个抽象显示

### bio

对块设备的读写操作基本是由`submit_bio`函数提交的申请，描述这个任务的详细信息是bio结构体描述的

![image-20250428154327189](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428154327189.png)

这个是主要的一个描述形式，`bi_io_vec`数组所维护的`bio_vec`是用来描述具体信息的，page是物理内存页的管理对象指针，len是字节数，offset是偏移量

```C
// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
enum rw_hint bio_data_dir(struct bio *bio);

// bvl：struct bio_vec 结构体指针，用于接收每次迭代得到的 bio_vec。
// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
// iter：struct bio_vec_iter 类型的迭代器，用于遍历 bio 中的 bio_vec。
#define bio_for_each_segment(bvl, bio, iter) \
    for (bio_iter_init(iter, bio); \
         (bvl = bio_iter_iovec(&iter)) != NULL; \
         bio_iter_advance(&iter))

// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
// iter：struct bio_vec_iter 类型的迭代器，指定要映射的 bio_vec。
void *__bio_kmap_atomic(struct bio *bio, struct bio_vec_iter iter);

// addr：之前通过 __bio_kmap_atomic 映射得到的虚拟地址。
void __bio_kunmap_atomic(void *addr);

// bio：指向 struct bio 结构体的指针，代表一个块 I/O 请求。
// error：错误码，0 表示 bio 正常结束，其他为错误码。
void bio_endio(struct bio *bio, int error);
```

bio_data_dir：获取本次块 I/O 操作的读写方向，在同一个 bio 中的 bio_vec 的读写都是一致的，要么全是读，要么全是写。
bio_for_each_segment：用迭代器 iter 遍历 bio 中的每一个 bio_vec，得到的 bio_vec 是 bvl。
bio_kmap_atomic：将迭代器 iter 对应的 bio_vec 中的物理页面映射，支持高端内存，返回的是映射后再加上 bv_offset 偏移的虚拟地址。bio_kunmap_atomic：解除前面的映射。
bio_endio：结束一个 bio，error 为 0 表示 bio 正常结束，否则是其他错误码，比如 - EIO。

### request

 bio是可以合并并排序的，将多个bio合并排序后整合到一个request中

```C
struct request {
	struct list_head queuelist;
	struct request_queue *q;
	struct bio *bio;
	struct bio *biotail;
    ...
};
```

* queuelist：链表成员，用于将多个请求组织成一个链表。
* q：当前请求属于的请求队列。
* bio：请求中包含的第一个 bio 对象。
* biotail：请求中包含的最后一个 bio 对象。

### request_queue

请求最后放入到请求队列中

```C
struct request_queue {
	struct list_head queue_head;
	request_fn_proc *request_fn;
	make_request_fn *make_request_fn;
    ...
};
```

* queue_head：请求队列的链表头。
* request_fn：指向由块设备驱动开发者提供的请求处理函数。采用这种方式内核会把块 I/O 请求（bio）首先经过排序、合并等手段来形成请求（struct request），然后再把请求放入到请求队列中，最后调用块设备驱动开发者提供的由该函数指针指向的请求处理函数来处理这个队列中的请求。
* make_request_fn：指向用于构造请求的函数（将 bio 排序、合并的函数），该函数可以由内核提供默认的请求构造函数，那么驱动开发者就应该提供请求处理函数来处理请求，这通常用于磁盘这类设备。还可以由驱动开发者来实现一个用于构造请求的函数，然后该指针指向这个函数，这通常适用对 bio 的排序和合并操作有特殊要求的设备，或者根本不需要将 bio 排序和合并的设备（驱动开发者也就不需要提供请求处理函数了），如固态硬盘、U 盘和 SD 卡等。



## 网络设备驱动

### 网络层次结构



![image-20250428170218904](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428170218904.png)

网络设备就是将设备的数据读上来，或将数据给设备给发送出去，没有设备节点，使用的是套接字编程接口。

### sk_buff

在数据传输的过程中`sk_buff`结构很重要（套接字缓冲区），这个用于各层之间传递数据包，各层是共享这个这一个缓冲区的，层和层之间传递的是缓冲区的指针。

![image-20250428183414474](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428183414474.png)

```c
struct sk_buff *alloc_skb(unsigned int size, gfp_t priority);
struct sk_buff *dev_alloc_skb(unsigned int length);
void kfree_skb(struct sk_buff *skb);
dev_kfree_skb(a);
```

* alloc_skb、dev_alloc_skb：用于分配并初始化 skb，size 或 length 是缓冲区的大小，priority 是内存分配掩码。dev_alloc_skb 用于不能休眠的上下文中。
* kfree_skb、dev_kfree_skb：释放 skb，kfree_skb 和 alloc_skb 配对使用，dev_kfree_skb 和 dev_alloc_skb 配对使用。

新分配的sk_buff如下：

![image-20250428184027796](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428184027796.png)

```C
static inline void skb_reserve(struct sk_buff *skb, int len)
{
	skb->data += len;
	skb->tail += len;
}
```

这个函数是将data和tail同时向end方向偏移len字节，也就是用在刚分配好的skb后为了预留足够的协议头空间或对齐操作

![](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428184134273.png)

```C
unsigned char *skb_put(struct sk_buff *skb, unsigned int len)
{
	……
	skb->tail += len;
	skb->len += len;
	……
}
```

将 tail 向 end 方向偏移 len 个字节，在 put 操作之前，tail 处于实线箭头的位置；在 put 操作之后，tail 在虚线箭头的位置。函数返回 put 操作之前的 tail 指针，通常用于添加尾部数据。

![image-20250428184418211](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428184418211.png)

```C
unsigned char *skb_push(struct sk_buff *skb, unsigned int len)
{
	skb->data -= len;
	skb->len += len;
	……
}
```

将 data 向 head 方向偏移 len 个字节，在 push 操作之前，data 处于实线箭头的位置；在 push 操作之后，data 在虚线箭头的位置。函数返回 push 操作之后的 data 指针，通常用于添加协议头数据。

![image-20250428184519005](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428184519005.png)

```C
unsigned char *skb_pull(struct sk_buff *skb, unsigned int len)
{
	……
	skb->len -= len;
	……
	return skb->data += len;
}
```

将 data 向 end 方向偏移 len 个字节，在 pull 操作之前，data 处于实线箭头的位置；在 push 操作之后，data 在虚线箭头的位置。函数返回 pull 操作之后的 data 指针，通常用于去掉协议头数据。

![image-20250428184610235](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250428184610235.png)



# Linux内核设计与实现

![image-20250429225453730](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250429225453730.png)

操作系统是指在整个系统中负责完成最基本功能和系统管理的那些部分。这些部分应该包括内核、设备驱动程序、启动引导程序、命令行 Shell 或者其他种类的用户界面、基本的文件管理工具和系统工具。

处理器的活动：

- 运行于用户空间，执行用户进程。
- 运行于内核空间，处于进程上下文，代表某个特定的进程执行。
- 运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断。

| 目录          | 描述                                 |
| ------------- | ------------------------------------ |
| arch          | 特定体系结构的源码                   |
| block         | 块设备 I/O 层                        |
| crypto        | 加密 API                             |
| Documentation | 内核源码文档                         |
| drivers       | 设备驱动程序                         |
| firmware      | 使用某些驱动程序而需要的设备固件     |
| fs            | VFS 和各种文件系统                   |
| include       | 内核头文件                           |
| init          | 内核引导和初始化                     |
| ipc           | 进程间通信代码                       |
| kernel        | 像调度程序这样的核心子系统           |
| lib           | 通用内核函数                         |
| mm            | 内存管理子系统和 VM                  |
| net           | 网络子系统                           |
| samples       | 示例，示范代码                       |
| scripts       | 编译内核所用的脚本                   |
| security      | Linux 安全模块                       |
| sound         | 语音子系统                           |
| usr           | 早期用户空间代码（所谓的 initramfs） |
| tools         | 在 Linux 开发中有用的工具            |
| virt          | 虚拟化基础结构                       |



## 进程管理

内核使用进程描述符来描述一个具体的进程任务，也就是  [task_struct](https://elixir.bootlin.com/linux/v6.14.5/C/ident/task_struct)  ，然后在内核进程栈中维护了一个 [thread_info](https://elixir.bootlin.com/linux/v6.14.5/C/ident/thread_info) 结构指向这个task结构

```C
struct thread_info {
	struct pcb_struct	pcb;		/* palcode state */

	struct task_struct	*task;		/* main task structure */
	unsigned int		flags;		/* low level flags */
	unsigned int		ieee_state;	/* see fpu.h */

	unsigned		cpu;		/* current CPU */
	int			preempt_count; /* 0 => preemptable, <0 => BUG */
	unsigned int		status;		/* thread-synchronous flags */

	int bpt_nsaved;
	unsigned long bpt_addr[2];		/* breakpoint handling  */
	unsigned int bpt_insn[2];
	unsigned long fp[32];
};
```

然后每个CPU存在[current_thread_info](https://elixir.bootlin.com/linux/v6.14.5/C/ident/current_thread_info) 这个结构指向这个thread_info结构用来获取当前正在执行的任务信息

![image-20250505164708175](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250505164708175.png)

进程描述符的存放是根据系统的硬件结构来决定的，有的硬件结构直接存在一个寄存器用来放这个task_struct的结构的指针，有的是需要根据这个内核栈的偏移来获取的数据结果。

| 比较项     | `fork`                                                       | `vfork`                                                      |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 功能目的   | 创建新进程，子进程是父进程的副本                             | 创建新进程，与父进程共享地址空间                             |
| 内存使用   | 子进程复制父进程的地址空间，包括数据段、堆栈段和代码段       | 子进程和父进程共享地址空间，直到子进程调用 `exec` 系列函数或 `_exit` 函数 |
| 执行顺序   | 父进程和子进程并发执行，执行顺序由操作系统调度器决定         | 子进程先执行，父进程被阻塞，直到子进程调用 `exec` 系列函数或 `_exit` 函数 |
| 资源消耗   | 由于复制地址空间，资源消耗相对较大                           | 共享地址空间，资源消耗小                                     |
| 返回值     | 父进程中返回子进程的进程 ID（正整数），子进程中返回 0，出错返回 -1 | 父进程中返回子进程的进程 ID（正整数），子进程中返回 0，出错返回 -1 |
| 数据独立性 | 子进程有自己独立的数据副本，对数据的修改不会影响父进程       | 子进程对数据的修改会影响父进程，因为共享地址空间             |
| 用途       | 适用于需要子进程独立运行且有自己的内存空间的场景             | 适用于子进程立即调用 `exec` 系列函数加载新程序的场景，以节省资源 |

当子进程调用 `exec` 系列函数时，会按以下步骤进行内存操作：

- **替换当前进程映像**：`exec` 函数的主要目的是用新的程序替换当前进程（即子进程）的映像。这里的 “替换” 不是简单地在原有共享内存上覆盖数据，而是一系列复杂的操作。
- **重新分配内存**：操作系统会为新程序重新分配内存空间。这是因为新程序有其自身的代码结构、数据布局和运行要求，原有的共享内存空间可能无法满足这些需求。新分配的内存区域会根据新程序的大小和内存布局进行规划，以确保新程序能够正常运行。
- **加载新程序**：在新分配的内存区域中，操作系统会将新程序的代码、数据等加载进去。这个过程会把新程序的二进制代码加载到代码段，初始化全局变量和静态变量到数据段，设置好堆栈等运行环境。
- **断开与父进程的共享关系**：一旦新程序加载完成，子进程就拥有了一个全新的、独立于父进程的内存映像。此时，子进程和父进程不再共享同一片地址空间，子进程对新内存区域的任何操作都不会影响到父进程，反之亦然。

### 在linux中创建线程

### 用户空间

在Linux中模糊了线程的概念，在其他的系统中可以是一个进程管理多个线程，但是在Linux中，将就进程中的线程看做进程，其实也就是多个任务共享资源

在Linux中创建线程使用的是`clone()`系统调用，根据参数（系统调用的传递的参数）可以创建不同的线程

| 参数标志             | 含义                                                |
| -------------------- | --------------------------------------------------- |
| CLONE_FILES          | 父子进程共享打开的文件                              |
| CLONE_FS             | 父子进程共享文件系统信息                            |
| CLONE_IDLETASK       | 将 PID 设置为 0（只供 idle 进程使用）               |
| CLONE_NEWNS          | 为子进程创建新的命名空间                            |
| CLONE_PARENT         | 指定子进程与父进程拥有同一个父进程                  |
| CLONE_PTRACE         | 继续调试子进程                                      |
| CLONE_SETTID         | 将 TID 回写至用户空间                               |
| CLONE_SETTLS         | 为子进程创建新的 TLS                                |
| CLONE_SIGHAND        | 父子进程共享信号处理函数及被阻断的信号              |
| CLONE_SYSVSEM        | 父子进程共享 System V SEM_UNDO 语义                 |
| CLONE_THREAD         | 父子进程放入相同的线程组                            |
| CLONE_VFORK          | 调用 vfork ()，所以父进程准备睡眠等待子进程将其唤醒 |
| CLONE_UNTRACED       | 防止跟踪进程在子进程上强制执行 CLONE_PTRACE         |
| CLONE_STOP           | 以 TASK_STOPPED 状态开始进程                        |
| CLONE_SETTLS         | 为子进程创建新的 TLS (thread-local storage)         |
| CLONE_CHILD_CLEARTID | 清除子进程的 TID                                    |
| CLONE_CHILD_SETTID   | 设置子进程的 TID                                    |
| CLONE_PARENT_SETTID  | 设置父进程的 TID                                    |
| CLONE_VM             | 父子进程共享地址空间                                |

### 内核空间

在内核中创建线程是通过 [kthread_create](https://elixir.bootlin.com/linux/v6.14.5/C/ident/kthread_create) 这个宏来实现的，内核线程只能由其他的内核线程创建

```C
/**
 * kthread_create - create a kthread on the current node
 * @threadfn: the function to run in the thread
 * @data: data pointer for @threadfn()
 * @namefmt: printf-style format string for the thread name
 * @arg: arguments for @namefmt.
 *
 * This macro will create a kthread on the current node, leaving it in
 * the stopped state.  This is just a helper for kthread_create_on_node();
 * see the documentation there for more details.
 */
#define kthread_create(threadfn, data, namefmt, arg...) \
	kthread_create_on_node(threadfn, data, NUMA_NO_NODE, namefmt, ##arg)
```

`kthread_run`宏是创建一个内核线程（进程 任务）并通过`wake_up_process`实现这个任务并执行，单纯创建后是不可以运行的需要使用这个宏进行唤醒

```C
/**
 * kthread_run - create and wake a thread.
 * @threadfn: the function to run until signal_pending(current).
 * @data: data ptr for @threadfn.
 * @namefmt: printf-style name for the thread.
 *
 * Description: Convenient wrapper for kthread_create() followed by
 * wake_up_process().  Returns the kthread or ERR_PTR(-ENOMEM).
 */
#define kthread_run(threadfn, data, namefmt, ...)			   \
({									   \
	struct task_struct *__k						   \
		= kthread_create(threadfn, data, namefmt, ## __VA_ARGS__); \
	if (!IS_ERR(__k))						   \
		wake_up_process(__k);					   \
	__k;								   \
})
```

内核线程结束是改线程执行`do_exit`退出，或者其他的内核线程调用这个`kthread_stop`

```c
int kthread_stop(struct task_struct* k);
```



## 进程调度

### 时间片相关概念

- **时间片定义**：时间片是指进程在被抢占前能持续运行的时间。它的值很关键，太长会使系统对交互响应变差，感觉系统无法并发执行应用程序；太短则会因进程频繁切换增加处理器开销，毕竟进程切换需要系统时间，而实际运行时间却很短。并且不同类型进程对时间片需求不同，I/O 消耗型进程不需要长的时间片，处理器消耗型进程希望时间片越长越好，比如长的时间片能提高其高速缓存命中率。
- **默认时间片设置**：很多操作系统为避免交互表现欠佳，默认时间片较短，如 10ms 。但 Linux 的 CFS（完全公平调度器）调度器不直接分配时间片给进程，而是给进程分配处理器使用比例，这个比例受进程 `nice` 值影响 。`nice` 值作为权重调整进程对处理器的使用比例，`nice` 值越高（优先级越低 ），权重越低，获得处理器使用比例小；`nice` 值越低（优先级越高 ），权重越高，获得处理器使用比例大。
- **Linux 的抢占机制**：Linux 是抢占式系统，进程进入可运行态就可投入运行。在多数操作系统中，是否抢占当前进程取决于进程优先级和时间片是否用完。而在 Linux 的 CFS 调度器中，抢占时机取决于新的可运行程序消耗的处理器使用比例。若新进程使用比例比当前进程小，就立刻抢占当前进程投入运行，否则推迟运行。

### 文字编辑程序和视频编码程序例子解读

- **进程类型特点**：文字编辑程序是 I/O 消耗型进程，大部分时间在等待用户键盘输入，用户期望按下键系统能马上响应；视频编码程序是处理器消耗型进程，除了开始读取原始数据流和最后输出处理好的视频外，大部分时间都在进行视频编码，对开始运行时间要求不严格，用户不太关心它是立刻运行还是半秒钟后运行，更关注它完成任务的早晚。
- **传统调度目标**：在多数操作系统中，为满足文字编辑程序的交互性，目标一是给它更多处理器时间，不是因为它实际需要，而是希望它需要时能随时得到处理器；目标二是让它被唤醒（用户打字时 ）能抢占视频编码程序，确保交互性能。通常靠给文字编辑程序分配比视频编码程序更高的优先级和时间片来实现。
- **Linux CFS 调度做法**：Linux 不通过分配固定优先级和时间片来实现上述目标。假设文字编辑程序和视频编码程序只有这两个运行进程且 `nice` 值相同，按 CFS 调度，它们理论上处理器使用比例各 50% 。但文字编辑程序因常等待用户输入，实际使用处理器时间少；视频编码程序会更多占用处理器时间。当文字编辑程序被唤醒（用户输入 ），CFS 发现它之前运行时间短，为保证所有进程公平分享处理器，会立刻抢占视频编码程序，让文字编辑程序运行。文字编辑程序处理完用户输入又进入睡眠等待下一次输入，因没消耗完 50% 的处理器使用比例，下次需要时 CFS 仍会优先让它运行，视频编码程序只能在剩余时间运行。 这样既保证了文字编辑程序的交互性，又在整体上兼顾了公平性 。

### 调度器的模块化设计目的

Linux 调度器采用模块化设计，以模块方式提供不同的调度算法。这么做是因为不同类型的进程对 CPU 资源分配的需求不同，例如实时进程要求低延迟、高优先级的资源分配，普通交互进程则更注重响应速度和公平性。通过模块化，能让不同类型进程针对性地选择合适的调度算法，满足多样化的调度需求。

### 调度器类（scheduler classes）

- **概念**：调度器类是一种将不同调度算法进行分类管理的模块化结构。它允许多种不同的调度算法同时存在于系统中，并且这些算法可以动态添加。每个调度器类负责调度属于自己范畴的进程。
- **优先级**：每个调度器类都有一个优先级。优先级决定了调度器类在竞争调度权时的先后顺序。基础的调度器代码定义在 `kernel/sched.c` 文件中，系统会按照优先级顺序遍历调度器类。
- **调度决策**：在调度时，系统会检查每个调度器类中是否有可执行的进程。拥有可执行进程且优先级最高的调度器类会胜出，由它来选择下一个要执行的进程。例如，实时调度器类的优先级通常高于普通进程调度器类，如果实时调度器类中有可执行的实时进程，那么它就会优先获得调度权，去选择实时进程执行，而普通进程调度器类则需等待。 这种机制确保了高优先级的进程能够优先获得 CPU 资源，同时也兼顾了不同类型进程的调度需求。

### CFS

CFS（完全公平调度器）的设计基于一个理想的多任务处理器模型理念，即系统能让每个进程都公平地获得处理器资源。在理想情况下，若有 `n` 个可运行进程，每个进程应获得 `1/n` 的处理器时间 。例如有两个进程，理想状态下它们应在相同时间内各使用一半的处理器能力，而不是像标准 Unix 调度模型那样，一个运行时独占处理器。但实际中，在一个处理器上无法真正同时运行多个进程，且进程频繁切换会带来开销，影响缓存效率 ，所以 CFS 要在追求公平的同时兼顾系统性能。

CFS 总是尝试选择 vruntime 最小的进程运行，即选择之前占用 CPU 时间短、受 “不公平” 对待的进程，以此保证公平性，同时兼顾高优先级进程（通过权重机制获得更多运行时间 ） 。 当进程从一个 CPU 的运行队列转移到另一个时，其 vruntime 会根据队列的 `min_vruntime` 值进行调整（出队列减去，入队列加上 ），维持相对公平 。 此外，CFS 还设定了进程占用 CPU 的最小时间值 `sched_min_granularity_ns` ，正在运行的进程若不足这个时间一般不调离 CPU ，避免过于短暂的进程切换消耗 。

#### 核心概念

- **虚拟运行时间（vruntime）** ：CFS 通过 vruntime 实现公平性。计算公式为 `vruntime = 实际运行时间 × 1024 / 进程权重` 。实际运行时间即进程实际占用 CPU 的时长；进程权重由进程的 nice 值决定，nice 值越小（优先级越高），权重越大，比如 nice = 0 时对应权重 1024 。通过该公式，将实际运行时间按权重标准化，便于比较不同进程对 CPU 资源的消耗情况 。例如，进程 A 权重为 1，运行 10ms，其 vruntime = 10×1024 / 1 = 10240；进程 B 权重为 2，运行 5ms，其 vruntime = 5×1024 / 2 = 2560 ，说明进程 B 消耗资源相对少。
- **调度周期与最小调度粒度** ：调度周期指将所有处于可运行状态（TASK_RUNNING）的进程都调度一遍的时间，默认约 20ms，保证所有进程至少运行一次。最小调度粒度默认 4ms ，防止进程频繁切换。当进程数量过多，导致按调度周期平分的时间片小于最小调度粒度时，调度周期会调整为 `最小粒度 × 进程数` ，确保每个进程至少运行最小粒度时间 。

#### 特殊处理机制

- **新进程处理** ：新进程初始 vruntime 设为当前运行队列的 `min_vruntime` ，防止新进程饥饿，使其与老进程在 vruntime 上保持合理差距，能较快获得运行机会 。
- **唤醒进程处理** ：休眠进程唤醒时会得到 vruntime 补偿，增加其 vruntime 值，使其更易抢占 CPU 。这样可提升交互式任务响应速度，因为交互式进程常因等待用户输入而休眠，唤醒后能尽快运行 。

#### 工作原理

1. **摒弃传统时间片分配**：CFS 不再像传统调度那样给每个进程分配固定时间片，而是基于可运行进程总数来计算每个进程应运行的时长。它会循环轮转，选择运行时间最少的进程作为下一个运行进程 。
2. **基于权重分配时间**：进程的 `nice` 值在 CFS 中作为权重，决定进程获得的处理器运行比例。`nice` 值越高（优先级越低 ），进程获得的处理器使用权重越低；`nice` 值越低（优先级越高 ），权重越高 。例如，`nice` 值为 5 的进程权重是默认 `nice` 值（0）进程的 `1/3` 。
3. **目标延迟与时间片计算**：CFS 设置了 “目标延迟”，它是对理想中无限小调度周期的近似值。目标延迟越小，交互性越好，但切换代价越高、系统总吞吐量越差 。假设目标延迟为 20ms，若有两个优先级相同的可运行任务，每个任务在被抢占前运行 10ms；若有 4 个，每个运行 5ms；若有 20 个，每个运行 1ms 。
4. **最小粒度限制**：当可运行任务数量趋于无穷时，为避免过高的切换消耗，CFS 引入最小粒度（默认 1ms ）。即使进程数量极多，每个进程最少也能获得 1ms 的运行时间 。
5. **公平性与相对值影响**：CFS 确保每个进程能公平获得处理器使用比例。进程获得的处理器时间由它与其他可运行进程 `nice` 值的相对差值决定，`nice` 值对时间片的作用是几何加权而非算数加权 。例如，`nice` 值为 10 和 15 的两个进程，分配的时间片可能和 `nice` 值为 0 和 5 的进程分配情况类似（假设目标延迟相同 ），因为起决定作用的是相对值 。

### 上下文切换

当要从一个正在执行的进程，换成另一个进程去执行的时候，就涉及到上下文切换。这一操作由定义在 `kernel/sched.c` 文件中的 `context_switch()` 函数来负责。每次 `schedule()` 函数选好一个新的进程准备让它运行时，就会调用这个 `context_switch()` 函数。它主要做两件事：

- 第一件事是调用 `switch_mm()` 函数（在 `<asm/mmu_context.h>` 中声明 ），这个函数的工作是把虚拟内存的映射，从上一个进程切换到新进程上。简单理解，就是让新进程能正确访问自己的内存空间。
- 第二件事是调用 `switch_to` 函数（在 `<asm/system.h>` 中声明 ），它负责把处理器的状态，从上一个进程切换到新进程。比如保存上一个进程的运行状态信息，像栈信息、寄存器信息等，再把新进程的这些状态信息恢复好，这样新进程就能接着之前的状态继续运行啦。

#### need_resched 标志

- **标志作用**：内核得知道啥时候该进行进程调度，如果光靠用户程序自己主动去调用调度函数，那可不行，可能会导致有些进程一直霸占着 CPU 不撒手。所以内核弄了个 `need_resched` 标志，用来告诉内核是不是得重新进行一次进程调度啦。要是某个进程该被别的进程挤下去（被抢占 ），或者有个优先级更高的进程准备好可以运行了，这个标志就会被设置。
- **相关操作函数**：有专门的函数来设置、清除和检查这个标志。比如 `set_task_need_resched()` 是设置标志，`clear_task_need_resched()` 是清除标志，`need_resched()` 是检查标志有没有被设置。
- **存储位置变化**：以前这个标志是全局变量，后来在不同内核版本里，它被挪到了进程描述符相关的结构体里，比如从 `task_struct` 又挪到了 `thread_info` 结构体里。为啥这么挪呢？因为从进程描述符里访问这个标志更快，这样能让内核更高效地判断是不是要调度。
- **检查时机**：当内核要从自己的地盘回到用户空间，或者从中断处理完要返回的时候，都会瞅一眼这个 `need_resched` 标志。要是发现标志被设置了，那就赶紧调用调度程序，重新选个合适的进程来运行。

#### 用户抢占

当内核快要回到用户空间的时候，会看一眼 `need_resched` 标志。要是标志被设置了，那就会调用 `schedule()` 函数，这时候就发生了用户抢占。简单说，就是本来在用户空间运行的进程，可能因为这个标志被设置，就被内核换成别的进程去运行了。这种用户抢占一般发生在从系统调用返回用户空间，或者从中断处理程序返回用户空间的时候。

#### 内核抢占

- **和其他系统的不同**：Linux 这个系统很厉害，它完整支持内核抢占，这和其他好多操作系统不太一样。在那些不支持内核抢占的系统里，内核代码一旦开始执行，就得等它全部执行完，别的进程才能有机会。但 Linux 不是，只要条件允许，随时能把正在执行的内核任务打断，去运行别的更合适的任务。
- **能抢占的条件**：啥时候能进行内核抢占呢？首先得没有锁，用 `preempt_count` 这个计数器来记录锁的情况，它初始是 0，每次加锁就加 1，解锁就减 1 。当 `preempt_count` 变回 0 而且 `need_resched` 标志被设置的时候，就说明可以安全地进行内核抢占啦。每次从中断返回内核空间时，内核都会检查这俩条件。
- **内核抢占发生场景**：一般在几种情况下会发生。比如内核代码变得又可以被抢占了；或者内核里的任务自己主动调用 `schedule()` 函数；还有就是内核里的任务被堵住（阻塞 ）了，这时候也会调用 `schedule()` 函数，然后可能就发生内核抢占。

### 调度策略类型

- **实时调度策略**：Linux 提供 `SCHED_FIFO` 和 `SCHED_RR` 两种实时调度策略 。
- **非实时调度策略**：`SCHED_NORMAL` 为普通的非实时调度策略。实时策略不由常规公平调度器管理，而是由特定实时调度器管理，其实现定义在 `kernel/sched_rt.c` 文件中。

#### SCHED_FIFO 调度策略

- **算法原理**：实现简单的先入先出调度算法，不使用时间片。可运行状态下，`SCHED_FIFO` 级进程优先于 `SCHED_NORMAL` 级进程被调度。一旦 `SCHED_FIFO` 进程开始执行，除非它主动阻塞（如等待资源 ）或释放处理器，否则会一直执行下去。
- **抢占规则**：只有更高优先级的 `SCHED_FIFO` 或 `SCHED_RR` 任务能抢占它。同优先级的 `SCHED_FIFO` 进程轮流执行，且仅在它们主动让出处理器时才会退出执行，其他较低级别进程需等待其执行完毕才有机会运行。

#### SCHED_RR 调度策略

- **算法原理**：类似于 `SCHED_FIFO`，但带有时间片，属于实时轮流调度算法。进程耗尽分配的时间片后，同一优先级的其他实时进程会被轮流调度。时间片仅用于重新调度同一优先级的进程。
- **抢占规则**：高优先级 `SCHED_RR` 能立即抢占低优先级进程，但低优先级进程不能抢占 `SCHED_RR` 任务，即便其时间片耗尽 。

#### 两种策略的共性

- 都是静态优先级算法，内核不为实时进程计算动态优先级，保证给定优先级的实时进程总能抢占优先级更低的进程。
- 提供软实时工作方式，内核尽力让进程在限定时间前运行，但不保证一定满足，区别于硬实时系统（能保证满足时间要求 ）。不过 Linux 对实时任务的调度性能较好，可满足较严格时间要求。

#### 优先级范围

- 实时优先级范围从 0 到 `MAX_RT_PRIO` - 1，默认 `MAX_RT_PRIO` 为 100，即默认实时优先级范围是 0 到 99 。
- `SCHED_NORMAL` 级进程的 `nice` 值与实时优先级共享取值空间，`nice` 值 -20 到 +19 对应 100 到 139 的实时优先级范围 。

### CFS和其他调度策略

#### CFS 与实时调度策略共存

- **实时调度策略**：Linux 提供 `SCHED_FIFO`（先进先出 ）和 `SCHED_RR`（时间片轮转 ）等实时调度策略，用于对时间敏感、要求低延迟的实时任务。比如工业控制系统中的数据采集与控制任务，多媒体播放中的音频、视频解码任务等 。
- **CFS**：主要负责普通进程（非实时进程 ）的调度，旨在为普通进程公平分配 CPU 时间。像日常使用的办公软件、浏览器等应用程序的进程，由 CFS 进行调度。
- **共存方式**：内核根据进程的调度策略类型进行区分调度。实时进程优先级高于普通进程，当系统中同时存在实时进程和普通进程时，实时进程会优先获得 CPU 资源。例如，在播放视频时（实时任务 ）同时打开文档编辑（普通任务 ），视频播放相关进程会优先被调度，保证视频流畅播放，而文档编辑进程在实时进程空闲时才得到执行机会 。

#### CFS 与其他普通调度策略共存

- **其他普通调度策略**：除 CFS 外，还有 `SCHED_BATCH` 用于批处理任务，这类任务对响应时间不敏感，如夜间进行的系统备份、数据批量处理任务等；`SCHED_IDLE` 用于低优先级任务，在系统空闲时运行，像一些系统清理、统计相关的后台任务 。
- **共存方式**：不同普通调度策略适用于不同应用场景，CFS 作为默认调度策略处理大多数普通用户进程。例如，系统会根据进程特性为批处理任务分配 `SCHED_BATCH` 策略，使其在系统资源相对空闲时运行，不与交互式任务（多由 CFS 调度 ）争夺资源；而 `SCHED_IDLE` 策略的任务仅在 CPU 无其他工作时才执行 。 此外，Linux 内核通过调度类（sched_class ）的框架来管理不同调度策略，每种调度类有各自的调度策略，调度类之间有优先级顺序，由高到低为：停机调度类 > 限期调度类 > 实时调度类 > 公平调度类（CFS ）> 空闲调度类 。这种机制保证了多种调度策略在系统中有序共存、协同工作 。

#### 停机调度类

- **例子**：在 Linux 系统中，迁移线程属于停机调度类 。比如当系统进行 CPU 热插拔操作时，为了保证操作期间系统状态的一致性和稳定性，迁移线程会被调度执行。它会将相关进程迁移到合适的 CPU 上，在此过程中，迁移线程可抢占其他进程，以确保自身能优先执行完成任务，而其他进程不能抢占它 。

#### 限期调度类（SCHED_DEADLINE ）

- **例子**：在一些对时间要求极为苛刻的工业自动化控制场景中，比如汽车生产线上的机器人动作控制。机器人需要在精确的时间点完成零件抓取、安装等操作。对应这些任务的进程会采用限期调度类 。系统会按照进程的绝对截止期限进行调度，保证机器人能在规定时间内准确执行动作，避免生产事故和产品质量问题 。

#### 实时调度类

- **SCHED_FIFO 例子**：在航空航天领域的飞行控制系统中，传感器数据采集进程常采用 `SCHED_FIFO` 调度策略。这些进程需要不间断地采集各类传感器数据（如高度、速度、姿态等 ），一旦开始执行，除非主动等待资源（如等待新的数据传输 ），否则要一直执行，以保证飞行控制系统能实时获取准确数据 。
- **SCHED_RR 例子**：在多媒体播放系统中，音频和视频解码进程可能采用 `SCHED_RR` 调度策略。这些进程需要按时间片轮流执行，以保证音频和视频的同步播放。每个解码进程在自己的时间片内执行，时间片耗尽后，同一优先级的其他解码进程会被轮流调度 。

#### 公平调度类（CFS，SCHED_NORMAL ）

- **例子**：日常使用的办公软件，如文字处理软件（如 WPS 文字 ）、电子表格软件（如 Excel ）等。当在这些软件中进行文字编辑、数据计算等操作时，相关进程由 CFS 调度。CFS 会根据进程的虚拟运行时间等因素，公平地为这些进程分配 CPU 时间，使得用户在操作过程中感觉流畅，不同办公软件进程之间也能合理共享 CPU 资源 。

#### 空闲调度类（SCHED_IDLE ）

- **例子**：系统中的一些统计信息收集任务，比如统计系统运行过程中各个进程的 CPU 使用率、内存占用率等信息的进程。这些任务对实时性要求极低，只有在系统处于空闲状态，没有其他可运行进程时，才会被调度执行 。这样既不会影响正常业务进程的运行，又能在空闲时完成系统信息收集工作 。





























































# 面试题目

## 滤波

### 一阶滞后滤波

原理：通过加权融合本次采样值和历史输出值，其中 alpha 为滤波系数（0~1），决定响应速度。
代码示例：

```C
float FilterIIR (float x, float prev_y, float alpha) {
	return (1 - alpha) * x + alpha * prev_y;
}
```

优点：计算量小，适用于实时性要求高的场景。
缺点：相位滞后，可能影响动态响应。
适用场景：电机转速、电池电压等实时控制信号

### 消抖滤波

原理：设置计数器，若连续多次采样值波动超过阈值，则判定为有效变化；否则维持原值。

```C
#define N 5
int filter() {
    static int count = 0, stable_val;
    int new_val = get_ad();
    if (abs(new_val - stable_val) > THRESHOLD) {
        count++;
        if (count >= N) {
            stable_val = new_val;
            count = 0;
        }
    } else count = 0;
    return stable_val;
}
```

优点：避免临界值抖动（如按键去抖）。
缺点：响应延迟，可能遗漏快速变化信号。



## 通信协议

### I2C

两根线，要配置为开漏输出，若采用推挽输出，当两个设备一个输出高电平，另一个输出低电平时，相当于电源与地直接相连，会引发短路，损坏设备 。而开漏输出只有 NMOS 管，仅能输出低电平 ，不会出现短路情况。但是开漏输出只能输出强低电平，所以需要加上拉电阻，来实现输出高电平

再I2C协议中，可以挂载多个设备，要实现线与的功能，这个线与功能要求多个输出端直接互连实现 “AND” 逻辑 ，即只要有一个输出拉低，总线就为低电平 。推挽输出的高电平由内部电路直接连接电源产生 ，低电平由直接连接地产生 ，当多个推挽输出相连 ，电平状态会相互冲突 ，无法按线与逻辑确定最终电平 。



## DMA

DMA 使用场景

- **数据密集型 I/O 操作**：在磁盘读写、网络数据收发场景中，数据量巨大。以磁盘读取为例，若由 CPU 搬运数据，需频繁中断 CPU 执行当前任务，严重影响系统整体性能。采用 DMA 技术，磁盘控制器可直接将数据从磁盘传输到内存，无需 CPU 频繁干预，大幅提升数据传输速度和系统运行效率。
- **高速数据采集**：在音频、视频采集设备工作时，要在短时间内获取大量数据。比如高清视频采集，每秒可能产生数 MB 甚至数十 MB 数据。利用 DMA，采集设备能直接把采集到的数据存储到内存，保证数据的快速、连续存储，避免数据丢失，为后续实时处理提供保障。
- **图形处理**：图形渲染过程中，GPU 需要大量纹理数据和顶点数据。通过 DMA，GPU 可直接从系统内存获取这些数据，无需 CPU 参与数据传输，让 CPU 专注于其他任务，提升图形渲染速度，优化游戏、3D 建模等图形应用的流畅度。

DMA 使用流程

- **初始化 DMA 控制器**：CPU 先对 DMA 控制器进行配置，设置源地址（数据来源，如设备内存地址）、目的地址（数据存储位置，如系统内存地址）、传输数据量、传输模式（如单次传输、块传输）等参数，确定数据传输的基本规则。
- **发起 DMA 请求**：外部设备准备好数据传输时，向 DMA 控制器发送 DMA 请求信号，告知 DMA 控制器有数据需要传输。
- **DMA 控制器响应**：DMA 控制器接收到请求，向 CPU 发送总线请求信号，申请系统总线控制权。CPU 收到信号，在当前总线周期结束后，响应 DMA 请求，让出总线控制权。
- **数据传输**：DMA 控制器获得总线控制权后，按照初始化设定的参数，在源地址和目的地址之间直接传输数据，无需 CPU 干预，完成数据的快速搬运。
- **传输完成处理**：数据传输结束，DMA 控制器向 CPU 发送中断请求信号，通知 CPU 数据传输已完成。CPU 响应中断，执行相应中断处理程序，进行后续操作，如检查传输结果、启动下一次数据传输等。

DMA 和 cache 的数据一致性解决方法

- **硬件层面**：采用写回（Write - Back）和写通（Write - Through）策略。写回策略下，数据先写入 cache，待 cache 块被替换时，才将修改后的数据写回内存。DMA 传输时，硬件会监测 cache 和内存的数据状态，若 DMA 操作涉及的内存区域在 cache 中，先将 cache 中对应数据写回内存，保证内存数据最新，避免 DMA 读取旧数据。写通策略是数据在写入 cache 的同时也写入内存，确保内存数据始终与 cache 一致，使 DMA 操作能获取最新数据，但这种方式会增加写操作的开销。
- **软件层面**：在进行 DMA 传输前，CPU 执行指令将相关 cache 数据写回内存，并使 cache 中对应的缓存行无效。传输完成后，再重新从内存读取数据到 cache。如在 x86 架构中，可使用特定指令（如 “invlpg” 指令使页缓存无效）来保证数据一致性。也可在操作系统的设备驱动程序中进行处理，在 DMA 传输前后调用特定函数来管理 cache，确保数据一致性。



## 在驱动中，物理地址向虚拟地址的转换接口是什么

**`ioremap`函数**：主要用于将物理地址映射到内核空间的虚拟地址，通常用于对 I/O 设备的物理地址进行映射，以便内核可以直接访问这些设备的寄存器。

```C
void __iomem *ioremap(resource_size_t phys_addr, unsigned long size);

//phys_addr：代表要映射的连续物理页的起始物理地址。
//size：表示要映射的内存区域的大小。
void *vmalloc_phys(unsigned long phys_addr, unsigned long size);

//page：这是一个指向struct page结构体的指针，该结构体代表一个物理页框。
void *kmap(struct page *page);

//page：指向struct page结构体的指针，代表一个物理页框。
//type：这是一个enum km_type类型的参数，用于指定映射的类型，其取值有多种，例如KM_USER0、KM_USER1等，不同的取值对应不同的映射用途。
void *kmap_atomic(struct page *page, enum km_type type);
```

- **`vmalloc_phys`函数**：如果需要将物理地址映射到内核的虚拟地址空间，并且分配的内存是连续的物理页，可以使用`vmalloc_phys`函数。不过该函数在实际使用中相对较少。
- **`kmap`和`kmap_atomic`函数**：对于已经在物理内存中分配好的页框，`kmap`函数可以将其映射到内核的虚拟地址空间。`kmap_atomic`则用于原子性的映射操作，通常在中断处理程序等不能睡眠的上下文中使用。

使用`kmap`和`kmap_atomic`进行映射之后，在不再需要映射时，需要分别使用`kunmap`和`kunmap_atomic`函数来解除映射，从而避免内存泄漏。



## ioremap 和 iommu 的区别是什么

- **`ioremap`**：是一个软件层面的函数，用于将物理地址映射到虚拟地址空间。它主要用于内核空间对 I/O 设备的物理地址进行映射，使得内核可以像访问普通内存一样访问这些设备的寄存器。例如，在驱动程序中，通过`ioremap`可以将硬件设备的寄存器地址映射到内核的虚拟地址，方便进行读写操作。
- **`IOMMU`**：是一种硬件机制，它提供了类似于 CPU 的内存管理单元（MMU）的功能，但专门用于 I/O 设备。`IOMMU`可以将设备看到的物理地址（设备地址空间）转换为系统的物理地址，实现设备和内存之间的地址转换。同时，`IOMMU`还可以提供内存保护、DMA 重映射等功能，增强系统的安全性和可靠性。

应用场景

- **`ioremap`**：主要应用于驱动程序开发中，用于对 I/O 设备的物理地址进行映射，方便内核与硬件设备进行交互。它通常用于访问设备的寄存器、配置设备等操作。
- **`IOMMU`**：主要应用于多设备共享内存、虚拟化等场景。在虚拟化环境中，`IOMMU`可以实现设备的隔离和地址转换，使得不同的虚拟机可以共享物理设备，同时保证设备访问的安全性和正确性。在多设备系统中，`IOMMU`可以避免设备之间的地址冲突，提高系统的可扩展性。



## 模块

### 模块状态

模块状态用于描述内核模块在系统中的当前情况。常见状态包括未加载、已加载未初始化、已初始化正常运行、出错、正在卸载等。比如，处于未加载状态的模块在系统中未被使用，没有占用系统资源；已加载未初始化的模块虽已被读入内存，但关键的初始化工作还未完成，无法正常工作；而已初始化正常运行的模块则正在发挥其功能。模块状态可通过内核中的相关数据结构和函数获取，在 Linux 中，`struct module`结构体包含模块状态信息，系统工具`lsmod`也能展示已加载模块的状态 。

### 静态加载和动态加载的区别

- **加载时机**：静态加载是在系统启动时，与内核一同被加载到内存，其代码和数据永久驻留，直至系统重启；动态加载则是在系统运行过程中，根据需要随时加载或卸载，灵活性高。例如，一些设备驱动若在启动时就确定会用到，可静态加载；而像一些不常用的文件系统驱动，可在需要访问特定文件系统时动态加载 
- **资源占用**：静态加载的模块一直占用内存和系统资源，可能造成资源浪费；动态加载只有在使用时才占用资源，不用时可卸载释放，能有效节省资源 。
- **依赖关系处理**：静态加载模块的依赖关系在编译内核时就已确定，不易更改；动态加载模块在加载时需处理依赖，可能要先加载依赖模块，卸载时也要按依赖顺序操作，更复杂但更灵活 。
- **可维护性和升级性**：动态加载模块便于单独升级、调试和维护，无需重新编译内核；静态加载模块若要修改，通常需重新编译内核，过程繁琐且可能影响系统稳定性 。

### 如果是静态加载，不用 insmod，怎么判断驱动状态是不是好的

- **系统日志检查**：查看系统日志文件（如`/var/log/syslog` ），若驱动在加载或运行过程中出错，会有相关错误信息记录，如 “`Driver XXXX initialization failed`” 。
- **设备状态查看**：通过系统工具查看相关设备状态。如查看网卡驱动，用`ifconfig`命令，若网卡正常显示且有正确的 IP 配置，说明网卡驱动可能正常；查看块设备驱动，用`lsblk`命令，若设备能正常列出，表明驱动可能正常 。
- **内核调试信息**：在内核代码中添加`printk`等调试语句输出驱动运行信息，重启系统查看内核日志获取这些信息，判断驱动运行状态 。
- **应用程序测试**：运行依赖该驱动的应用程序，若能正常使用，说明驱动状态可能良好。例如，运行文件系统挂载命令测试文件系统驱动 。

### 静态加载模块时，内核是如何找到并加载模块的

1. **模块编译进内核镜像**：在构建内核时，开发者会通过配置选项决定将哪些模块静态编译到内核中。例如，在使用`make menuconfig`等工具配置内核时，会有大量的模块选项可供选择，若将某个驱动模块（如网卡驱动模块）配置为静态编译，该模块的代码就会被整合进内核源代码的编译过程。编译过程中，模块代码与内核其他部分一起被编译成目标文件（`.o`文件），最终链接到内核镜像文件（如`vmlinuz` ）中。这就相当于把模块的二进制代码嵌入到了内核的可执行文件里，成为内核的一部分。
2. 内核启动时的初始化流程：当系统启动加载内核时，内核会按照既定的初始化流程执行一系列操作。在初始化过程中，内核会遍历其内部维护的数据结构，以识别和初始化静态编译进来的模块。
   - **初始化函数调用**：每个静态编译的模块都有特定的初始化函数（通常命名为`module_init`指定的函数）。内核在启动过程中，会调用这些初始化函数来启动相应的模块。这些初始化函数负责完成模块的初始化工作，如分配内存、设置数据结构、注册设备驱动等。
   - **设备探测与注册**：模块初始化函数中通常会包含设备探测代码，用于检测硬件设备是否存在。例如，对于网卡驱动模块，初始化函数会尝试探测网卡硬件，检查其是否响应。如果探测到设备，模块会通过内核提供的注册函数（如`register_chrdev`用于注册字符设备驱动，`register_netdev`用于注册网络设备驱动等）将设备注册到内核中。内核维护着各种设备的链表，模块注册的设备信息会被添加到相应链表中，这样内核就能管理和使用这些设备。
3. **模块间依赖处理**：如果静态加载的模块存在依赖关系，内核会在加载过程中处理这些依赖。内核会先确保被依赖的模块已经被加载和初始化，然后再加载依赖它的模块。例如，模块 A 依赖模块 B，内核会先查找并加载模块 B，调用其初始化函数完成初始化，再加载模块 A。这种依赖处理机制保证了模块在加载时能正确运行，避免因依赖未满足而导致的错误。



## 大小端

大小端指的是数据在内存中的存储方式。大端模式里，数据的高位字节存于低地址，低位字节存于高地址；小端模式则相反，数据的低位字节存于低地址，高位字节存于高地址。

````C
#include <stdio.h>

int is_little_endian() {
    unsigned int num = 1;
    // 取num的首地址并转换为char*类型
    char *ptr = (char *)&num;
    // 小端模式下，低地址存储的是1
    return (*ptr == 1); 
}

int main() {
    if (is_little_endian()) {
        printf("This system is little endian.\n");
    } else {
        printf("This system is big endian.\n");
    }
    return 0;
}
````



## 网络

### MTU 和 MSS

- **MTU（最大传输单元）**：链路层规定的帧数据部分的最大长度，不同网络链路（如以太网、PPP 链路）MTU 值不同，以太网常见为 1500 字节 。它限制了一次能传输的最大数据量，超过会分片。
- **MSS（最大报文段长度）**：TCP 报文段中数据部分的最大长度 ，由双方在三次握手时协商确定，一般为 MTU 减去 IP 首部和 TCP 首部长度（约 40 字节 ）。例如以太网 MTU 为 1500 字节时，MSS 约为 1460 字节。MSS 确保 TCP 报文段能在链路层帧中完整传输，避免链路层分片。

###  TCP 包长度

- **首部长度**：固定部分 20 字节，若有选项字段，最多可达 60 字节 。固定首部包含源端口、目的端口、序号、确认号、首部长度、标志位、窗口、校验和、紧急指针等字段。
- **数据部分长度**：受 MSS 限制，最大为 MSS 值。TCP 包总长度即首部长度与数据部分长度之和 ，最大理论上受 IP 数据报长度限制（65535 字节 ，减去 20 字节 IP 首部和 20 字节 TCP 首部后，TCP 数据部分最大约 65495 字节 ，实际受 MSS 及网络路径 MTU 影响）。



## 进程和线程

1. 定义
   - **进程**：是程序的一次执行过程，是系统进行资源分配和调度的一个独立单位 。比如打开一个浏览器程序，操作系统就会为其创建一个进程，这个进程包含了浏览器运行所需的资源，如内存空间、文件描述符等。
   - **线程**：是进程的一个实体，是 CPU 调度和分派的基本单位 ，它是比进程更小的能独立运行的基本单位。一个进程可以包含多个线程，例如浏览器进程中，可能有负责页面渲染的线程、处理网络请求的线程等。
2. 资源分配
   - **进程**：有独立的地址空间，拥有自己的代码段、数据段和堆栈段等，进程间的资源相互独立，互不干扰。
   - **线程**：共享所属进程的地址空间和资源，同一进程内的线程共享代码段、数据段、打开的文件等资源，只拥有自己独立的栈空间 。
3. 调度开销
   - **进程**：由于进程间资源独立，进程切换时，需要保存和恢复大量的上下文信息，包括寄存器状态、内存管理信息等，开销较大。
   - **线程**：因为共享进程资源，线程切换时只需保存和恢复少量的寄存器状态等信息，开销相对较小 。
4. 通信方式
   - **进程**：进程间通信（IPC）需要专门的机制，如管道、消息队列、共享内存、信号量等，以实现不同进程间的数据交换和同步。
   - **线程**：同一进程内的线程间通信相对简单，可直接通过共享内存空间进行数据交换，也可使用锁、信号量等同步机制来保证数据的一致性。
5. 并发性
   - **进程**：多个进程可以并发执行，操作系统通过调度算法分配 CPU 时间片给不同进程，实现宏观上的并行。
   - **线程**：一个进程内的多个线程也可以并发执行，而且在多处理器系统中，多个线程可以真正并行执行，充分利用 CPU 资源 。

### 进程的优点

1. **稳定性高**：进程有独立的地址空间，一个进程崩溃一般不会影响其他进程，不会导致整个系统崩溃。例如多个应用程序以进程形式运行，其中一个应用程序出现异常崩溃，不会影响其他应用程序的正常运行。
2. **资源隔离性好**：进程间资源相互独立，能有效防止进程间非法访问和干扰。比如不同用户的进程，其资源相互隔离，保证了用户数据的安全性和隐私性 。
3. **便于管理和控制**：操作系统对进程的管理机制较为完善，可方便地进行进程的创建、终止、调度等操作，能更好地协调不同进程间的资源分配和执行顺序 。



## 串口

### RS232

**RS - 232**：全双工通信，有独立的发送和接收线路 ，可同时进行数据的发送和接收，常见连接需发送线、接收线和地线三条线 。例如计算机与串口打印机连接，可一边发送打印任务数据，一边接收打印机状态反馈 。单端信号传输，以信号地为参考电位，信号电平是相对于地线的电压值 。如 DB9 或 DB25 连接器连接设备，典型信号电平在正负 12 伏和 5 伏间摆动 ，易受噪声干扰 。受电容允许值限制，传输距离有限，标准最大传输距离为 15 米 。最高传输速率为 20Kbps 。适用于短距离、点对点通信场景 ，如早期计算机与调制解调器、串口鼠标、键盘等设备连接 。

![image-20250429224103467](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250429224103467.png)

![image-20250429224117170](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250429224117170.png)

### RS485

**RS - 485**：多为半双工通信 ，同一时刻只能发送或接收数据 。采用两线制时，数据在一对双绞线上分时双向传输；四线制可实现全双工，但较少用。如工业现场多个传感器节点与主控制器通信，节点分时向主控制器上报数据 。差分信号传输，用一对双绞线，两线分别定义为 A 和 B 。通过检测两线间电位差传输信号，抗干扰能力强 。传输距离长，最大无线传输距离 1200 米 ，采用阻抗匹配、低衰减专用电缆可达 1800 米 ，超过距离可加中继器扩展，加 8 只中继器传输距离接近 10km 。最大传输速率 10Mbps ，且传输速率与距离相关，距离越长速率越低 。用于长距离、多点通信及对抗干扰能力要求高的工业控制系统 ，像工业自动化生产线设备联网、智能楼宇监控系统中传感器与控制器通信 。

![image-20250429224232517](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250429224232517.png)

![image-20250429224429008](%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E5%92%8C%E7%9F%A5%E8%AF%86.assets/image-20250429224429008.png)



# 电子书籍网

https://github.com/YoungLC/ebooks.git

## 内核源码在线阅读网站

https://elixir.bootlin.com/

# 力扣每日一题

## 1

https://leetcode.cn/problems/build-array-from-permutation/

给你一个 **从 0 开始的排列** `nums`（**下标也从 0 开始**）。请你构建一个 **同样长度** 的数组 `ans` ，其中，对于每个 `i`（`0 <= i < nums.length`），都满足 `ans[i] = nums[nums[i]]` 。返回构建好的数组 `ans` 。

**从 0 开始的排列** `nums` 是一个由 `0` 到 `nums.length - 1`（`0` 和 `nums.length - 1` 也包含在内）的不同整数组成的数组。

提交

```C
/**
 * Note: The returned array must be malloced, assume caller calls free().
 */
int* buildArray(int* nums, int numsSize, int* returnSize) {
    int *ret = (int *)malloc(sizeof(int) * numsSize);
    *returnSize = numsSize;
    for(int i = 0;i < numsSize ;i++ )
    {
        ret[i] = nums[nums[i]];
    }

    return ret;
}
```

好的题解

```C
int* buildArray(int* nums, int numsSize, int* returnSize) {
    for (int i = 0; i < numsSize; i++) {
        int x = nums[i];
        if (x < 0) { // 已搬家
            continue;
        }
        int cur = i;
        while (nums[cur] != i) {
            int nxt = nums[cur];
            nums[cur] = ~nums[nxt]; // 把下一个数搬过来，同时做标记（取反）
            cur = nxt;
        }
        nums[cur] = ~x; // 对于这一组的最后一个数，把起点 x=nums[i] 搬过来
    }

    for (int i = 0; i < numsSize; i++) {
        nums[i] = ~nums[i]; // 复原
    }
    *returnSize = numsSize;
    return nums;
}
```

注意 nums 是一个 0 到 n−1 的排列。元素值的范围和下标的范围，都是 [0,n−1]。所以从 i 开始，不断迭代 i=nums[i]，一定会回到起点 i。同时也就是可以将这个数据类型想象为图，每个节点只有一条出边和入边，所以整体只存在多个不交叉的环，每个的i最后一定会在后续的num[cur]中找到i，这个时候也就是找到最后的节点，这节点的出边就是最开始的nums[i]这个节点，对这个最后的节点进行数据更新



## 2

https://leetcode.cn/problems/find-minimum-time-to-reach-last-room-i/

有一个地窖，地窖中有 `n x m` 个房间，它们呈网格状排布。

给你一个大小为 `n x m` 的二维数组 `moveTime` ，其中 `moveTime[i][j]` 表示在这个时刻 **以后** 你才可以 **开始** 往这个房间 **移动** 。你在时刻 `t = 0` 时从房间 `(0, 0)` 出发，每次可以移动到 **相邻** 的一个房间。在 **相邻** 房间之间移动需要的时间为 1 秒。

请你返回到达房间 `(n - 1, m - 1)` 所需要的 **最少** 时间。

如果两个房间有一条公共边（可以是水平的也可以是竖直的），那么我们称这两个房间是 **相邻** 的。

```C
#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>

#define MAX_QUEUE_SIZE 10000

typedef struct {
    int x, y;
    int time;
} Node;

// 比较函数，用于优先队列
int cmp(const void *a, const void *b) {
    return ((Node *)a)->time - ((Node *)b)->time;
}

int minTimeToReach(int **moveTime, int moveTimeSize, int *moveTimeColSize) {
    int n = moveTimeSize;
    int m = moveTimeColSize[0];
    bool visited[n][m];
    for (int i = 0; i < n; i++) {
        for (int j = 0; j < m; j++) {
            visited[i][j] = false;
        }
    }
    Node pq[MAX_QUEUE_SIZE];
    int pq_size = 0;
    pq[pq_size].x = 0;
    pq[pq_size].y = 0;
    pq[pq_size].time = 0;
    pq_size++;

    int dx[] = {-1, 1, 0, 0};
    int dy[] = {0, 0, -1, 1};

    while (pq_size > 0) {
        // 从优先队列中取出时间最小的节点
        qsort(pq, pq_size, sizeof(Node), cmp);
        Node current = pq[0];
        for (int i = 0; i < pq_size - 1; i++) {
            pq[i] = pq[i + 1];
        }
        pq_size--;

        int x = current.x;
        int y = current.y;
        int time = current.time;

        if (visited[x][y]) {
            continue;
        }
        visited[x][y] = true;

        if (x == n - 1 && y == m - 1) {
            return time;
        }

        for (int j = 0; j < 4; j++) {
            int newX = x + dx[j];
            int newY = y + dy[j];
            if (newX >= 0 && newX < n && newY >= 0 && newY < m &&!visited[newX][newY]) {
                int nextTime = (time > moveTime[newX][newY]) ? time + 1 : moveTime[newX][newY] + 1;
                pq[pq_size].x = newX;
                pq[pq_size].y = newY;
                printf("%d %d %d\n",newX,newY,nextTime) ;
                pq[pq_size].time = nextTime;
                pq_size++;
            }
        }
    }
    return -1;
}
```

就是维护一个优先的队列，每次从这个队列中取出最小时间的节点，将这个节点的周围到达的时间更新到队列中，直到找到目标的节点，Dijkstra算法，更像洪水的那个解题思路，标记过的就跳过，表示这个时间能到达这个点，但是没有Dijkstra好，会得到最后的一个最小的时间图
